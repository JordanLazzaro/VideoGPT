{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YM-sO50_MNBM"
      },
      "outputs": [],
      "source": [
        "!pip install -q wandb pytorch_lightning av imageio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "2bkr0JVXBgxw"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import math\n",
        "import wandb\n",
        "import imageio\n",
        "import torch\n",
        "import torchvision\n",
        "import numpy as np\n",
        "import pytorch_lightning as pl\n",
        "from tqdm.notebook import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.animation import FuncAnimation\n",
        "from IPython.display import display, HTML\n",
        "from torch import nn, optim\n",
        "from torch.nn import functional as F\n",
        "from torchvision.datasets.video_utils import VideoClips\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torchvision.transforms import Compose, Lambda, Resize, ToTensor, CenterCrop, Grayscale\n",
        "import torchvision.transforms.functional as TF\n",
        "\n",
        "from pytorch_lightning.loggers import WandbLogger\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint, LearningRateMonitor, EarlyStopping\n",
        "from pytorch_lightning.tuner import Tuner\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR, OneCycleLR\n",
        "\n",
        "pl.seed_everything(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wCCwpzdaEHw9",
        "outputId": "6faaa634-d8a2-4596-93f7-820b2885c312"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "steamboat_willie_gdrive_path = '/content/drive/My Drive/SteamboatWillie/SteamboatWillie.mp4'\n",
        "!cp -r /content/drive/My\\ Drive/SteamboatWillie/clips ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d1AMOdC0FLCM",
        "outputId": "91c5cc7f-e91a-4419-e4f3-22d498051ec4"
      },
      "outputs": [],
      "source": [
        "class SteamboatWillieDataset(Dataset):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "\n",
        "        self.preprocess_transforms = Compose([\n",
        "                Lambda(lambda x: x.permute(0, 3, 1, 2)), # (T, H, W, C) to (T, C, H, W) for Greyscale\n",
        "                Grayscale(num_output_channels=1), # Convert to grayscale\n",
        "                Lambda(lambda x: x.permute(1, 0, 2, 3)), # (T, C, H, W) to (C, T, H, W) for Conv3d\n",
        "                Lambda(lambda x: CenterCrop((480, 575))(x)), # Center crop to remove virtical bars\n",
        "                Lambda(lambda x: Resize((config.img_size, config.img_size))(x))\n",
        "                # Lambda(lambda x: Resize((config.img_size, config.img_size), interpolation=TF.InterpolationMode.BICUBIC)(x)), # Resize frames\n",
        "        ])\n",
        "\n",
        "        self.postprocess_transforms = Compose([\n",
        "            Lambda(lambda x: x / 255.),\n",
        "            Lambda(lambda x: x.view(self.config.in_channels, self.config.clip_length, self.config.img_size, self.config.img_size))\n",
        "        ])\n",
        "\n",
        "        if os.path.exists(config.dest_dir):\n",
        "            clip_paths = self.build_existing_clip_paths(config.dest_dir)\n",
        "            self.clips = self.build_clip_refs(clip_paths)\n",
        "        else:\n",
        "            video_clips = VideoClips(\n",
        "                config.paths,\n",
        "                clip_length_in_frames=config.clip_length,\n",
        "                frames_between_clips=config.clip_length\n",
        "            )\n",
        "\n",
        "            self.clips = self.build_clip_refs(self.build_clip_paths(video_clips, self.preprocess_transforms, config.dest_dir))\n",
        "\n",
        "    def build_clip_paths(self, video_clips, transforms, dest_dir):\n",
        "        \"\"\"\n",
        "        Build set of binary files to store processed video clips\n",
        "        returns dict of clip_idx -> mmapped file path\n",
        "        \"\"\"\n",
        "        clip_paths = {}\n",
        "\n",
        "        if not os.path.exists(dest_dir):\n",
        "            os.makedirs(dest_dir)\n",
        "\n",
        "        for idx in tqdm(range(video_clips.num_clips()), desc='Creating clip .bin files'):\n",
        "            # transform clips and write to mmap file\n",
        "            clip, _, _, _ = video_clips.get_clip(idx)\n",
        "            clip = self.preprocess_transforms(clip)\n",
        "            clip_np = clip.numpy().astype(np.uint8)\n",
        "\n",
        "            mmapped_file_path = os.path.join(dest_dir, f'clip_{idx}.bin')\n",
        "            fp = np.memmap(mmapped_file_path, dtype='uint8', mode='w+', shape=clip_np.shape)\n",
        "            fp[:] = clip_np[:]\n",
        "            fp.flush()\n",
        "            del fp\n",
        "            clip_paths[idx] = mmapped_file_path\n",
        "\n",
        "        return clip_paths\n",
        "\n",
        "    def build_existing_clip_paths(self, dest_dir):\n",
        "        \"\"\"\"\n",
        "        returns dict of clip_idx -> mmapped file path\n",
        "        from existing .bin files\n",
        "        \"\"\"\n",
        "        clips_paths = {}\n",
        "        for filename in os.listdir(dest_dir):\n",
        "            if filename.startswith('clip_') and filename.endswith('.bin'):\n",
        "                idx = int(filename.split('_')[1].split('.')[0])\n",
        "                file_path = os.path.join(dest_dir, filename)\n",
        "                clips_paths[idx] = file_path\n",
        "\n",
        "        return clips_paths\n",
        "\n",
        "    def build_clip_refs(self, clip_paths):\n",
        "        \"\"\"\n",
        "        Build mmap reference to bin files\n",
        "        returns dict of clip_idx -> np.array mmapped to respective bin file\n",
        "        \"\"\"\n",
        "        clips = {}\n",
        "        for idx, path in tqdm(clip_paths.items(), desc='Building clip refs'):\n",
        "            clips[idx] = np.memmap(path, dtype='uint8', mode='r')\n",
        "\n",
        "        return clips\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.clips)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        clip = self.clips[idx]\n",
        "        return self.postprocess_transforms(torch.tensor(clip, dtype=torch.float32))\n",
        "\n",
        "\n",
        "class SteamboatWillieDataModule(pl.LightningDataModule):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.batch_size = config.batch_size\n",
        "        self.config = config\n",
        "\n",
        "    def prepare_data(self):\n",
        "        self.full_dataset = SteamboatWillieDataset(self.config)\n",
        "\n",
        "    def setup(self, stage=None):\n",
        "        if stage == 'fit' or stage is None:\n",
        "            train_len = int(len(self.full_dataset) * self.config.train_split)\n",
        "            val_len = len(self.full_dataset) - train_len\n",
        "            self.train_dataset, self.val_dataset = random_split(self.full_dataset, [train_len, val_len])\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return torch.utils.data.DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True, num_workers=self.config.num_workers)\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return torch.utils.data.DataLoader(self.val_dataset, batch_size=self.batch_size, shuffle=False, num_workers=self.config.num_workers)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model Components"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ResBlock(nn.Module):\n",
        "    def __init__(\n",
        "            self,\n",
        "            in_channels,\n",
        "            out_channels\n",
        "        ):\n",
        "        super().__init__()\n",
        "        if in_channels != out_channels:\n",
        "            self.identity = nn.Conv3d(\n",
        "                in_channels=in_channels,\n",
        "                out_channels=out_channels,\n",
        "                kernel_size=1\n",
        "            )\n",
        "        else:\n",
        "            self.identity = nn.Identity()\n",
        "\n",
        "        self.block = nn.Sequential(\n",
        "            nn.Conv3d(\n",
        "                in_channels=in_channels,\n",
        "                out_channels=in_channels,\n",
        "                kernel_size=3,\n",
        "                padding=1\n",
        "            ),\n",
        "            nn.BatchNorm3d(in_channels),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv3d(\n",
        "                in_channels=in_channels,\n",
        "                out_channels=out_channels,\n",
        "                kernel_size=1\n",
        "            ),\n",
        "            nn.BatchNorm3d(out_channels)\n",
        "        )\n",
        "        self.res_act = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.block(x) + self.identity(x)\n",
        "        return self.res_act(out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(\n",
        "            self,\n",
        "            in_channels,\n",
        "            hidden_channels,\n",
        "            out_channels,\n",
        "            nlayers,\n",
        "            nblocks\n",
        "        ):\n",
        "        super().__init__()\n",
        "        self.downsample_blocks = nn.Sequential(*[\n",
        "            nn.Sequential(\n",
        "                nn.Conv3d(\n",
        "                    in_channels=in_channels if i==0 else hidden_channels,\n",
        "                    out_channels=hidden_channels,\n",
        "                    kernel_size=(4, 4, 4),\n",
        "                    stride=(2, 2, 2),\n",
        "                    padding=(1, 1, 1)\n",
        "                ),\n",
        "                nn.BatchNorm3d(hidden_channels),\n",
        "                nn.ReLU()\n",
        "            ) for i in range(nlayers)\n",
        "        ])\n",
        "\n",
        "        self.res_blocks = nn.Sequential(*[\n",
        "            ResBlock(\n",
        "                in_channels=hidden_channels,\n",
        "                out_channels=out_channels if i==nblocks-1 else hidden_channels\n",
        "            ) for i in range(nblocks)\n",
        "        ])\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.downsample_blocks(x)\n",
        "        x = self.res_blocks(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Decoder(torch.nn.Module):\n",
        "    def __init__(\n",
        "            self,\n",
        "            in_channels,\n",
        "            hidden_channels,\n",
        "            out_channels,\n",
        "            nlayers,\n",
        "            nblocks\n",
        "        ):\n",
        "        super().__init__()\n",
        "        self.res_blocks = nn.Sequential(*[\n",
        "            ResBlock(\n",
        "                in_channels=in_channels if i==0 else hidden_channels,\n",
        "                out_channels=hidden_channels,\n",
        "            )\n",
        "            for i in range(nblocks)\n",
        "        ])\n",
        "\n",
        "        self.upsample_blocks = nn.Sequential(*[\n",
        "            nn.Sequential(\n",
        "                nn.ConvTranspose3d(\n",
        "                    in_channels=hidden_channels,\n",
        "                    out_channels=hidden_channels,\n",
        "                    kernel_size=(4, 4, 4),\n",
        "                    stride=(2, 2, 2),\n",
        "                    padding=(1, 1, 1)),\n",
        "                nn.BatchNorm3d(hidden_channels),\n",
        "                nn.ReLU()\n",
        "            ) for i in range(nlayers-1)\n",
        "        ])\n",
        "\n",
        "        self.out_layer = nn.ConvTranspose3d(\n",
        "            in_channels=hidden_channels,\n",
        "            out_channels=out_channels,\n",
        "            kernel_size=(4, 4, 4),\n",
        "            stride=(2, 2, 2),\n",
        "            padding=(1, 1, 1)\n",
        "        )\n",
        "\n",
        "        self.out_act = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.res_blocks(x)\n",
        "        x = self.upsample_blocks(x)\n",
        "        x = self.out_layer(x)\n",
        "        x = self.out_act(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class QuantizerEMA(nn.Module):\n",
        "    def __init__(\n",
        "            self,\n",
        "            codebook_size,\n",
        "            latent_channels,\n",
        "            ema_gamma,\n",
        "            commit_loss_beta,\n",
        "            track_codebook\n",
        "        ):\n",
        "        super().__init__()\n",
        "        self.codebook_size = codebook_size\n",
        "        self.latent_channels = latent_channels\n",
        "        self.ema_gamma = ema_gamma\n",
        "        self.commit_loss_beta = commit_loss_beta\n",
        "        self.track_codebook = track_codebook\n",
        "\n",
        "        self.codebook = nn.Embedding(codebook_size, latent_channels)\n",
        "        nn.init.uniform_(self.codebook.weight, -1/codebook_size, 1/codebook_size)\n",
        "\n",
        "        self.register_buffer('N', torch.zeros(codebook_size) + 1e-6)\n",
        "        self.register_buffer('m', torch.zeros(codebook_size, latent_channels))\n",
        "        nn.init.uniform_(self.m, -1/codebook_size, 1/codebook_size)\n",
        "\n",
        "        if track_codebook:\n",
        "            self.register_buffer('codebook_usage', torch.zeros(codebook_size, dtype=torch.float))\n",
        "            self.register_buffer('total_usage', torch.tensor(0, dtype=torch.float))\n",
        "\n",
        "    def ema_update(self, code_idxs, flat_inputs):\n",
        "        # we don't want to track grads for ops in EMA calculation\n",
        "        code_idxs, flat_inputs = code_idxs.detach(), flat_inputs.detach()\n",
        "\n",
        "        # number of vectors for each i which quantize to e_i\n",
        "        n = torch.bincount(code_idxs, minlength=self.codebook_size)\n",
        "\n",
        "        # sum of vectors for each i which quantize to code e_i\n",
        "        one_hot_indices = F.one_hot(code_idxs, num_classes=self.codebook_size).type(flat_inputs.dtype)\n",
        "        embed_sums = one_hot_indices.T @ flat_inputs\n",
        "\n",
        "        # update EMA of code usage and sum of codes\n",
        "        self.N = self.N * self.ema_gamma + n * (1 - self.ema_gamma)\n",
        "        self.m = self.m * self.ema_gamma + embed_sums * (1 - self.ema_gamma)\n",
        "\n",
        "        self.codebook.weight.data.copy_(self.m / self.N.unsqueeze(-1))\n",
        "\n",
        "    def reset_usage_stats(self):\n",
        "        self.codebook_usage.zero_()\n",
        "        self.total_usage.zero_()\n",
        "\n",
        "    def calculate_perplexity(self, enc_idxs):\n",
        "        unique_indices, counts = torch.unique(enc_idxs, return_counts=True)\n",
        "        self.codebook_usage.index_add_(0, unique_indices, counts.float())\n",
        "        self.total_usage += torch.sum(counts)\n",
        "\n",
        "        if self.total_usage > 0:\n",
        "            probs = self.codebook_usage / self.total_usage\n",
        "            perplexity = torch.exp(-torch.sum(probs * torch.log(probs + 1e-10)))\n",
        "            return perplexity\n",
        "        else:\n",
        "            return torch.tensor([0.0])\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        B, C, T, H, W = inputs.shape\n",
        "\n",
        "        # (B, C, T, H, W) --> (B, T, H, W, C)\n",
        "        inputs_permuted = inputs.permute(0, 2, 3, 4, 1).contiguous()\n",
        "        # (B, T, H, W, C) --> (BTHW, C)\n",
        "        flat_inputs = inputs_permuted.reshape(-1, self.latent_channels)\n",
        "\n",
        "        # (BTHW, Codebook Size)\n",
        "        # Σ(x-y)^2 = Σx^2 - 2xy + Σy^2\n",
        "        dists = (\n",
        "            torch.sum(flat_inputs ** 2, dim=1, keepdim=True) - # Σx^2\n",
        "            2 * (flat_inputs @ self.codebook.weight.t()) +     # 2*xy\n",
        "            torch.sum(self.codebook.weight ** 2, dim=1)        # Σy^2\n",
        "        )\n",
        "\n",
        "        # (BTHW, 1)\n",
        "        code_idxs = torch.argmin(dists, dim=1)\n",
        "        # (BTHW, C)\n",
        "        codes = self.codebook(code_idxs)\n",
        "        # (BTHW, C) --> (B, T, H, W, C)\n",
        "        quantized_inputs = codes.reshape(B, T, H, W, C)\n",
        "        # (B, T, H, W, C) --> (B, C, T, H, W)\n",
        "        quantized_inputs = quantized_inputs.permute(0, 4, 1, 2, 3).contiguous()\n",
        "\n",
        "        if self.training:\n",
        "            # perform exponential moving average update for codebook\n",
        "            self.ema_update(code_idxs, flat_inputs)\n",
        "\n",
        "        # \"since the volume of the embedding space is dimensionless, it can grow\n",
        "        # arbitrarily if the embeddings e_i do not train as fast as the encoder\n",
        "        # parameters. To make sure the encoder commits to an embedding and its\n",
        "        # output does not grow, we add a commitment loss\"\n",
        "        commitment_loss = F.mse_loss(quantized_inputs.detach(), inputs)\n",
        "\n",
        "        # part 3 of full loss (ie. not including reconstruciton loss or embedding loss)\n",
        "        vq_loss = commitment_loss * self.commit_loss_beta\n",
        "\n",
        "        # sets the output to be the input plus the residual value between the\n",
        "        # quantized latents and the inputs like a resnet for Straight Through\n",
        "        # Estimation (STE)\n",
        "        quantized_inputs = inputs + (quantized_inputs - inputs).detach()\n",
        "\n",
        "        if self.track_codebook:\n",
        "            perplexity = self.calculate_perplexity(code_idxs)\n",
        "\n",
        "        return {\n",
        "            'q_z':              quantized_inputs,\n",
        "            'vq_loss':          vq_loss,\n",
        "            'commitment_loss':  commitment_loss,\n",
        "            'perplexity':       perplexity if self.track_codebook else torch.tensor([0.0])\n",
        "        }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class VQVAE(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.encoder = Encoder(\n",
        "            in_channels     = config.in_channels,\n",
        "            hidden_channels = config.hidden_channels,\n",
        "            out_channels    = config.latent_channels,\n",
        "            nlayers         = config.nlayers,\n",
        "            nblocks         = config.nblocks\n",
        "        )\n",
        "        self.decoder = Decoder(\n",
        "            in_channels     = config.latent_channels,\n",
        "            hidden_channels = config.hidden_channels,\n",
        "            out_channels    = config.in_channels,\n",
        "            nlayers         = config.nlayers,\n",
        "            nblocks         = config.nblocks\n",
        "        )\n",
        "        self.quantizer = QuantizerEMA(\n",
        "            codebook_size    = config.codebook_size,\n",
        "            latent_channels  = config.latent_channels,\n",
        "            ema_gamma        = config.ema_gamma,\n",
        "            commit_loss_beta = config.commit_loss_beta,\n",
        "            track_codebook   = config.track_codebook,\n",
        "        )\n",
        "\n",
        "    def encode(self, inputs):\n",
        "        return self.encoder(inputs)\n",
        "\n",
        "    def quantize(self, z):\n",
        "        return self.quantizer(z)\n",
        "\n",
        "    def decode(self, q_z):\n",
        "        return self.decoder(q_z)\n",
        "\n",
        "    def loss(self, x_hat, x, quantized):\n",
        "        MSE = F.mse_loss(x_hat, x)\n",
        "        loss = MSE + quantized['vq_loss']\n",
        "\n",
        "        return {\n",
        "            'MSE':  MSE,\n",
        "            'loss': loss,\n",
        "            **quantized\n",
        "        }\n",
        "\n",
        "    def forward(self, x):\n",
        "        z = self.encode(x)\n",
        "        quantized = self.quantize(z)\n",
        "        x_hat = self.decode(quantized['q_z'])\n",
        "        losses = self.loss(x_hat, x, quantized)\n",
        "\n",
        "        return {'x_hat': x_hat, **losses}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Lightning Module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class LitVQVAE(pl.LightningModule):\n",
        "    def __init__(self, model, config):\n",
        "        super().__init__()\n",
        "        self.model = model\n",
        "        self.config = config\n",
        "        self.lr = config.lr\n",
        "\n",
        "        if self.logger:\n",
        "            self.logger.experiment.config.update(config)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        x = batch\n",
        "        out = self(x)\n",
        "\n",
        "        self.log('train/loss',            out['loss'],            prog_bar=True)\n",
        "        self.log('train/MSE',             out['MSE'],             prog_bar=True)\n",
        "        self.log('train/vq_loss',         out['vq_loss'],         prog_bar=True)\n",
        "        self.log('train/commitment_loss', out['commitment_loss'], prog_bar=True)\n",
        "        self.log('train/perplexity',      out['perplexity'],      prog_bar=True)\n",
        "\n",
        "        return out['loss']\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        x = batch\n",
        "        out = self(x)\n",
        "\n",
        "        self.log('val/loss',            out['loss'],            prog_bar=True)\n",
        "        self.log('val/MSE',             out['MSE'],             prog_bar=True)\n",
        "        self.log('val/vq_loss',         out['vq_loss'],         prog_bar=True)\n",
        "        self.log('val/commitment_loss', out['commitment_loss'], prog_bar=True)\n",
        "\n",
        "        if batch_idx == 0:\n",
        "            self.log_val_clips(x, out)\n",
        "\n",
        "        return out['loss']\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        # tracking perplexity per epoch\n",
        "        self.model.quantizer.reset_usage_stats()\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.AdamW(\n",
        "            self.parameters(),\n",
        "            lr=self.lr,\n",
        "            betas=(self.config.beta1, self.config.beta2),\n",
        "            weight_decay=self.config.weight_decay\n",
        "        )\n",
        "\n",
        "        if self.config.use_lr_schedule:\n",
        "            scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=self.config.max_epochs)\n",
        "            return [optimizer], [scheduler]\n",
        "\n",
        "        return optimizer\n",
        "\n",
        "    def log_val_clips(self, x, out, num_clips=2):\n",
        "        n_clips = min(x.size(0), num_clips)\n",
        "\n",
        "        for i in range(n_clips):\n",
        "            # Extract the ith original and reconstructed clip\n",
        "            original_clip = x[i]  # (C, T, H, W)\n",
        "            reconstructed_clip = out['x_hat'][i]  # (C, T, H, W)\n",
        "\n",
        "            # convert tensors to numpy arrays and transpose to (T, H, W, C) for GIF creation\n",
        "            original_clip_np = original_clip.permute(1, 2, 3, 0).cpu().numpy()\n",
        "            reconstructed_clip_np = reconstructed_clip.permute(1, 2, 3, 0).cpu().numpy()\n",
        "\n",
        "            original_clip_np = (original_clip_np - original_clip_np.min()) / (original_clip_np.max() - original_clip_np.min())\n",
        "            reconstructed_clip_np = (reconstructed_clip_np - reconstructed_clip_np.min()) / (reconstructed_clip_np.max() - reconstructed_clip_np.min())\n",
        "\n",
        "            original_clip_np = (original_clip_np * 255).astype(np.uint8)\n",
        "            reconstructed_clip_np = (reconstructed_clip_np * 255).astype(np.uint8)\n",
        "\n",
        "            # grayscale videos need to be of shape (T, H, W)\n",
        "            if original_clip_np.shape[-1] == 1:\n",
        "                original_clip_np = original_clip_np.squeeze(-1)\n",
        "\n",
        "            if reconstructed_clip_np.shape[-1] == 1:\n",
        "                reconstructed_clip_np = reconstructed_clip_np.squeeze(-1)\n",
        "\n",
        "            # create GIFs for the original and reconstructed clips\n",
        "            original_gif_path = f'/tmp/original_clip_{i}.gif'\n",
        "            reconstructed_gif_path = f'/tmp/reconstructed_clip_{i}.gif'\n",
        "            imageio.mimsave(original_gif_path, original_clip_np, fps=5)\n",
        "            imageio.mimsave(reconstructed_gif_path, reconstructed_clip_np, fps=5)\n",
        "\n",
        "            # log the GIFs to wandb\n",
        "            self.logger.experiment.log({\n",
        "                f\"val/original_clip_{i}\": wandb.Video(original_gif_path, fps=5, format=\"gif\", caption=\"Original\"),\n",
        "                f\"val/reconstructed_clip_{i}\": wandb.Video(reconstructed_gif_path, fps=5, format=\"gif\", caption=\"Reconstructed\")\n",
        "            })"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_num_downsample_layers(img_size):\n",
        "    \"\"\"\n",
        "    Get the number of strided Conv2D layers\n",
        "    required to produce a 2x2 output volume\n",
        "    \"\"\"\n",
        "    if img_size < 2:\n",
        "        raise ValueError(\"Image size must be at least 2x2.\")\n",
        "\n",
        "    # Calculate the minimum number of downsample layers required for 2x2 final\n",
        "    num_layers = math.ceil(math.log2(img_size / 2))\n",
        "    return num_layers\n",
        "\n",
        "def build_channel_dims(start_channels, nlayers):\n",
        "    \"\"\"\n",
        "    Construct a list of channel counts for nlayers downsample layers\n",
        "    assuming the channels double as spatial dims halve\n",
        "    \"\"\"\n",
        "    channels = []\n",
        "    for _ in range(nlayers):\n",
        "        channels.append(start_channels)\n",
        "        start_channels *= 2\n",
        "    return channels\n",
        "\n",
        "class VideoVQVAEConfig:\n",
        "    def __init__(self):\n",
        "        # dataset properties\n",
        "        self.paths = ['/content/drive/My Drive/SteamboatWillie/SteamboatWillie.mp4']\n",
        "        self.dest_dir = './clips/'\n",
        "        # model checkpoints\n",
        "        self.checkpoint_path = \"./checkpoints\"\n",
        "        self.save_top_k = 1\n",
        "        # training\n",
        "        self.train_split = 0.8\n",
        "        self.batch_size = 32\n",
        "        self.max_epochs = 120\n",
        "        self.training_steps = 100000\n",
        "        self.num_workers = 2\n",
        "        # optimizer\n",
        "        self.lr = 1e-4\n",
        "        self.beta1 = 0.9\n",
        "        self.beta2 = 0.999\n",
        "        self.weight_decay = 0.0 # 1e-2\n",
        "        self.use_wd_schedule = False\n",
        "        self.use_lr_schedule = False\n",
        "        # input properties\n",
        "        self.clip_length = 16\n",
        "        self.img_size = 256\n",
        "        self.in_channels = 1\n",
        "        # latents / quantization\n",
        "        self.latent_channels = 16\n",
        "        self.codebook_size = 1024\n",
        "        self.commit_loss_beta = 0.25\n",
        "        self.track_codebook = True\n",
        "        self.use_ema = True\n",
        "        self.ema_gamma = 0.99\n",
        "        # encoder/decoder\n",
        "        self.hidden_channels = 256\n",
        "        self.nblocks = 2\n",
        "        self.nlayers = 4\n",
        "\n",
        "    def update(self, updates):\n",
        "        for key, value in updates.items():\n",
        "            if hasattr(self, key):\n",
        "                setattr(self, key, value)\n",
        "\n",
        "    def to_dict(self):\n",
        "        return {k: v for k, v in self.__dict__.items()}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Component Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "config = VideoVQVAEConfig()\n",
        "data_module = SteamboatWillieDataModule(config)\n",
        "data_module.prepare_data()\n",
        "data_module.setup()\n",
        "\n",
        "train_loader = data_module.train_dataloader()\n",
        "val_loader = data_module.val_dataloader()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_batch = next(iter(train_loader))\n",
        "train_batch.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "encoder = Encoder()\n",
        "sample_enc = encoder(train_batch)\n",
        "sample_enc.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "quantizer = QuantizerEMA(\n",
        "    codebook_size=512,\n",
        "    latent_channels=16,\n",
        "    ema_gamma=0.99,\n",
        "    commit_loss_beta=0.25,\n",
        "    track_codebook=False\n",
        ")\n",
        "qz = quantizer(sample_enc)\n",
        "qz['quantized_inputs'].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "decoder = Decoder()\n",
        "recon_clip = decoder(sample_enc)\n",
        "recon_clip.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "config = VideoVQVAEConfig()\n",
        "vqvae = VQVAE(config)\n",
        "output = vqvae(train_batch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f'q_z.shape: {output[\"q_z\"].shape}')\n",
        "print(f'x_hat.shape: {output[\"x_hat\"].shape}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "config = VideoVQVAEConfig()\n",
        "model = VQVAE(config)\n",
        "lit_model = LitVQVAE(model, config)\n",
        "steamboat_willie_data = SteamboatWillieDataModule(config)\n",
        "\n",
        "wandb.init(project=\"VQ-VAE Steamboat Willie\", config=config.to_dict())\n",
        "wandb_logger = WandbLogger(project=\"VQ-VAE Steamboat Willie\", log_model=False)\n",
        "wandb_logger.watch(lit_model, log=\"all\")\n",
        "\n",
        "lr_monitor = LearningRateMonitor(logging_interval='step')\n",
        "\n",
        "checkpoint_callback = ModelCheckpoint(\n",
        "    dirpath=config.checkpoint_path,\n",
        "    filename='model-{epoch:02d}-{val_loss:.2f}',\n",
        "    every_n_epochs=5,\n",
        "    save_top_k=config.save_top_k,\n",
        "    monitor='val/loss',\n",
        "    mode='min',\n",
        "    save_last=True\n",
        ")\n",
        "\n",
        "# Define the EarlyStopping callback\n",
        "early_stop_callback = EarlyStopping(\n",
        "    monitor='val/loss',\n",
        "    min_delta=0.00,\n",
        "    patience=3,\n",
        "    verbose=True,\n",
        "    check_finite=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#wandb.finish()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "trainer = pl.Trainer(\n",
        "    max_epochs=config.max_epochs,\n",
        "    devices=1,\n",
        "    accelerator=\"gpu\",\n",
        "    precision=\"16-mixed\",\n",
        "    logger=wandb_logger,\n",
        "    callbacks=[\n",
        "        lr_monitor,\n",
        "        early_stop_callback,\n",
        "        # checkpoint_callback\n",
        "    ],\n",
        "    log_every_n_steps=1,\n",
        "    # overfit_batches=1,\n",
        ")\n",
        "\n",
        "# tuner = Tuner(trainer)\n",
        "# tuner.lr_find(lit_model, datamodule=steamboat_willie_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "trainer.fit(lit_model, steamboat_willie_data)\n",
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Display Clip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def display_clip(clip):\n",
        "    def update(frame_idx):\n",
        "        ax.clear()\n",
        "        ax.imshow(video_clip_np[frame_idx], cmap='gray')\n",
        "        ax.axis('off')\n",
        "\n",
        "    video_clip_np = clip.permute(1, 2, 3, 0).numpy()\n",
        "    fig, ax = plt.subplots()\n",
        "    ani = FuncAnimation(fig, update, frames=range(video_clip_np.shape[0]), interval=50)\n",
        "    plt.close()\n",
        "    display(HTML(ani.to_html5_video()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "clip = train_batch[7].view(1, -1, 256, 256)\n",
        "display_clip(clip)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
