{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "oiMtbbTAKpOP"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def simplified_bound(z, L):\n",
        "    eps = 1e-3\n",
        "    half_l = (L - 1) * (1 - eps) / 2\n",
        "    offset = 0.5 if L % 2 == 0 else 0.0\n",
        "    shift = np.tan(offset / half_l)\n",
        "    z_hat = np.tanh(z + shift) * half_l - offset\n",
        "    quantized = round_ste(z_hat)\n",
        "\n",
        "    half_width = L // 2\n",
        "    return quantized / half_width\n",
        "\n",
        "def simplified_bound_no_shift(z, L):\n",
        "    eps = 1e-3\n",
        "    half_l = (L - 1) * (1 - eps) / 2\n",
        "    offset = 0.5 if L % 2 == 0 else 0.0\n",
        "    z_hat = np.tanh(z) * half_l - offset\n",
        "    quantized = round_ste(z_hat)\n",
        "\n",
        "    half_width = L // 2\n",
        "    return quantized / half_width\n",
        "\n",
        "def simplified_bound_no_shift_no_offset(z, L):\n",
        "    eps = 1e-3\n",
        "    half_l = (L - 1) * (1 - eps) / 2\n",
        "    z_hat = np.tanh(z) * half_l\n",
        "\n",
        "def simplified_bound_no_shift_no_offset_no_eps(z, L):\n",
        "    half_l = (L - 1) / 2\n",
        "    z_hat = np.tanh(z) * half_l\n",
        "\n",
        "def simplified_bound_no_shift_no_offset_no_scale(z, L):\n",
        "    z_hat = np.tanh(z)\n",
        "\n",
        "def round_ste(z):\n",
        "    z_hat = np.round(z)\n",
        "    return z_hat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "AbNI3D8vKu5o",
        "outputId": "74a60c3e-f6b0-4591-92bd-4fe4c9dbc5c5"
      },
      "outputs": [],
      "source": [
        "z = np.linspace(-3, 3, 1000)\n",
        "\n",
        "z_bound_even = simplified_bound(z, 4)\n",
        "z_bound_even_no_shift = simplified_bound_no_shift(z, 4)\n",
        "z_bound_even_no_shift_no_offset = simplified_bound_no_shift_no_offset(z, 4)\n",
        "z_bound_even_no_shift_no_offset_no_scale = simplified_bound_no_shift_no_offset_no_scale(z, 4)\n",
        "\n",
        "z_bound_even_no_shift_rounded = z_bound_even_no_shift\n",
        "z_bound_even_rounded = z_bound_even\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# plt.plot(z, z_bound_even_no_shift_no_offset_no_scale, label='Step 1: Bound Function, L=4 (tanh w/ eps)', color='purple', linestyle='--')\n",
        "# plt.plot(z, z_bound_even_no_shift_no_offset, label='Step 2: Bound Function, L=4 (tanh + scale)', color='red', linestyle='--')\n",
        "plt.plot(z, z_bound_even_no_shift, label='Step 3: Bound Function, L=4 (tanh + scale + offset)', color='gold', linestyle='--')\n",
        "plt.plot(z, z_bound_even, label='Step 4: Bound Function, L=4 (tanh + scale + offset + shift)', color='green')\n",
        "plt.plot(z, z_bound_even_rounded, label='Step 5: Round', color='black')\n",
        "plt.plot(z, z_bound_even_no_shift_rounded, label='Step 5: Round (No Shift)', color='grey', linestyle='--')\n",
        "\n",
        "# Adding the title, labels, and legend\n",
        "plt.title('Effect of Bound Function for Even L')\n",
        "plt.xlabel('Input z')\n",
        "plt.ylabel('Transformed z')\n",
        "plt.legend()\n",
        "\n",
        "# Adding grid for better readability\n",
        "plt.grid(True)\n",
        "\n",
        "# Display the combined graph\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "1EZMG923KyeW",
        "outputId": "0606f814-ff81-4434-d580-d4ed8a6fc6cf"
      },
      "outputs": [],
      "source": [
        "z_bound_odd = simplified_bound(z, 5)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(z, z_bound_odd, label='Bound Function, L=5 (Odd)')\n",
        "plt.title('Effect of Bound Function for Odd L')\n",
        "plt.xlabel('Input z')\n",
        "plt.ylabel('Transformed z')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class FSQ(nn.Module):\n",
        "    def __init__(self, levels, eps=1e-3):\n",
        "        super().__init__()\n",
        "        self.register_buffer('levels', torch.tensor(levels))\n",
        "        self.register_buffer(\n",
        "            'basis',\n",
        "            torch.cumprod(torch.tensor([1] + levels[:-1]), dim=0, dtype=torch.int32)\n",
        "        )\n",
        "\n",
        "        self.eps = eps\n",
        "        self.codebook_size = torch.prod(self.levels)\n",
        "\n",
        "        self.register_buffer('implicit_codebook', self.idxs_to_codes(torch.arange(self.codebook_size)))\n",
        "\n",
        "    def round_ste(self, z):\n",
        "        z_q = torch.round(z)\n",
        "        return z + (z_q - z).detach()\n",
        "\n",
        "    def quantize(self, z):\n",
        "        # half_l is used to determine how to scale tanh; we\n",
        "        # subtract 1 from the number of levels to account for 0\n",
        "        # being a quantization bin and tanh being symmetric around 0\n",
        "        half_l = (self.levels - 1) * (1 - self.eps) / 2\n",
        "\n",
        "        # if a given level is even, it will result in a scale for tanh\n",
        "        # which is halfway between integer values, so we offset\n",
        "        # the tanh output down by 0.5 to line it with whole integers\n",
        "        offset = torch.where(self.levels % 2 == 0, 0.5, 0.0)\n",
        "\n",
        "        # if our level is even, we want to shift the tanh input to\n",
        "        # ensure the 0 quantization bin is centered\n",
        "        shift = torch.tan(offset / half_l)\n",
        "\n",
        "        # once we have our shift and offset (in the case of an even level)\n",
        "        # we can round to the nearest integer bin and allow for STE\n",
        "        z_q = self.round_ste(torch.tanh(z + shift) * half_l - offset)\n",
        "\n",
        "        # after quantization, we want to renormalize the quantized\n",
        "        # values to be within the range expected by the model (ie. [-1, 1])\n",
        "        half_width = self.levels // 2\n",
        "        return z_q / half_width\n",
        "\n",
        "    def scale_and_shift(self, z_q_normalized):\n",
        "        half_width = self.levels // 2\n",
        "        return (z_q_normalized * half_width) + half_width\n",
        "\n",
        "    def scale_and_shift_inverse(self, z_q):\n",
        "        half_width = self.levels // 2\n",
        "        return (z_q - half_width) / half_width\n",
        "\n",
        "    def code_to_idxs(self, z_q):\n",
        "        z_q = self.scale_and_shift(z_q)\n",
        "        return (z_q * self.basis).sum(dim=-1).to(torch.int32)\n",
        "\n",
        "    def idxs_to_code(self, idxs):\n",
        "        idxs = idxs.unsqueeze(-1)\n",
        "        codes_not_centered = (idxs // self.basis) % self.levels\n",
        "        return self.scale_and_shift_inverse(codes_not_centered)\n",
        "\n",
        "    def forward(self, z):\n",
        "        # TODO: make this work for generic tensor sizes\n",
        "        # TODO: use einops to clean up\n",
        "        B, C, T, H, W = z.shape\n",
        "\n",
        "        # (B, C, T, H, W) -> (B, T, H, W, C)\n",
        "        z_c_last = z.permute(0, 2, 3, 4, 1).contiguous()\n",
        "        \n",
        "        # (B, T, H, W, C) -> (BTHW, C)\n",
        "        z_flatten = z_c_last.reshape(-1, C)\n",
        "        \n",
        "        z_flatten_q = self.quantize(z_flatten)\n",
        "        \n",
        "        # (BTHW, C) -> (B, T, H, W, C) -> (B, C, T, H, W)\n",
        "        z_q = z_flatten_q.reshape(B, T, H, W, C).permute(0, 4, 1, 2, 3).contiguous()\n",
        "        \n",
        "        return z_q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fsq = FSQ(levels=[3, 5, 4])\n",
        "z = torch.tensor([0.25, 0.6, 6])\n",
        "z_q = fsq.quantize(z)\n",
        "\n",
        "print(f'{z} -> {z_q}')\n",
        "\n",
        "idx = fsq.code_to_idxs(z_q)\n",
        "print(f'code {z_q} is the {idx}-th index')\n",
        "\n",
        "code = fsq.idxs_to_code(idx)\n",
        "print(f'idx {idx} mapped back to {code}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for i in range(fsq.codebook_size + 1):\n",
        "    print(f'{i} -> {fsq.idxs_to_code(torch.tensor([i]))}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "B, C, T, H, W = 32, 5, 10, 16, 16\n",
        "z = torch.randn((B, C, T, H, W))\n",
        "fsq = FSQ(levels=[5 for _ in range(C)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "z_c_last = z.permute(0, 2, 3, 4, 1).contiguous()\n",
        "z_c_last.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "z_flatten = z_c_last.reshape(-1, C)\n",
        "z_flatten.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "z_flatten[51]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "z_flatten_q = fsq.quantize(z_flatten)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "z_flatten_q[51]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "z_q = z_flatten_q.reshape(B, T, H, W, C).permute(0, 4, 1, 2, 3).contiguous()\n",
        "z_q.shape"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
