{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NopJ0hY5u6vj"
      },
      "source": [
        "The goal here is to build a FSQ-VAE which builds latent vectors for spatio-temporal \"tublets\" described in the ViViT. Sequences of tublet latent vectors are to then be modeled by a Transformer Decoder\n",
        "\n",
        "![](https://i.imgur.com/9G7QTfV.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install -q wandb pytorch_lightning av imageio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "hXn2eB7nRgLD"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import gc\n",
        "import math\n",
        "import wandb\n",
        "import imageio\n",
        "import torch\n",
        "import torchvision\n",
        "import numpy as np\n",
        "import pytorch_lightning as pl\n",
        "from tqdm.notebook import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.animation import FuncAnimation\n",
        "from IPython.display import display, HTML\n",
        "from torch import nn, optim\n",
        "from torch.nn import functional as F\n",
        "from torchvision.datasets.video_utils import VideoClips\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torchvision.transforms import Compose, Lambda, Resize, ToTensor, CenterCrop, Grayscale\n",
        "import torchvision.transforms.functional as TF\n",
        "\n",
        "from pytorch_lightning.loggers import WandbLogger\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint, LearningRateMonitor, EarlyStopping\n",
        "from pytorch_lightning.tuner import Tuner\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR, OneCycleLR\n",
        "\n",
        "pl.seed_everything(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "steamboat_willie_gdrive_path = '/content/drive/My Drive/SteamboatWillie/SteamboatWillie.mp4'\n",
        "!cp -r /content/drive/My\\ Drive/SteamboatWillie/clips ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class RandomHorizontalFlipVideo(torch.nn.Module):\n",
        "    def __init__(self, p=0.5):\n",
        "        super().__init__()\n",
        "        self.p = p\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape is expected to be (C, T, H, W)\n",
        "        if torch.rand(1) < self.p:\n",
        "            # Flip all frames in the clip\n",
        "            return x.flip(-1)\n",
        "        return x\n",
        "\n",
        "\n",
        "class SteamboatWillieDataset(Dataset):\n",
        "    def __init__(self, config, mode='train', train_split=0.8):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.train_split = train_split\n",
        "        self.mode = mode\n",
        "\n",
        "        self.preprocess_transforms = Compose([\n",
        "                Lambda(lambda x: x.permute(0, 3, 1, 2)),   # (T, H, W, C) to (T, C, H, W) for Greyscale\n",
        "                Grayscale(num_output_channels=1),          # Convert to grayscale\n",
        "                Lambda(lambda x: x.permute(1, 0, 2, 3)),   # (T, C, H, W) to (C, T, H, W) for Conv3d\n",
        "                CenterCrop((480, 575)),                    # Center crop to remove virtical bars\n",
        "                Resize((config.img_size, config.img_size))\n",
        "        ])\n",
        "\n",
        "        self.postprocess_transforms = Compose([\n",
        "            Lambda(lambda x: x / 255.),\n",
        "            Lambda(lambda x: x.view(self.config.in_channels, self.config.clip_length, self.config.img_size, self.config.img_size))\n",
        "        ])\n",
        "\n",
        "        if self.mode == 'train':\n",
        "            self.postprocess_transforms.transforms.append(RandomHorizontalFlipVideo(p=0.5))\n",
        "\n",
        "        if os.path.exists(config.dest_dir):\n",
        "            clip_paths = self.build_existing_clip_paths(config.dest_dir)\n",
        "            self.clips = self.build_clip_refs(clip_paths)\n",
        "        else:\n",
        "            video_clips = VideoClips(\n",
        "                config.paths,\n",
        "                clip_length_in_frames=config.clip_length,\n",
        "                frames_between_clips=config.clip_length\n",
        "            )\n",
        "\n",
        "            self.clips = self.build_clip_refs(self.build_clip_paths(video_clips, self.preprocess_transforms, config.dest_dir))\n",
        "\n",
        "        if mode in ['train', 'val']:\n",
        "            total_clips = len(self.clips)\n",
        "\n",
        "            indices = torch.randperm(total_clips).tolist()\n",
        "            train_size = int(total_clips * train_split)\n",
        "\n",
        "            if mode == 'train':\n",
        "                self.clip_indices = indices[:train_size]\n",
        "            else:\n",
        "                self.clip_indices = indices[train_size:]\n",
        "        else:\n",
        "            self.clip_indices = list(range(len(self.clips)))\n",
        "\n",
        "    def build_clip_paths(self, video_clips, transforms, dest_dir):\n",
        "        \"\"\"\n",
        "        Build set of binary files to store processed video clips\n",
        "        returns dict of clip_idx -> mmapped file path\n",
        "        \"\"\"\n",
        "        clip_paths = {}\n",
        "\n",
        "        if not os.path.exists(dest_dir):\n",
        "            os.makedirs(dest_dir)\n",
        "\n",
        "        for idx in tqdm(range(video_clips.num_clips()), desc='Creating clip .bin files'):\n",
        "            # transform clips and write to mmap file\n",
        "            clip, _, _, _ = video_clips.get_clip(idx)\n",
        "            clip = self.preprocess_transforms(clip)\n",
        "            clip_np = clip.numpy().astype(np.uint8)\n",
        "\n",
        "            mmapped_file_path = os.path.join(dest_dir, f'clip_{idx}.bin')\n",
        "            fp = np.memmap(mmapped_file_path, dtype='uint8', mode='w+', shape=clip_np.shape)\n",
        "            fp[:] = clip_np[:]\n",
        "            fp.flush()\n",
        "            del fp\n",
        "            clip_paths[idx] = mmapped_file_path\n",
        "\n",
        "        return clip_paths\n",
        "\n",
        "    def build_existing_clip_paths(self, dest_dir):\n",
        "        \"\"\"\"\n",
        "        returns dict of clip_idx -> mmapped file path\n",
        "        from existing .bin files\n",
        "        \"\"\"\n",
        "        clips_paths = {}\n",
        "        for filename in os.listdir(dest_dir):\n",
        "            if filename.startswith('clip_') and filename.endswith('.bin'):\n",
        "                idx = int(filename.split('_')[1].split('.')[0])\n",
        "                file_path = os.path.join(dest_dir, filename)\n",
        "                clips_paths[idx] = file_path\n",
        "\n",
        "        return clips_paths\n",
        "\n",
        "    def build_clip_refs(self, clip_paths):\n",
        "        \"\"\"\n",
        "        Build mmap reference to bin files\n",
        "        returns dict of clip_idx -> np.array mmapped to respective bin file\n",
        "        \"\"\"\n",
        "        clips = {}\n",
        "        for idx, path in tqdm(clip_paths.items(), desc='Building clip refs'):\n",
        "            clips[idx] = np.memmap(path, dtype='uint8', mode='r')\n",
        "\n",
        "        return clips\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.clip_indices)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        clip = self.clips[self.clip_indices[idx]]\n",
        "        return self.postprocess_transforms(torch.tensor(clip, dtype=torch.float32))\n",
        "\n",
        "\n",
        "class SteamboatWillieDataModule(pl.LightningDataModule):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.batch_size = config.batch_size\n",
        "        self.config = config\n",
        "\n",
        "    def setup(self, stage=None):\n",
        "        if stage == 'fit' or stage is None:\n",
        "            self.train_dataset = SteamboatWillieDataset(self.config, mode='train')\n",
        "            self.val_dataset = SteamboatWillieDataset(self.config, mode='val')\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return torch.utils.data.DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True, num_workers=self.config.num_workers)\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return torch.utils.data.DataLoader(self.val_dataset, batch_size=self.batch_size, shuffle=False, num_workers=self.config.num_workers)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(\n",
        "            self,\n",
        "            in_channels,\n",
        "            hidden_channels,\n",
        "            out_channels,\n",
        "            nlayers=3\n",
        "        ):\n",
        "        super().__init__()\n",
        "        self.downsample_blocks = nn.Sequential(*[\n",
        "            nn.Sequential(\n",
        "                nn.Conv3d(\n",
        "                    in_channels=in_channels if i==0 else hidden_channels,\n",
        "                    out_channels=out_channels if i==nlayers-1 else hidden_channels,\n",
        "                    kernel_size=(3, 3, 3),\n",
        "                    stride=(2, 2, 2),\n",
        "                    padding=(1, 1, 1)\n",
        "                ),\n",
        "                nn.BatchNorm3d(out_channels if i==nlayers-1 else hidden_channels),\n",
        "                nn.ReLU()\n",
        "            ) for i in range(nlayers)\n",
        "        ])\n",
        "\n",
        "    def forward(self, x):\n",
        "        # ((B * n_t * n_h * n_w), C, t, p, p)\n",
        "        x = self.downsample_blocks(x)\n",
        "        # ((B * n_t * n_h * n_w), C, 1, 2, 2) -> ((B * n_t * n_h * n_w), (C * 1 * 2 * 2))\n",
        "        x = torch.flatten(x, start_dim=1)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Decoder(torch.nn.Module):\n",
        "    def __init__(\n",
        "            self,\n",
        "            in_channels,\n",
        "            hidden_channels,\n",
        "            out_channels,\n",
        "            nlayers=3,\n",
        "        ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.upsample_blocks = nn.Sequential(*[\n",
        "            nn.Sequential(\n",
        "                nn.ConvTranspose3d(\n",
        "                    in_channels=in_channels if i==0 else hidden_channels,\n",
        "                    out_channels=out_channels if i==nlayers-1 else hidden_channels,\n",
        "                    kernel_size=(3, 3, 3),\n",
        "                    stride=(2, 2, 2),\n",
        "                    padding=(1, 1, 1),\n",
        "                    output_padding=(1, 1, 1)\n",
        "                    ),\n",
        "                nn.BatchNorm3d(out_channels if i==nlayers-1 else hidden_channels) if i<nlayers-1 else nn.Identity(),\n",
        "                nn.ReLU() if i<nlayers-1 else nn.Identity()\n",
        "            ) for i in range(nlayers)\n",
        "        ])\n",
        "\n",
        "        self.out_act = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # ((B * n_t * n_h * n_w), (C * 1 * 2 * 2)) -> ((B * n_t * n_h * n_w), C, 1, 2, 2)\n",
        "        x = x.reshape(x.shape[0], -1, 1, 2, 2)\n",
        "        # ((B * n_t * n_h * n_w), C, 1, 2, 2)\n",
        "        x = self.upsample_blocks(x)\n",
        "        x = self.out_act(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class FSQ(nn.Module):\n",
        "    def __init__(self, levels, eps=1e-3):\n",
        "        super().__init__()\n",
        "        self.register_buffer('levels', torch.tensor(levels))\n",
        "        self.register_buffer(\n",
        "            'basis',\n",
        "            torch.cumprod(torch.tensor([1] + levels[:-1]), dim=0, dtype=torch.int32)\n",
        "        )\n",
        "\n",
        "        self.eps = eps\n",
        "        self.codebook_size = torch.prod(self.levels)\n",
        "\n",
        "        # self.register_buffer('implicit_codebook', self.idxs_to_code(torch.arange(self.codebook_size)))\n",
        "\n",
        "    def round_ste(self, z):\n",
        "        z_q = torch.round(z)\n",
        "        return z + (z_q - z).detach()\n",
        "\n",
        "    def quantize(self, z):\n",
        "        # half_l is used to determine how to scale tanh; we\n",
        "        # subtract 1 from the number of levels to account for 0\n",
        "        # being a quantization bin and tanh being symmetric around 0\n",
        "        half_l = (self.levels - 1) * (1 - self.eps) / 2\n",
        "\n",
        "        # if a given level is even, it will result in a scale for tanh\n",
        "        # which is halfway between integer values, so we offset\n",
        "        # the tanh output down by 0.5 to line it with whole integers\n",
        "        offset = torch.where(self.levels % 2 == 0, 0.5, 0.0)\n",
        "\n",
        "        # if our level is even, we want to shift the tanh input to\n",
        "        # ensure the 0 quantization bin is centered\n",
        "        shift = torch.tan(offset / half_l)\n",
        "\n",
        "        # once we have our shift and offset (in the case of an even level)\n",
        "        # we can round to the nearest integer bin and allow for STE\n",
        "        z_q = self.round_ste(torch.tanh(z + shift) * half_l - offset)\n",
        "\n",
        "        # after quantization, we want to renormalize the quantized\n",
        "        # values to be within the range expected by the model (ie. [-1, 1])\n",
        "        half_width = self.levels // 2\n",
        "        return z_q / half_width\n",
        "\n",
        "    def scale_and_shift(self, z_q_normalized):\n",
        "        half_width = self.levels // 2\n",
        "        return (z_q_normalized * half_width) + half_width\n",
        "\n",
        "    def scale_and_shift_inverse(self, z_q):\n",
        "        half_width = self.levels // 2\n",
        "        return (z_q - half_width) / half_width\n",
        "\n",
        "    def code_to_idxs(self, z_q):\n",
        "        z_q = self.scale_and_shift(z_q)\n",
        "        return (z_q * self.basis).sum(dim=-1).to(torch.int32)\n",
        "\n",
        "    def idxs_to_code(self, idxs):\n",
        "        idxs = idxs.unsqueeze(-1)\n",
        "        codes_not_centered = (idxs // self.basis) % self.levels\n",
        "        return self.scale_and_shift_inverse(codes_not_centered)\n",
        "\n",
        "    def forward(self, z):\n",
        "        # TODO: make this work for generic tensor sizes\n",
        "        # TODO: use einops to clean up\n",
        "        \n",
        "        # B, C, T, H, W = z.shape\n",
        "\n",
        "        # # (B, C, T, H, W) -> (B, T, H, W, C)\n",
        "        # z_c_last = z.permute(0, 2, 3, 4, 1).contiguous()\n",
        "\n",
        "        # # (B, T, H, W, C) -> (BTHW, C)\n",
        "        # z_flatten = z_c_last.reshape(-1, C)\n",
        "\n",
        "        # z_flatten_q = self.quantize(z_flatten)\n",
        "\n",
        "        # # (BTHW, C) -> (B, T, H, W, C) -> (B, C, T, H, W)\n",
        "        # z_q = z_flatten_q.reshape(B, T, H, W, C).permute(0, 4, 1, 2, 3).contiguous()\n",
        "        \n",
        "        # z already of shape (B, C)\n",
        "        z_q = self.quantize(z)\n",
        "\n",
        "        return {'z_q': z_q}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TubeletFSQVAE(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.encoder = Encoder(\n",
        "            in_channels     = config.in_channels,\n",
        "            hidden_channels = config.hidden_channels,\n",
        "            out_channels    = config.latent_channels,\n",
        "        )\n",
        "        self.decoder = Decoder(\n",
        "            in_channels     = config.latent_channels,\n",
        "            hidden_channels = config.hidden_channels,\n",
        "            out_channels    = config.in_channels\n",
        "        )\n",
        "\n",
        "        self.quantizer = FSQ(config.levels)\n",
        "\n",
        "        self.config = config\n",
        "\n",
        "    def encode(self, inputs):\n",
        "        inputs = self.extract_tubelets(inputs)\n",
        "        return self.encoder(inputs)\n",
        "\n",
        "    def quantize(self, z):\n",
        "        return self.quantizer(z)\n",
        "\n",
        "    def decode(self, z_q):\n",
        "        return self.decoder(z_q)\n",
        "\n",
        "    def extract_tubelets(self, x, t=8, p=16):\n",
        "        '''\n",
        "        extract tubelet sequence of shape ((B * n_t * n_h * n_w), C, t, p, p)\n",
        "        from a video tensor of shape (B, C, T, H, W)\n",
        "        where n_t = T // t, n_h = H // p, and n_w = W // p\n",
        "        '''\n",
        "        assert len(x.shape) == 5, 'vid.shape must be (B, C, T, H, W)'\n",
        "\n",
        "        B, C, T, H, W = x.shape\n",
        "\n",
        "        assert T % t == 0, 't must divide T (vid.shape[2]) evenly'\n",
        "        assert H % p == 0, 'p must divide H (vid.shape[3]) evenly'\n",
        "        assert W % p == 0, 'p must divide W (vid.shape[4]) evenly'\n",
        "\n",
        "        n_t, n_h, n_w = T // t, H // p, W // p\n",
        "\n",
        "        # (B, C, T, H, W) -> (B, C, n_t, t, n_h, p, n_w, p)\n",
        "        x = x.reshape(B, C, n_t, t, n_h, p, n_w, p)\n",
        "        # (B, C, n_t, t, n_h, p, n_w, p) -> (B, n_t, n_h, n_w, C, t, p, p)\n",
        "        x = x.permute(0, 2, 4, 6, 1, 3, 5, 7).contiguous()\n",
        "        # (B, n_t, n_h, n_w, C, t, p, p) -> ((B * n_t * n_h * n_w), C, t, p, p)\n",
        "        x = x.reshape(-1, C, t, p, p)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def loss(self, x_hat, x, quantized):\n",
        "        MSE = F.mse_loss(x_hat, x)\n",
        "\n",
        "        return {\n",
        "            'loss': MSE,\n",
        "            **quantized\n",
        "        }\n",
        "\n",
        "    def forward(self, x):\n",
        "        z = self.encode(x)\n",
        "        quantized = self.quantize(z)\n",
        "        x_hat = self.decode(quantized['z_q'])\n",
        "        losses = self.loss(x_hat, self.extract_tubelets(x), quantized)\n",
        "\n",
        "        return {'x_hat': x_hat, **losses}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_tubelets(x, t=8, p=16):\n",
        "    '''\n",
        "    extract tubelet sequence of shape ((B * n_t * n_h * n_w), C, t, p, p)\n",
        "    from a video tensor of shape (B, C, T, H, W)\n",
        "    where n_t = T // t, n_h = H // p, and n_w = W // p\n",
        "    '''\n",
        "    assert len(x.shape) == 5, 'vid.shape must be (B, C, T, H, W)'\n",
        "\n",
        "    B, C, T, H, W = x.shape\n",
        "\n",
        "    assert T % t == 0, 't must divide T (vid.shape[2]) evenly'\n",
        "    assert H % p == 0, 'p must divide H (vid.shape[3]) evenly'\n",
        "    assert W % p == 0, 'p must divide W (vid.shape[4]) evenly'\n",
        "\n",
        "    n_t, n_h, n_w = T // t, H // p, W // p\n",
        "\n",
        "    # (B, C, T, H, W) -> (B, C, n_t, t, n_h, p, n_w, p)\n",
        "    x = x.reshape(B, C, n_t, t, n_h, p, n_w, p)\n",
        "    # (B, C, n_t, t, n_h, p, n_w, p) -> (B, n_t, n_h, n_w, C, t, p, p)\n",
        "    x = x.permute(0, 2, 4, 6, 1, 3, 5, 7).contiguous()\n",
        "    # (B, n_t, n_h, n_w, C, t, p, p) -> ((B * n_t * n_h * n_w), C, t, p, p)\n",
        "    x = x.reshape(-1, C, t, p, p)\n",
        "\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Lightning Module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TubeletFSQVAE(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.encoder = Encoder(\n",
        "            in_channels     = config.in_channels,\n",
        "            hidden_channels = config.hidden_channels,\n",
        "            out_channels    = config.latent_channels,\n",
        "        )\n",
        "        self.decoder = Decoder(\n",
        "            in_channels     = config.latent_channels,\n",
        "            hidden_channels = config.hidden_channels,\n",
        "            out_channels    = config.in_channels\n",
        "        )\n",
        "\n",
        "        self.quantizer = FSQ(config.levels)\n",
        "\n",
        "        self.config = config\n",
        "\n",
        "    def encode(self, inputs):\n",
        "        inputs = self.extract_tubelets(inputs)\n",
        "        return self.encoder(inputs)\n",
        "\n",
        "    def quantize(self, z):\n",
        "        return self.quantizer(z)\n",
        "\n",
        "    def decode(self, z_q):\n",
        "        return self.decoder(z_q)\n",
        "\n",
        "    def extract_tubelets(self, x, t=8, p=16):\n",
        "        '''\n",
        "        extract tubelet sequence of shape ((B * n_t * n_h * n_w), C, t, p, p)\n",
        "        from a video tensor of shape (B, C, T, H, W)\n",
        "        where n_t = T // t, n_h = H // p, and n_w = W // p\n",
        "        '''\n",
        "        assert len(x.shape) == 5, 'vid.shape must be (B, C, T, H, W)'\n",
        "\n",
        "        B, C, T, H, W = x.shape\n",
        "\n",
        "        assert T % t == 0, 't must divide T (vid.shape[2]) evenly'\n",
        "        assert H % p == 0, 'p must divide H (vid.shape[3]) evenly'\n",
        "        assert W % p == 0, 'p must divide W (vid.shape[4]) evenly'\n",
        "\n",
        "        n_t, n_h, n_w = T // t, H // p, W // p\n",
        "\n",
        "        # (B, C, T, H, W) -> (B, C, n_t, t, n_h, p, n_w, p)\n",
        "        x = x.reshape(B, C, n_t, t, n_h, p, n_w, p)\n",
        "        # (B, C, n_t, t, n_h, p, n_w, p) -> (B, n_t, n_h, n_w, C, t, p, p)\n",
        "        x = x.permute(0, 2, 4, 6, 1, 3, 5, 7).contiguous()\n",
        "        # (B, n_t, n_h, n_w, C, t, p, p) -> ((B * n_t * n_h * n_w), C, t, p, p)\n",
        "        x = x.reshape(-1, C, t, p, p)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def loss(self, x_hat, x, quantized):\n",
        "        MSE = F.mse_loss(x_hat, x)\n",
        "\n",
        "        return {\n",
        "            'loss': MSE,\n",
        "            **quantized\n",
        "        }\n",
        "\n",
        "    def forward(self, x):\n",
        "        z = self.encode(x)\n",
        "        quantized = self.quantize(z)\n",
        "        x_hat = self.decode(quantized['z_q'])\n",
        "        losses = self.loss(x_hat, self.extract_tubelets(x), quantized)\n",
        "\n",
        "        return {'x_hat': x_hat, **losses}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class VideoVAEConfig:\n",
        "    def __init__(self):\n",
        "        self.project_name = 'Tubelet FSQ-VAE Steamboat Willie'\n",
        "        # dataset properties\n",
        "        self.paths = ['/content/drive/My Drive/SteamboatWillie/SteamboatWillie.mp4']\n",
        "        self.dest_dir = './clips/'\n",
        "        # model checkpoints\n",
        "        self.checkpoint_path = \"./checkpoints\"\n",
        "        self.save_top_k = 1\n",
        "        # training\n",
        "        self.train_split = 0.8\n",
        "        self.batch_size = 16\n",
        "        self.max_epochs = 500\n",
        "        self.training_steps = 100000\n",
        "        self.num_workers = 2\n",
        "        # optimizer\n",
        "        self.lr = 5e-2\n",
        "        self.beta1 = 0.9\n",
        "        self.beta2 = 0.999\n",
        "        self.weight_decay = 0.0 # 1e-2\n",
        "        self.use_wd_schedule = False\n",
        "        self.use_lr_schedule = True\n",
        "        # input properties\n",
        "        self.clip_length = 16\n",
        "        self.img_size = 256\n",
        "        self.in_channels = 1\n",
        "        # quantization\n",
        "        self.quant_mode = 'fsq' # 'vq'\n",
        "        self.latent_channels = 128 # 8\n",
        "        self.codebook_size = 512\n",
        "        self.commit_loss_beta = 0.25\n",
        "        self.track_codebook = True\n",
        "        self.use_ema = True\n",
        "        self.ema_gamma = 0.99\n",
        "        self.level = 7\n",
        "        self.levels = [self.level for _ in range(self.latent_channels * 4)]\n",
        "        # encoder/decoder\n",
        "        self.hidden_channels = 256\n",
        "        self.start_channels = 32\n",
        "        self.nblocks = 5\n",
        "        self.nlayers = 3\n",
        "\n",
        "    def update(self, updates):\n",
        "        for key, value in updates.items():\n",
        "            if hasattr(self, key):\n",
        "                setattr(self, key, value)\n",
        "\n",
        "    def to_dict(self):\n",
        "        return {k: v for k, v in self.__dict__.items()}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Component Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "config = VideoVAEConfig()\n",
        "data_module = SteamboatWillieDataModule(config)\n",
        "data_module.prepare_data()\n",
        "data_module.setup()\n",
        "\n",
        "train_loader = data_module.train_dataloader()\n",
        "train_batch = next(iter(train_loader))\n",
        "print(f'train_batch.shape: {train_batch.shape}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tublets = extract_tubelets(train_batch)\n",
        "print(tublets.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "enc=Encoder(in_channels=1,\n",
        "            hidden_channels=32,\n",
        "            out_channels=5)\n",
        "fsq=FSQ(levels=[5, 5, 5, 5, 5])\n",
        "dec=Decoder(in_channels=5,\n",
        "            hidden_channels=32,\n",
        "            out_channels=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tubelets_enc = enc(tublets)\n",
        "print(tubelets_enc.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tubelets_fsq = fsq(tubelets_enc)\n",
        "print(tubelets_fsq['z_q'].shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tubelets_dec = dec(tubelets_fsq['z_q'])\n",
        "print(tubelets_dec.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Display Clips"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def display_clip(clip):\n",
        "    def update(frame_idx):\n",
        "        ax.clear()\n",
        "        ax.imshow(video_clip_np[frame_idx], cmap='gray')\n",
        "        ax.axis('off')\n",
        "\n",
        "    video_clip_np = clip.permute(1, 2, 3, 0).numpy()\n",
        "    fig, ax = plt.subplots()\n",
        "    ani = FuncAnimation(fig, update, frames=range(video_clip_np.shape[0]), interval=50)\n",
        "    plt.close()\n",
        "    display(HTML(ani.to_html5_video()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tublets = extract_tubelets(train_batch)\n",
        "print(tublets.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "display_clip(tublets[1001])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Initial Tubelet Shape Exploration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# (B, C, T, H, W) -> (B, C, n_t, t, n_h, p, n_w, p)\n",
        "tubelets = vid.reshape(B, C, n_t, t, n_h, p, n_w, p)\n",
        "print(tubelets.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# (B, C, n_t, t, n_h, p, n_w, p) -> (B, n_t, n_h, n_w, C, t, p, p)\n",
        "tubelets = tubelets.permute(0, 2, 4, 6, 1, 3, 5, 7).contiguous()\n",
        "print(tubelets.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# (B, n_t, n_h, n_w, C, t, p, p) -> ((B * n_t * n_h * n_w), C, t, p, p)\n",
        "tubelets = tubelets.reshape(-1, C, t, p, p)\n",
        "print(tubelets.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "conv3d_1 = nn.Conv3d(in_channels=C, out_channels=32, kernel_size=3, stride=2, padding=1)\n",
        "out1 = conv3d_1(tubelets)\n",
        "print(out1.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "conv3d_2 = nn.Conv3d(in_channels=32, out_channels=32, kernel_size=3, stride=2, padding=1)\n",
        "out2 = conv3d_2(out1)\n",
        "print(out2.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "conv3d_3 = nn.Conv3d(in_channels=32, out_channels=32, kernel_size=3, stride=2, padding=1)\n",
        "out3 = conv3d_3(out2)\n",
        "print(out3.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "conv3d_4 = nn.Conv3d(in_channels=32, out_channels=32, kernel_size=3, stride=(1, 2, 2), padding=1)\n",
        "out4 = conv3d_4(out3)\n",
        "print(out4.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "out5 = out4.reshape(-1, 32)\n",
        "print(out5.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# separate out batch dimension post VAE encoding since dim=0 is of shape (B * n_t * n_h * n_w)\n",
        "out6 = out5.reshape(out5.shape[0] // (n_t * n_h * n_w), -1, 32)\n",
        "print(out6.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "out7 = out6.reshape(-1, 32)\n",
        "print(out7.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "out7 = out7.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1)\n",
        "print(out7.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "decoder1 = nn.ConvTranspose3d(in_channels=32, out_channels=32, kernel_size=3, stride=(1, 2, 2), padding=1, output_padding=(0, 1, 1))\n",
        "out8 = decoder1(out7)\n",
        "print(out8.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "decoder2 = nn.ConvTranspose3d(in_channels=32, out_channels=32, kernel_size=3, stride=2, padding=1, output_padding=(1, 1, 1))\n",
        "out9 = decoder2(out8)\n",
        "print(out9.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "decoder3 = nn.ConvTranspose3d(in_channels=32, out_channels=32, kernel_size=3, stride=2, padding=1, output_padding=(1, 1, 1))\n",
        "out10 = decoder2(out9)\n",
        "print(out10.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "decoder3 = nn.ConvTranspose3d(in_channels=32, out_channels=32, kernel_size=3, stride=2, padding=1, output_padding=(1, 1, 1))\n",
        "out11 = decoder2(out10)\n",
        "print(out11.shape)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
