{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NopJ0hY5u6vj"
      },
      "source": [
        "The goal here is to build a FSQ-VAE which builds latent vectors for spatio-temporal \"tublets\" described in the ViViT. Sequences of tublet latent vectors are to then be modeled by a Transformer Decoder\n",
        "\n",
        "![](https://i.imgur.com/9G7QTfV.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install -q wandb pytorch_lightning av imageio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "hXn2eB7nRgLD"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import gc\n",
        "import math\n",
        "import wandb\n",
        "import imageio\n",
        "import torch\n",
        "import torchvision\n",
        "import numpy as np\n",
        "import pytorch_lightning as pl\n",
        "from tqdm.notebook import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.animation import FuncAnimation\n",
        "from IPython.display import display, HTML\n",
        "from torch import nn, optim\n",
        "from torch.nn import functional as F\n",
        "from torchvision.datasets.video_utils import VideoClips\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torchvision.transforms import Compose, Lambda, Resize, ToTensor, CenterCrop, Grayscale\n",
        "import torchvision.transforms.functional as TF\n",
        "\n",
        "from pytorch_lightning.loggers import WandbLogger\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint, LearningRateMonitor, EarlyStopping\n",
        "from pytorch_lightning.tuner import Tuner\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR, OneCycleLR\n",
        "\n",
        "pl.seed_everything(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "steamboat_willie_gdrive_path = '/content/drive/My Drive/SteamboatWillie/SteamboatWillie.mp4'\n",
        "!cp -r /content/drive/My\\ Drive/SteamboatWillie/clips ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class RandomHorizontalFlipVideo(torch.nn.Module):\n",
        "    def __init__(self, p=0.5):\n",
        "        super().__init__()\n",
        "        self.p = p\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape is expected to be (C, T, H, W)\n",
        "        if torch.rand(1) < self.p:\n",
        "            # Flip all frames in the clip\n",
        "            return x.flip(-1)\n",
        "        return x\n",
        "\n",
        "\n",
        "class SteamboatWillieDataset(Dataset):\n",
        "    def __init__(self, config, mode='train', train_split=0.8):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.train_split = train_split\n",
        "        self.mode = mode\n",
        "\n",
        "        self.preprocess_transforms = Compose([\n",
        "                Lambda(lambda x: x.permute(0, 3, 1, 2)),   # (T, H, W, C) to (T, C, H, W) for Greyscale\n",
        "                Grayscale(num_output_channels=1),          # Convert to grayscale\n",
        "                Lambda(lambda x: x.permute(1, 0, 2, 3)),   # (T, C, H, W) to (C, T, H, W) for Conv3d\n",
        "                CenterCrop((480, 575)),                    # Center crop to remove virtical bars\n",
        "                Resize((config.img_size, config.img_size))\n",
        "        ])\n",
        "\n",
        "        self.postprocess_transforms = Compose([\n",
        "            Lambda(lambda x: x / 255.),\n",
        "            Lambda(lambda x: x.view(self.config.in_channels, self.config.clip_length, self.config.img_size, self.config.img_size))\n",
        "        ])\n",
        "\n",
        "        if self.mode == 'train':\n",
        "            self.postprocess_transforms.transforms.append(RandomHorizontalFlipVideo(p=0.5))\n",
        "\n",
        "        if os.path.exists(config.dest_dir):\n",
        "            clip_paths = self.build_existing_clip_paths(config.dest_dir)\n",
        "            self.clips = self.build_clip_refs(clip_paths)\n",
        "        else:\n",
        "            video_clips = VideoClips(\n",
        "                config.paths,\n",
        "                clip_length_in_frames=config.clip_length,\n",
        "                frames_between_clips=config.clip_length\n",
        "            )\n",
        "\n",
        "            self.clips = self.build_clip_refs(self.build_clip_paths(video_clips, self.preprocess_transforms, config.dest_dir))\n",
        "\n",
        "        if mode in ['train', 'val']:\n",
        "            total_clips = len(self.clips)\n",
        "\n",
        "            indices = torch.randperm(total_clips).tolist()\n",
        "            train_size = int(total_clips * train_split)\n",
        "\n",
        "            if mode == 'train':\n",
        "                self.clip_indices = indices[:train_size]\n",
        "            else:\n",
        "                self.clip_indices = indices[train_size:]\n",
        "        else:\n",
        "            self.clip_indices = list(range(len(self.clips)))\n",
        "\n",
        "    def build_clip_paths(self, video_clips, transforms, dest_dir):\n",
        "        \"\"\"\n",
        "        Build set of binary files to store processed video clips\n",
        "        returns dict of clip_idx -> mmapped file path\n",
        "        \"\"\"\n",
        "        clip_paths = {}\n",
        "\n",
        "        if not os.path.exists(dest_dir):\n",
        "            os.makedirs(dest_dir)\n",
        "\n",
        "        for idx in tqdm(range(video_clips.num_clips()), desc='Creating clip .bin files'):\n",
        "            # transform clips and write to mmap file\n",
        "            clip, _, _, _ = video_clips.get_clip(idx)\n",
        "            clip = self.preprocess_transforms(clip)\n",
        "            clip_np = clip.numpy().astype(np.uint8)\n",
        "\n",
        "            mmapped_file_path = os.path.join(dest_dir, f'clip_{idx}.bin')\n",
        "            fp = np.memmap(mmapped_file_path, dtype='uint8', mode='w+', shape=clip_np.shape)\n",
        "            fp[:] = clip_np[:]\n",
        "            fp.flush()\n",
        "            del fp\n",
        "            clip_paths[idx] = mmapped_file_path\n",
        "\n",
        "        return clip_paths\n",
        "\n",
        "    def build_existing_clip_paths(self, dest_dir):\n",
        "        \"\"\"\"\n",
        "        returns dict of clip_idx -> mmapped file path\n",
        "        from existing .bin files\n",
        "        \"\"\"\n",
        "        clips_paths = {}\n",
        "        for filename in os.listdir(dest_dir):\n",
        "            if filename.startswith('clip_') and filename.endswith('.bin'):\n",
        "                idx = int(filename.split('_')[1].split('.')[0])\n",
        "                file_path = os.path.join(dest_dir, filename)\n",
        "                clips_paths[idx] = file_path\n",
        "\n",
        "        return clips_paths\n",
        "\n",
        "    def build_clip_refs(self, clip_paths):\n",
        "        \"\"\"\n",
        "        Build mmap reference to bin files\n",
        "        returns dict of clip_idx -> np.array mmapped to respective bin file\n",
        "        \"\"\"\n",
        "        clips = {}\n",
        "        for idx, path in tqdm(clip_paths.items(), desc='Building clip refs'):\n",
        "            clips[idx] = np.memmap(path, dtype='uint8', mode='r')\n",
        "\n",
        "        return clips\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.clip_indices)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        clip = self.clips[self.clip_indices[idx]]\n",
        "        return self.postprocess_transforms(torch.tensor(clip, dtype=torch.float32))\n",
        "\n",
        "\n",
        "class SteamboatWillieDataModule(pl.LightningDataModule):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.batch_size = config.batch_size\n",
        "        self.config = config\n",
        "\n",
        "    def setup(self, stage=None):\n",
        "        if stage == 'fit' or stage is None:\n",
        "            self.train_dataset = SteamboatWillieDataset(self.config, mode='train')\n",
        "            self.val_dataset = SteamboatWillieDataset(self.config, mode='val')\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return torch.utils.data.DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True, num_workers=self.config.num_workers)\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return torch.utils.data.DataLoader(self.val_dataset, batch_size=self.batch_size, shuffle=False, num_workers=self.config.num_workers)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ResBlock(nn.Module):\n",
        "    def __init__(\n",
        "            self,\n",
        "            in_channels,\n",
        "            out_channels\n",
        "        ):\n",
        "        super().__init__()\n",
        "        if in_channels != out_channels:\n",
        "            self.identity = nn.Conv3d(\n",
        "                in_channels=in_channels,\n",
        "                out_channels=out_channels,\n",
        "                kernel_size=1\n",
        "            )\n",
        "        else:\n",
        "            self.identity = nn.Identity()\n",
        "\n",
        "        hidden_channels = in_channels // 4\n",
        "\n",
        "        self.block = nn.Sequential(\n",
        "            nn.Conv3d(\n",
        "                in_channels=in_channels,\n",
        "                out_channels=hidden_channels,\n",
        "                kernel_size=(1, 3, 3),\n",
        "                padding=(0, 1, 1)\n",
        "            ),\n",
        "            nn.BatchNorm3d(hidden_channels),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv3d(\n",
        "                in_channels=hidden_channels,\n",
        "                out_channels=hidden_channels,\n",
        "                kernel_size=(1, 3, 3),\n",
        "                padding=(0, 1, 1)\n",
        "            ),\n",
        "            nn.BatchNorm3d(hidden_channels),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv3d(\n",
        "                in_channels=hidden_channels,\n",
        "                out_channels=hidden_channels,\n",
        "                kernel_size=(1, 3, 3),\n",
        "                padding=(0, 1, 1)\n",
        "            ),\n",
        "            nn.BatchNorm3d(hidden_channels),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv3d(\n",
        "                in_channels=hidden_channels,\n",
        "                out_channels=out_channels,\n",
        "                kernel_size=1\n",
        "            ),\n",
        "            nn.BatchNorm3d(out_channels)\n",
        "        )\n",
        "        self.res_act = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.block(x) + self.identity(x)\n",
        "        return self.res_act(out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(\n",
        "            self,\n",
        "            in_channels,\n",
        "            hidden_channels,\n",
        "            out_channels,\n",
        "            nlayers=3,\n",
        "            nblocks=2\n",
        "        ):\n",
        "        super().__init__()\n",
        "        self.downsample_blocks = nn.Sequential(*[\n",
        "            nn.Sequential(\n",
        "                nn.Conv3d(\n",
        "                    in_channels=in_channels if i==0 else hidden_channels,\n",
        "                    out_channels=hidden_channels,\n",
        "                    kernel_size=(3, 3, 3),\n",
        "                    stride=(2, 2, 2),\n",
        "                    padding=(1, 1, 1)\n",
        "                ),\n",
        "                nn.BatchNorm3d(hidden_channels),\n",
        "                nn.ReLU()\n",
        "            ) for i in range(nlayers)\n",
        "        ])\n",
        "        self.res_blocks = nn.Sequential(*[\n",
        "            ResBlock(\n",
        "                in_channels=hidden_channels,\n",
        "                out_channels=out_channels if i==nblocks-1 else hidden_channels\n",
        "            ) for i in range(nblocks)\n",
        "        ])\n",
        "\n",
        "    def forward(self, x):\n",
        "        # ((B * n_t * n_h * n_w), C, t, p, p)\n",
        "        x = self.downsample_blocks(x)\n",
        "        x = self.res_blocks(x)\n",
        "        # ((B * n_t * n_h * n_w), C, 1, 2, 2) -> ((B * n_t * n_h * n_w), (C * 1 * 2 * 2))\n",
        "        x = torch.flatten(x, start_dim=1)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Decoder(torch.nn.Module):\n",
        "    def __init__(\n",
        "            self,\n",
        "            in_channels,\n",
        "            hidden_channels,\n",
        "            out_channels,\n",
        "            nlayers=3,\n",
        "            nblocks=2\n",
        "        ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.res_blocks = nn.Sequential(*[\n",
        "            ResBlock(\n",
        "                in_channels=in_channels if i==0 else hidden_channels,\n",
        "                out_channels=hidden_channels,\n",
        "            )\n",
        "            for i in range(nblocks)\n",
        "        ])\n",
        "\n",
        "        self.upsample_blocks = nn.Sequential(*[\n",
        "            nn.Sequential(\n",
        "                nn.ConvTranspose3d(\n",
        "                    in_channels=hidden_channels,\n",
        "                    out_channels=out_channels if i==nlayers-1 else hidden_channels,\n",
        "                    kernel_size=(3, 3, 3),\n",
        "                    stride=(2, 2, 2),\n",
        "                    padding=(1, 1, 1),\n",
        "                    output_padding=(1, 1, 1)\n",
        "                    ),\n",
        "                nn.BatchNorm3d(hidden_channels) if i<nlayers-1 else nn.Identity(),\n",
        "                nn.ReLU() if i<nlayers-1 else nn.Identity()\n",
        "            ) for i in range(nlayers)\n",
        "        ])\n",
        "\n",
        "        self.out_act = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # ((B * n_t * n_h * n_w), (C * 1 * 2 * 2)) -> ((B * n_t * n_h * n_w), C, 1, 2, 2)\n",
        "        x = x.reshape(x.shape[0], -1, 1, 2, 2)\n",
        "        x = self.res_blocks(x)\n",
        "        # ((B * n_t * n_h * n_w), C, 1, 2, 2)\n",
        "        x = self.upsample_blocks(x)\n",
        "        x = self.out_act(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class FSQ(nn.Module):\n",
        "    def __init__(self, levels, eps=1e-3):\n",
        "        super().__init__()\n",
        "        self.register_buffer('levels', torch.tensor(levels))\n",
        "        self.register_buffer(\n",
        "            'basis',\n",
        "            torch.cumprod(torch.tensor([1] + levels[:-1]), dim=0, dtype=torch.int32)\n",
        "        )\n",
        "\n",
        "        self.eps = eps\n",
        "        self.codebook_size = torch.prod(self.levels)\n",
        "\n",
        "        # self.register_buffer('implicit_codebook', self.idxs_to_code(torch.arange(self.codebook_size)))\n",
        "\n",
        "    def round_ste(self, z):\n",
        "        z_q = torch.round(z)\n",
        "        return z + (z_q - z).detach()\n",
        "\n",
        "    def quantize(self, z):\n",
        "        # half_l is used to determine how to scale tanh; we\n",
        "        # subtract 1 from the number of levels to account for 0\n",
        "        # being a quantization bin and tanh being symmetric around 0\n",
        "        half_l = (self.levels - 1) * (1 - self.eps) / 2\n",
        "\n",
        "        # if a given level is even, it will result in a scale for tanh\n",
        "        # which is halfway between integer values, so we offset\n",
        "        # the tanh output down by 0.5 to line it with whole integers\n",
        "        offset = torch.where(self.levels % 2 == 0, 0.5, 0.0)\n",
        "\n",
        "        # if our level is even, we want to shift the tanh input to\n",
        "        # ensure the 0 quantization bin is centered\n",
        "        shift = torch.tan(offset / half_l)\n",
        "\n",
        "        # once we have our shift and offset (in the case of an even level)\n",
        "        # we can round to the nearest integer bin and allow for STE\n",
        "        z_q = self.round_ste(torch.tanh(z + shift) * half_l - offset)\n",
        "\n",
        "        # after quantization, we want to renormalize the quantized\n",
        "        # values to be within the range expected by the model (ie. [-1, 1])\n",
        "        half_width = self.levels // 2\n",
        "        return z_q / half_width\n",
        "\n",
        "    def scale_and_shift(self, z_q_normalized):\n",
        "        half_width = self.levels // 2\n",
        "        return (z_q_normalized * half_width) + half_width\n",
        "\n",
        "    def scale_and_shift_inverse(self, z_q):\n",
        "        half_width = self.levels // 2\n",
        "        return (z_q - half_width) / half_width\n",
        "\n",
        "    def code_to_idxs(self, z_q):\n",
        "        z_q = self.scale_and_shift(z_q)\n",
        "        return (z_q * self.basis).sum(dim=-1).to(torch.int32)\n",
        "\n",
        "    def idxs_to_code(self, idxs):\n",
        "        idxs = idxs.unsqueeze(-1)\n",
        "        codes_not_centered = (idxs // self.basis) % self.levels\n",
        "        return self.scale_and_shift_inverse(codes_not_centered)\n",
        "\n",
        "    def forward(self, z):\n",
        "        # TODO: make this work for generic tensor sizes\n",
        "        # TODO: use einops to clean up\n",
        "        \n",
        "        # B, C, T, H, W = z.shape\n",
        "\n",
        "        # # (B, C, T, H, W) -> (B, T, H, W, C)\n",
        "        # z_c_last = z.permute(0, 2, 3, 4, 1).contiguous()\n",
        "\n",
        "        # # (B, T, H, W, C) -> (BTHW, C)\n",
        "        # z_flatten = z_c_last.reshape(-1, C)\n",
        "\n",
        "        # z_flatten_q = self.quantize(z_flatten)\n",
        "\n",
        "        # # (BTHW, C) -> (B, T, H, W, C) -> (B, C, T, H, W)\n",
        "        # z_q = z_flatten_q.reshape(B, T, H, W, C).permute(0, 4, 1, 2, 3).contiguous()\n",
        "        \n",
        "        # z already of shape (B, C)\n",
        "        z_q = self.quantize(z)\n",
        "\n",
        "        return {'z_q': z_q}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TubeletFSQVAE(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.encoder = Encoder(\n",
        "            in_channels     = config.in_channels,\n",
        "            hidden_channels = config.hidden_channels,\n",
        "            out_channels    = config.latent_channels,\n",
        "        )\n",
        "        self.decoder = Decoder(\n",
        "            in_channels     = config.latent_channels,\n",
        "            hidden_channels = config.hidden_channels,\n",
        "            out_channels    = config.in_channels\n",
        "        )\n",
        "\n",
        "        # TODO: change self.T = config.clip_length if possible\n",
        "        self.t, self.p = config.t, config.p\n",
        "        self.B, self.C, self.T, self.H, self.W = config.batch_size, config.in_channels, config.clip_length, config.img_size, config.img_size\n",
        "        self.n_t, self.n_h, self.n_w = self.T // self.t, self.H // self.p, self.W // self.p\n",
        "\n",
        "        self.quantizer = FSQ(config.levels)\n",
        "\n",
        "        self.config = config\n",
        "\n",
        "    def encode(self, inputs):\n",
        "        inputs = self.extract_tubelets(inputs)\n",
        "        return self.encoder(inputs)\n",
        "\n",
        "    def quantize(self, z):\n",
        "        return self.quantizer(z)\n",
        "\n",
        "    def decode(self, z_q):\n",
        "        return self.decoder(z_q)\n",
        "\n",
        "    def extract_tubelets(self, x, t=8, p=16):\n",
        "        '''\n",
        "        extract tubelet sequence of shape ((B * n_t * n_h * n_w), C, t, p, p)\n",
        "        from a video tensor of shape (B, C, T, H, W)\n",
        "        where n_t = T // t, n_h = H // p, and n_w = W // p\n",
        "        '''\n",
        "        assert len(x.shape) == 5, 'vid.shape must be (B, C, T, H, W)'\n",
        "\n",
        "        B, C, T, H, W = x.shape\n",
        "\n",
        "        assert T % t == 0, 't must divide T (vid.shape[2]) evenly'\n",
        "        assert H % p == 0, 'p must divide H (vid.shape[3]) evenly'\n",
        "        assert W % p == 0, 'p must divide W (vid.shape[4]) evenly'\n",
        "\n",
        "        n_t, n_h, n_w = T // t, H // p, W // p\n",
        "\n",
        "        # (B, C, T, H, W) -> (B, C, n_t, t, n_h, p, n_w, p)\n",
        "        x = x.reshape(B, C, n_t, t, n_h, p, n_w, p)\n",
        "        # (B, C, n_t, t, n_h, p, n_w, p) -> (B, n_t, n_h, n_w, C, t, p, p)\n",
        "        x = x.permute(0, 2, 4, 6, 1, 3, 5, 7).contiguous()\n",
        "        # (B, n_t, n_h, n_w, C, t, p, p) -> ((B * n_t * n_h * n_w), C, t, p, p)\n",
        "        x = x.reshape(-1, C, t, p, p)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def assemble_tubelets(self, x):\n",
        "        # ((B * n_t * n_h * n_w), C, t, p, p) -> (B, n_t, n_h, n_w, C, t, p, p)\n",
        "        x = x.reshape(self.B, self.n_t, self.n_h, self.n_w, self.C, self.t, self.p, self.p)\n",
        "        # (B, n_t, n_h, n_w, C, t, p, p) -> (B, C, n_t, t, n_h, p, n_w, p)\n",
        "        x = x.permute(0, 4, 1, 5, 2, 6, 3, 7).contiguous()\n",
        "        # (B, C, n_t, t, n_h, p, n_w, p) -> (B, C, T, H, W)\n",
        "        x = x.reshape(self.B, self.C, self.T, self.H, self.W)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def loss(self, x_hat, x, quantized):\n",
        "        MSE = F.mse_loss(x_hat, x)\n",
        "\n",
        "        return {\n",
        "            'loss': MSE,\n",
        "            **quantized\n",
        "        }\n",
        "\n",
        "    def forward(self, x):\n",
        "        z = self.encode(x)\n",
        "        quantized = self.quantize(z)\n",
        "        x_hat = self.decode(quantized['z_q'])\n",
        "        losses = self.loss(x_hat, self.extract_tubelets(x), quantized)\n",
        "\n",
        "        return {'x_hat': x_hat, **losses}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_tubelets(x, t=8, p=16):\n",
        "    '''\n",
        "    extract tubelet sequence of shape ((B * n_t * n_h * n_w), C, t, p, p)\n",
        "    from a video tensor of shape (B, C, T, H, W)\n",
        "    where n_t = T // t, n_h = H // p, and n_w = W // p\n",
        "    '''\n",
        "    assert len(x.shape) == 5, 'vid.shape must be (B, C, T, H, W)'\n",
        "\n",
        "    B, C, T, H, W = x.shape\n",
        "\n",
        "    assert T % t == 0, 't must divide T (vid.shape[2]) evenly'\n",
        "    assert H % p == 0, 'p must divide H (vid.shape[3]) evenly'\n",
        "    assert W % p == 0, 'p must divide W (vid.shape[4]) evenly'\n",
        "\n",
        "    n_t, n_h, n_w = T // t, H // p, W // p\n",
        "\n",
        "    # (B, C, T, H, W) -> (B, C, n_t, t, n_h, p, n_w, p)\n",
        "    x = x.reshape(B, C, n_t, t, n_h, p, n_w, p)\n",
        "    # (B, C, n_t, t, n_h, p, n_w, p) -> (B, n_t, n_h, n_w, C, t, p, p)\n",
        "    x = x.permute(0, 2, 4, 6, 1, 3, 5, 7).contiguous()\n",
        "    # (B, n_t, n_h, n_w, C, t, p, p) -> ((B * n_t * n_h * n_w), C, t, p, p)\n",
        "    x = x.reshape(-1, C, t, p, p)\n",
        "\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Lightning Module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class LitTubeletFSQVAE(pl.LightningModule):\n",
        "    def __init__(self, model, config):\n",
        "        super().__init__()\n",
        "        self.model = model\n",
        "        self.config = config\n",
        "        self.lr = config.lr\n",
        "\n",
        "        if self.logger:\n",
        "            self.logger.experiment.config.update(config)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        x = batch\n",
        "        out = self(x)\n",
        "\n",
        "        self.log('train/loss', out['loss'], prog_bar=True)\n",
        "\n",
        "        return out['loss']\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        x = batch\n",
        "        out = self(x)\n",
        "\n",
        "        self.log('val/loss', out['loss'], prog_bar=True)\n",
        "\n",
        "        if batch_idx == 0:\n",
        "            out['x_hat'] = self.model.assemble_tubelets(out['x_hat'])\n",
        "            self.log_val_clips(x, out)\n",
        "\n",
        "        return out['loss']\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.AdamW(\n",
        "            self.parameters(),\n",
        "            lr=self.lr,\n",
        "            betas=(self.config.beta1, self.config.beta2),\n",
        "            weight_decay=self.config.weight_decay\n",
        "        )\n",
        "\n",
        "        if self.config.use_lr_schedule:\n",
        "            scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=self.config.max_epochs)\n",
        "            return [optimizer], [scheduler]\n",
        "\n",
        "        return optimizer\n",
        "\n",
        "    def log_val_clips(self, x, out, num_clips=2):\n",
        "        n_clips = min(x.size(0), num_clips)\n",
        "\n",
        "        for i in range(n_clips):\n",
        "            # extract the ith original and reconstructed clip\n",
        "            original_clip = x[i]  # (C, T, H, W)\n",
        "            reconstructed_clip = out['x_hat'][i]  # (C, T, H, W)\n",
        "\n",
        "            # convert tensors to numpy arrays and transpose to (T, H, W, C) for GIF creation\n",
        "            original_clip_np = original_clip.permute(1, 2, 3, 0).cpu().numpy()\n",
        "            reconstructed_clip_np = reconstructed_clip.permute(1, 2, 3, 0).cpu().numpy()\n",
        "\n",
        "            original_clip_np = (original_clip_np - original_clip_np.min()) / (original_clip_np.max() - original_clip_np.min())\n",
        "            reconstructed_clip_np = (reconstructed_clip_np - reconstructed_clip_np.min()) / (reconstructed_clip_np.max() - reconstructed_clip_np.min())\n",
        "\n",
        "            original_clip_np = (original_clip_np * 255).astype(np.uint8)\n",
        "            reconstructed_clip_np = (reconstructed_clip_np * 255).astype(np.uint8)\n",
        "\n",
        "            # grayscale videos need to be of shape (T, H, W)\n",
        "            if original_clip_np.shape[-1] == 1:\n",
        "                original_clip_np = original_clip_np.squeeze(-1)\n",
        "\n",
        "            if reconstructed_clip_np.shape[-1] == 1:\n",
        "                reconstructed_clip_np = reconstructed_clip_np.squeeze(-1)\n",
        "\n",
        "            # create GIFs for the original and reconstructed clips\n",
        "            original_gif_path = f'/tmp/original_clip_{i}.gif'\n",
        "            reconstructed_gif_path = f'/tmp/reconstructed_clip_{i}.gif'\n",
        "            imageio.mimsave(original_gif_path, original_clip_np, fps=10)\n",
        "            imageio.mimsave(reconstructed_gif_path, reconstructed_clip_np, fps=10)\n",
        "\n",
        "            # log the GIFs to wandb\n",
        "            self.logger.experiment.log({\n",
        "                f\"val/original_clip_{i}\": wandb.Video(original_gif_path, fps=10, format=\"gif\", caption=\"Original\"),\n",
        "                f\"val/reconstructed_clip_{i}\": wandb.Video(reconstructed_gif_path, fps=10, format=\"gif\", caption=\"Reconstructed\")\n",
        "            })"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class VideoVAEConfig:\n",
        "    def __init__(self):\n",
        "        self.project_name = 'Tubelet FSQ-VAE Steamboat Willie'\n",
        "        # dataset properties\n",
        "        self.paths = ['/content/drive/My Drive/SteamboatWillie/SteamboatWillie.mp4']\n",
        "        self.dest_dir = './clips/'\n",
        "        # model checkpoints\n",
        "        self.checkpoint_path = \"./checkpoints\"\n",
        "        self.save_top_k = 1\n",
        "        # training\n",
        "        self.train_split = 0.8\n",
        "        self.batch_size = 16\n",
        "        self.max_epochs = 500\n",
        "        self.training_steps = 100000\n",
        "        self.num_workers = 2\n",
        "        # optimizer\n",
        "        self.lr = 5e-2\n",
        "        self.beta1 = 0.9\n",
        "        self.beta2 = 0.999\n",
        "        self.weight_decay = 0.0 # 1e-2\n",
        "        self.use_wd_schedule = False\n",
        "        self.use_lr_schedule = False\n",
        "        # input properties\n",
        "        self.clip_length = 16\n",
        "        self.img_size = 256\n",
        "        self.in_channels = 1\n",
        "        # quantization\n",
        "        self.quant_mode = 'fsq' # 'vq'\n",
        "        self.latent_channels = 64 # 8\n",
        "        self.codebook_size = 512\n",
        "        self.commit_loss_beta = 0.25\n",
        "        self.track_codebook = True\n",
        "        self.use_ema = True\n",
        "        self.ema_gamma = 0.99\n",
        "        self.level = 9\n",
        "        self.levels = [self.level for _ in range(self.latent_channels * 4)]\n",
        "        # encoder/decoder\n",
        "        self.hidden_channels = 256\n",
        "        self.start_channels = 32\n",
        "        self.nblocks = 5\n",
        "        self.nlayers = 3\n",
        "        # tubelet\n",
        "        self.t = 8\n",
        "        self.p = 16\n",
        "\n",
        "    def update(self, updates):\n",
        "        for key, value in updates.items():\n",
        "            if hasattr(self, key):\n",
        "                setattr(self, key, value)\n",
        "                if key == 'level':\n",
        "                    self.levels = [self.level for _ in range(self.latent_channels * 4)]\n",
        "\n",
        "    def to_dict(self):\n",
        "        return {k: v for k, v in self.__dict__.items()}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Component Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "config = VideoVAEConfig()\n",
        "data_module = SteamboatWillieDataModule(config)\n",
        "data_module.prepare_data()\n",
        "data_module.setup()\n",
        "\n",
        "train_loader = data_module.train_dataloader()\n",
        "train_batch = next(iter(train_loader))\n",
        "print(f'train_batch.shape: {train_batch.shape}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tublets = extract_tubelets(train_batch)\n",
        "print(tublets.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "enc=Encoder(in_channels=1,\n",
        "            hidden_channels=32,\n",
        "            out_channels=5)\n",
        "fsq=FSQ(levels=[5, 5, 5, 5, 5])\n",
        "dec=Decoder(in_channels=5,\n",
        "            hidden_channels=32,\n",
        "            out_channels=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tubelets_enc = enc(tublets)\n",
        "print(tubelets_enc.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tubelets_fsq = fsq(tubelets_enc)\n",
        "print(tubelets_fsq['z_q'].shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tubelets_dec = dec(tubelets_fsq['z_q'])\n",
        "print(tubelets_dec.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Display Clips"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_tubelets(x, t=8, p=16):\n",
        "    '''\n",
        "    extract tubelet sequence of shape ((B * n_t * n_h * n_w), C, t, p, p)\n",
        "    from a video tensor of shape (B, C, T, H, W)\n",
        "    where n_t = T // t, n_h = H // p, and n_w = W // p\n",
        "    '''\n",
        "    assert len(x.shape) == 5, 'vid.shape must be (B, C, T, H, W)'\n",
        "\n",
        "    B, C, T, H, W = x.shape\n",
        "\n",
        "    assert T % t == 0, 't must divide T (vid.shape[2]) evenly'\n",
        "    assert H % p == 0, 'p must divide H (vid.shape[3]) evenly'\n",
        "    assert W % p == 0, 'p must divide W (vid.shape[4]) evenly'\n",
        "\n",
        "    n_t, n_h, n_w = T // t, H // p, W // p\n",
        "\n",
        "    # (B, C, T, H, W) -> (B, C, n_t, t, n_h, p, n_w, p)\n",
        "    x = x.reshape(B, C, n_t, t, n_h, p, n_w, p)\n",
        "    # (B, C, n_t, t, n_h, p, n_w, p) -> (B, n_t, n_h, n_w, C, t, p, p)\n",
        "    x = x.permute(0, 2, 4, 6, 1, 3, 5, 7).contiguous()\n",
        "    # (B, n_t, n_h, n_w, C, t, p, p) -> ((B * n_t * n_h * n_w), C, t, p, p)\n",
        "    x = x.reshape(-1, C, t, p, p)\n",
        "\n",
        "    # ((B * n_t * n_h * n_w), C, t, p, p) -> (B, n_t, n_h, n_w, C, t, p, p)\n",
        "    x_ = x.reshape(B, n_t, n_h, n_w, C, t, p, p)\n",
        "    # (B, n_t, n_h, n_w, C, t, p, p) -> (B, C, n_t, t, n_h, p, n_w, p)\n",
        "    x_ = x_.permute(0, 4, 1, 5, 2, 6, 3, 7).contiguous()\n",
        "    # (B, C, n_t, t, n_h, p, n_w, p) -> (B, C, T, H, W)\n",
        "    x_ = x_.reshape(B, C, T, H, W)\n",
        "\n",
        "    return x, x_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def display_clip(clip):\n",
        "    def update(frame_idx):\n",
        "        ax.clear()\n",
        "        ax.imshow(video_clip_np[frame_idx], cmap='gray')\n",
        "        ax.axis('off')\n",
        "\n",
        "    video_clip_np = clip.permute(1, 2, 3, 0).numpy()\n",
        "    fig, ax = plt.subplots()\n",
        "    ani = FuncAnimation(fig, update, frames=range(video_clip_np.shape[0]), interval=50)\n",
        "    plt.close()\n",
        "    display(HTML(ani.to_html5_video()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tublets, tublets_ = extract_tubelets(train_batch)\n",
        "print(tublets.shape)\n",
        "print(tublets_.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "display_clip(tublets_[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Initial Tubelet Shape Exploration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# (B, C, T, H, W) -> (B, C, n_t, t, n_h, p, n_w, p)\n",
        "tubelets = vid.reshape(B, C, n_t, t, n_h, p, n_w, p)\n",
        "print(tubelets.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# (B, C, n_t, t, n_h, p, n_w, p) -> (B, n_t, n_h, n_w, C, t, p, p)\n",
        "tubelets = tubelets.permute(0, 2, 4, 6, 1, 3, 5, 7).contiguous()\n",
        "print(tubelets.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# (B, n_t, n_h, n_w, C, t, p, p) -> ((B * n_t * n_h * n_w), C, t, p, p)\n",
        "tubelets = tubelets.reshape(-1, C, t, p, p)\n",
        "print(tubelets.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "conv3d_1 = nn.Conv3d(in_channels=C, out_channels=32, kernel_size=3, stride=2, padding=1)\n",
        "out1 = conv3d_1(tubelets)\n",
        "print(out1.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "conv3d_2 = nn.Conv3d(in_channels=32, out_channels=32, kernel_size=3, stride=2, padding=1)\n",
        "out2 = conv3d_2(out1)\n",
        "print(out2.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "conv3d_3 = nn.Conv3d(in_channels=32, out_channels=32, kernel_size=3, stride=2, padding=1)\n",
        "out3 = conv3d_3(out2)\n",
        "print(out3.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "conv3d_4 = nn.Conv3d(in_channels=32, out_channels=32, kernel_size=3, stride=(1, 2, 2), padding=1)\n",
        "out4 = conv3d_4(out3)\n",
        "print(out4.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "out5 = out4.reshape(-1, 32)\n",
        "print(out5.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# separate out batch dimension post VAE encoding since dim=0 is of shape (B * n_t * n_h * n_w)\n",
        "out6 = out5.reshape(out5.shape[0] // (n_t * n_h * n_w), -1, 32)\n",
        "print(out6.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "out7 = out6.reshape(-1, 32)\n",
        "print(out7.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "out7 = out7.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1)\n",
        "print(out7.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "decoder1 = nn.ConvTranspose3d(in_channels=32, out_channels=32, kernel_size=3, stride=(1, 2, 2), padding=1, output_padding=(0, 1, 1))\n",
        "out8 = decoder1(out7)\n",
        "print(out8.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "decoder2 = nn.ConvTranspose3d(in_channels=32, out_channels=32, kernel_size=3, stride=2, padding=1, output_padding=(1, 1, 1))\n",
        "out9 = decoder2(out8)\n",
        "print(out9.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "decoder3 = nn.ConvTranspose3d(in_channels=32, out_channels=32, kernel_size=3, stride=2, padding=1, output_padding=(1, 1, 1))\n",
        "out10 = decoder2(out9)\n",
        "print(out10.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "decoder3 = nn.ConvTranspose3d(in_channels=32, out_channels=32, kernel_size=3, stride=2, padding=1, output_padding=(1, 1, 1))\n",
        "out11 = decoder2(out10)\n",
        "print(out11.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "config = VideoVAEConfig()\n",
        "\n",
        "model = TubeletFSQVAE(config)\n",
        "lit_model = LitTubeletFSQVAE(model, config)\n",
        "\n",
        "steamboat_willie_data = SteamboatWillieDataModule(config)\n",
        "\n",
        "wandb.init(project=config.project_name, config=config.to_dict())\n",
        "wandb_logger = WandbLogger(project=config.project_name, log_model=True)\n",
        "wandb_logger.watch(lit_model, log=\"all\")\n",
        "\n",
        "lr_monitor = LearningRateMonitor(logging_interval='step')\n",
        "\n",
        "checkpoint_callback = ModelCheckpoint(\n",
        "    dirpath=config.checkpoint_path,\n",
        "    filename='model-{epoch:02d}-{val_loss:.2f}',\n",
        "    every_n_epochs=5,\n",
        "    save_top_k=config.save_top_k,\n",
        "    monitor='val/loss',\n",
        "    mode='min',\n",
        "    save_last=True\n",
        ")\n",
        "\n",
        "# Define the EarlyStopping callback\n",
        "early_stop_callback = EarlyStopping(\n",
        "    monitor='val/loss',\n",
        "    min_delta=0.0000000,\n",
        "    patience=50,\n",
        "    verbose=True,\n",
        "    check_finite=True\n",
        ")\n",
        "\n",
        "trainer = pl.Trainer(\n",
        "    max_epochs=config.max_epochs,\n",
        "    devices=1,\n",
        "    accelerator=\"gpu\",\n",
        "    precision=\"16-mixed\",\n",
        "    logger=wandb_logger,\n",
        "    callbacks=[\n",
        "        lr_monitor,\n",
        "        early_stop_callback,\n",
        "        checkpoint_callback\n",
        "    ],\n",
        "    log_every_n_steps=1,\n",
        "    gradient_clip_val=1.0,\n",
        "    # overfit_batches=1,\n",
        ")\n",
        "\n",
        "tuner = Tuner(trainer)\n",
        "lr_result = tuner.lr_find(lit_model, datamodule=steamboat_willie_data, max_lr=1)\n",
        "lr_result.plot(show=True, suggest=True)\n",
        "\n",
        "trainer.fit(lit_model, steamboat_willie_data)\n",
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Sweeps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "config = VideoVAEConfig()\n",
        "\n",
        "sweep_config = {\n",
        "    'method': 'random',\n",
        "    'metric': {\n",
        "        'name': 'val/loss',\n",
        "        'goal': 'minimize'\n",
        "    },\n",
        "    'parameters': {\n",
        "        'lr': {\n",
        "            'min': 1e-4,\n",
        "            'max': 2e-4,\n",
        "            'distribution': 'log_uniform_values'\n",
        "        },\n",
        "\n",
        "    }\n",
        "}\n",
        "\n",
        "sweep_id = wandb.sweep(sweep_config, project=config.project_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train():\n",
        "    with wandb.init() as run:\n",
        "        config = VideoVAEConfig()\n",
        "        config.update(wandb.config)\n",
        "\n",
        "        model = TubeletFSQVAE(config)\n",
        "        lit_model = LitTubeletFSQVAE(model, config)\n",
        "        steamboat_willie_data = SteamboatWillieDataModule(config)\n",
        "        wandb_logger = WandbLogger(project=config.project_name, log_model=False)\n",
        "\n",
        "        early_stop_callback = EarlyStopping(\n",
        "            monitor='val/loss',\n",
        "            min_delta=0.00,\n",
        "            patience=10,\n",
        "            verbose=True,\n",
        "            check_finite=True\n",
        "        )\n",
        "\n",
        "        trainer = pl.Trainer(\n",
        "            max_epochs=20,\n",
        "            devices=1,\n",
        "            accelerator=\"gpu\",\n",
        "            precision=\"16-mixed\",\n",
        "            logger=wandb_logger,\n",
        "            callbacks=[early_stop_callback]\n",
        "        )\n",
        "\n",
        "        # tuner = Tuner(trainer)\n",
        "        # lr_result = tuner.lr_find(lit_model, datamodule=steamboat_willie_data, max_lr=1)\n",
        "        # lr_result.plot(show=True, suggest=True)\n",
        "\n",
        "        trainer.fit(lit_model, steamboat_willie_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# wandb.agent(sweep_id, train, count=25)\n",
        "wandb.agent(sweep_id, train)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
