{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NopJ0hY5u6vj"
      },
      "source": [
        "The goal here is to build a FSQ-VAE which builds latent vectors for spatio-temporal \"tublets\" described in the ViViT. Sequences of tublet latent vectors are to then be modeled by a Transformer Decoder\n",
        "\n",
        "![](https://i.imgur.com/9G7QTfV.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install -q wandb pytorch_lightning av imageio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "hXn2eB7nRgLD"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import gc\n",
        "import math\n",
        "import wandb\n",
        "import imageio\n",
        "import torch\n",
        "import torchvision\n",
        "import numpy as np\n",
        "import pytorch_lightning as pl\n",
        "from tqdm.notebook import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.animation import FuncAnimation\n",
        "from IPython.display import display, HTML\n",
        "from torch import nn, optim\n",
        "from torch.nn import functional as F\n",
        "from torchvision.datasets.video_utils import VideoClips\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torchvision.transforms import Compose, Lambda, Resize, ToTensor, CenterCrop, Grayscale\n",
        "import torchvision.transforms.functional as TF\n",
        "\n",
        "from pytorch_lightning.loggers import WandbLogger\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint, LearningRateMonitor, EarlyStopping\n",
        "from pytorch_lightning.tuner import Tuner\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR, OneCycleLR\n",
        "\n",
        "pl.seed_everything(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "steamboat_willie_gdrive_path = '/content/drive/My Drive/SteamboatWillie/SteamboatWillie.mp4'\n",
        "!cp -r /content/drive/My\\ Drive/SteamboatWillie/clips ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Clip Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class RandomHorizontalFlipVideo(torch.nn.Module):\n",
        "    def __init__(self, p=0.5):\n",
        "        super().__init__()\n",
        "        self.p = p\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape is expected to be (C, T, H, W)\n",
        "        if torch.rand(1) < self.p:\n",
        "            # Flip all frames in the clip\n",
        "            return x.flip(-1)\n",
        "        return x\n",
        "\n",
        "\n",
        "class SteamboatWillieDataset(Dataset):\n",
        "    def __init__(self, config, mode='train', train_split=0.8):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.train_split = train_split\n",
        "        self.mode = mode\n",
        "\n",
        "        self.preprocess_transforms = Compose([\n",
        "                Lambda(lambda x: x.permute(0, 3, 1, 2)),   # (T, H, W, C) to (T, C, H, W) for Greyscale\n",
        "                Grayscale(num_output_channels=1),          # Convert to grayscale\n",
        "                Lambda(lambda x: x.permute(1, 0, 2, 3)),   # (T, C, H, W) to (C, T, H, W) for Conv3d\n",
        "                CenterCrop((480, 575)),                    # Center crop to remove virtical bars\n",
        "                Resize((config.img_size, config.img_size))\n",
        "        ])\n",
        "\n",
        "        self.postprocess_transforms = Compose([\n",
        "            Lambda(lambda x: x / 255.),\n",
        "            Lambda(lambda x: x.view(self.config.in_channels, self.config.clip_length, self.config.img_size, self.config.img_size))\n",
        "        ])\n",
        "\n",
        "        if self.mode == 'train':\n",
        "            self.postprocess_transforms.transforms.append(RandomHorizontalFlipVideo(p=0.5))\n",
        "\n",
        "        if os.path.exists(config.dest_dir):\n",
        "            clip_paths = self.build_existing_clip_paths(config.dest_dir)\n",
        "            self.clips = self.build_clip_refs(clip_paths)\n",
        "        else:\n",
        "            video_clips = VideoClips(\n",
        "                config.paths,\n",
        "                clip_length_in_frames=config.clip_length,\n",
        "                frames_between_clips=config.clip_length\n",
        "            )\n",
        "\n",
        "            self.clips = self.build_clip_refs(self.build_clip_paths(video_clips, self.preprocess_transforms, config.dest_dir))\n",
        "\n",
        "        if mode in ['train', 'val']:\n",
        "            total_clips = len(self.clips)\n",
        "\n",
        "            indices = torch.randperm(total_clips).tolist()\n",
        "            train_size = int(total_clips * train_split)\n",
        "\n",
        "            if mode == 'train':\n",
        "                self.clip_indices = indices[:train_size]\n",
        "            else:\n",
        "                self.clip_indices = indices[train_size:]\n",
        "        elif mode == 'full':\n",
        "            self.clip_indices = list(range(len(self.clips)))\n",
        "\n",
        "    def build_clip_paths(self, video_clips, transforms, dest_dir):\n",
        "        \"\"\"\n",
        "        Build set of binary files to store processed video clips\n",
        "        returns dict of clip_idx -> mmapped file path\n",
        "        \"\"\"\n",
        "        clip_paths = {}\n",
        "\n",
        "        if not os.path.exists(dest_dir):\n",
        "            os.makedirs(dest_dir)\n",
        "\n",
        "        for idx in tqdm(range(video_clips.num_clips()), desc='Creating clip .bin files'):\n",
        "            # transform clips and write to mmap file\n",
        "            clip, _, _, _ = video_clips.get_clip(idx)\n",
        "            clip = self.preprocess_transforms(clip)\n",
        "            clip_np = clip.numpy().astype(np.uint8)\n",
        "\n",
        "            mmapped_file_path = os.path.join(dest_dir, f'clip_{idx}.bin')\n",
        "            fp = np.memmap(mmapped_file_path, dtype='uint8', mode='w+', shape=clip_np.shape)\n",
        "            fp[:] = clip_np[:]\n",
        "            fp.flush()\n",
        "            del fp\n",
        "            clip_paths[idx] = mmapped_file_path\n",
        "\n",
        "        return clip_paths\n",
        "\n",
        "    def build_existing_clip_paths(self, dest_dir):\n",
        "        \"\"\"\"\n",
        "        returns dict of clip_idx -> mmapped file path\n",
        "        from existing .bin files\n",
        "        \"\"\"\n",
        "        clips_paths = {}\n",
        "        for filename in os.listdir(dest_dir):\n",
        "            if filename.startswith('clip_') and filename.endswith('.bin'):\n",
        "                idx = int(filename.split('_')[1].split('.')[0])\n",
        "                file_path = os.path.join(dest_dir, filename)\n",
        "                clips_paths[idx] = file_path\n",
        "\n",
        "        return clips_paths\n",
        "\n",
        "    def build_clip_refs(self, clip_paths):\n",
        "        \"\"\"\n",
        "        Build mmap reference to bin files\n",
        "        returns dict of clip_idx -> np.array mmapped to respective bin file\n",
        "        \"\"\"\n",
        "        clips = {}\n",
        "        for idx, path in tqdm(clip_paths.items(), desc='Building clip refs'):\n",
        "            clips[idx] = np.memmap(path, dtype='uint8', mode='r')\n",
        "\n",
        "        return clips\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.clip_indices)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        clip = self.clips[self.clip_indices[idx]]\n",
        "        return self.postprocess_transforms(torch.tensor(clip, dtype=torch.float32))\n",
        "\n",
        "\n",
        "class SteamboatWillieDataModule(pl.LightningDataModule):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.batch_size = config.batch_size\n",
        "        self.config = config\n",
        "\n",
        "    def setup(self, stage=None):\n",
        "        if stage == 'fit' or stage is None:\n",
        "            self.train_dataset = SteamboatWillieDataset(self.config, mode='train')\n",
        "            self.val_dataset = SteamboatWillieDataset(self.config, mode='val')\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return torch.utils.data.DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True, num_workers=self.config.num_workers)\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return torch.utils.data.DataLoader(self.val_dataset, batch_size=self.batch_size, shuffle=False, num_workers=self.config.num_workers)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# FSQ-VAE Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ResBlock3d(nn.Module):\n",
        "    def __init__(\n",
        "            self,\n",
        "            in_channels,\n",
        "            out_channels,\n",
        "            singleton_time_dim=False\n",
        "        ):\n",
        "        super().__init__()\n",
        "        if in_channels != out_channels:\n",
        "            self.identity = nn.Conv3d(\n",
        "                in_channels=in_channels,\n",
        "                out_channels=out_channels,\n",
        "                kernel_size=1\n",
        "            )\n",
        "        else:\n",
        "            self.identity = nn.Identity()\n",
        "\n",
        "        hidden_channels = in_channels // 4\n",
        "\n",
        "        self.block = nn.Sequential(\n",
        "            nn.Conv3d(\n",
        "                in_channels=in_channels,\n",
        "                out_channels=hidden_channels,\n",
        "                kernel_size=(1, 3, 3) if singleton_time_dim else (3, 3, 3),\n",
        "                padding=(0, 1, 1) if singleton_time_dim else (1, 1, 1)\n",
        "            ),\n",
        "            nn.BatchNorm3d(hidden_channels),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv3d(\n",
        "                in_channels=hidden_channels,\n",
        "                out_channels=hidden_channels,\n",
        "                kernel_size=(1, 3, 3) if singleton_time_dim else (3, 3, 3),\n",
        "                padding=(0, 1, 1) if singleton_time_dim else (1, 1, 1)\n",
        "            ),\n",
        "            nn.BatchNorm3d(hidden_channels),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv3d(\n",
        "                in_channels=hidden_channels,\n",
        "                out_channels=hidden_channels,\n",
        "                kernel_size=(1, 3, 3) if singleton_time_dim else (3, 3, 3),\n",
        "                padding=(0, 1, 1) if singleton_time_dim else (1, 1, 1)\n",
        "            ),\n",
        "            nn.BatchNorm3d(hidden_channels),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv3d(\n",
        "                in_channels=hidden_channels,\n",
        "                out_channels=out_channels,\n",
        "                kernel_size=1\n",
        "            ),\n",
        "            nn.BatchNorm3d(out_channels)\n",
        "        )\n",
        "        self.res_act = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.block(x) + self.identity(x)\n",
        "        return self.res_act(out)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(\n",
        "            self,\n",
        "            in_channels,\n",
        "            hidden_channels,\n",
        "            out_channels,\n",
        "            nlayers,\n",
        "            nblocks\n",
        "        ):\n",
        "        super().__init__()\n",
        "        self.downsample_blocks = nn.Sequential(*[\n",
        "            nn.Sequential(\n",
        "                nn.Conv3d(\n",
        "                    in_channels=in_channels if i==0 else hidden_channels,\n",
        "                    out_channels=hidden_channels,\n",
        "                    kernel_size=(3, 3, 3),\n",
        "                    stride=(2, 2, 2),\n",
        "                    padding=(1, 1, 1)\n",
        "                ),\n",
        "                nn.BatchNorm3d(hidden_channels),\n",
        "                nn.ReLU(),\n",
        "                nn.Sequential(*[\n",
        "                    ResBlock3d(\n",
        "                        in_channels=hidden_channels,\n",
        "                        out_channels=out_channels if i==nlayers-1 and j==nblocks-1 else hidden_channels,\n",
        "                        singleton_time_dim=(i==nlayers-1)\n",
        "                    ) for j in range(nblocks)\n",
        "                ])\n",
        "            ) for i in range(nlayers)\n",
        "        ])\n",
        "\n",
        "    def forward(self, x):\n",
        "        # ((B * n_t * n_h * n_w), C, t, p, p) -> ((B * n_t * n_h * n_w), latent_channels, 1, 4, 4)\n",
        "        x = self.downsample_blocks(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Decoder(torch.nn.Module):\n",
        "    def __init__(\n",
        "            self,\n",
        "            in_channels,\n",
        "            hidden_channels,\n",
        "            out_channels,\n",
        "            nlayers,\n",
        "            nblocks\n",
        "        ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.res_blocks = nn.Sequential(*[\n",
        "            ResBlock3d(\n",
        "                in_channels=in_channels if i==0 else hidden_channels,\n",
        "                out_channels=hidden_channels,\n",
        "            )\n",
        "            for i in range(nblocks)\n",
        "        ])\n",
        "\n",
        "        self.upsample_blocks = nn.Sequential(*[\n",
        "            nn.Sequential(\n",
        "                nn.Sequential(*[\n",
        "                    ResBlock3d(\n",
        "                        in_channels=in_channels if i==0 and j==0 else hidden_channels,\n",
        "                        out_channels=hidden_channels,\n",
        "                        singleton_time_dim=(i==0)\n",
        "                    )\n",
        "                    for j in range(nblocks)\n",
        "                ]),\n",
        "                nn.ConvTranspose3d(\n",
        "                    in_channels=hidden_channels,\n",
        "                    out_channels=out_channels if i==nlayers-1 else hidden_channels,\n",
        "                    kernel_size=(3, 3, 3),\n",
        "                    stride=(2, 2, 2),\n",
        "                    padding=(1, 1, 1),\n",
        "                    output_padding=(1, 1, 1)\n",
        "                    ),\n",
        "                nn.BatchNorm3d(hidden_channels) if i<nlayers-1 else nn.Identity(),\n",
        "                nn.ReLU() if i<nlayers-1 else nn.Identity()\n",
        "            ) for i in range(nlayers)\n",
        "        ])\n",
        "        self.out_act = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # ((B * n_t * n_h * n_w), latent_channels, 1, 4, 4) -> ((B * n_t * n_h * n_w), C, t, p, p)\n",
        "        x = self.upsample_blocks(x)\n",
        "        x = self.out_act(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class FSQ(nn.Module):\n",
        "    def __init__(self, levels, eps=1e-3):\n",
        "        super().__init__()\n",
        "        self.register_buffer('levels', torch.tensor(levels))\n",
        "        self.register_buffer(\n",
        "            'basis',\n",
        "            torch.cumprod(torch.tensor([1] + levels[:-1]), dim=0, dtype=torch.int32)\n",
        "        )\n",
        "\n",
        "        self.eps = eps\n",
        "        self.codebook_size = torch.prod(self.levels)\n",
        "\n",
        "        # self.register_buffer('implicit_codebook', self.idxs_to_code(torch.arange(self.codebook_size)))\n",
        "\n",
        "    def round_ste(self, z):\n",
        "        z_q = torch.round(z)\n",
        "        return z + (z_q - z).detach()\n",
        "\n",
        "    def quantize(self, z):\n",
        "        # half_l is used to determine how to scale tanh; we\n",
        "        # subtract 1 from the number of levels to account for 0\n",
        "        # being a quantization bin and tanh being symmetric around 0\n",
        "        half_l = (self.levels - 1) * (1 - self.eps) / 2\n",
        "\n",
        "        # if a given level is even, it will result in a scale for tanh\n",
        "        # which is halfway between integer values, so we offset\n",
        "        # the tanh output down by 0.5 to line it with whole integers\n",
        "        offset = torch.where(self.levels % 2 == 0, 0.5, 0.0)\n",
        "\n",
        "        # if our level is even, we want to shift the tanh input to\n",
        "        # ensure the 0 quantization bin is centered\n",
        "        shift = torch.tan(offset / half_l)\n",
        "\n",
        "        # once we have our shift and offset (in the case of an even level)\n",
        "        # we can round to the nearest integer bin and allow for STE\n",
        "        z_q = self.round_ste(torch.tanh(z + shift) * half_l - offset)\n",
        "\n",
        "        # after quantization, we want to renormalize the quantized\n",
        "        # values to be within the range expected by the model (ie. [-1, 1])\n",
        "        half_width = self.levels // 2\n",
        "        return z_q / half_width\n",
        "\n",
        "    def scale_and_shift(self, z_q_normalized):\n",
        "        half_width = self.levels // 2\n",
        "        return (z_q_normalized * half_width) + half_width\n",
        "\n",
        "    def scale_and_shift_inverse(self, z_q):\n",
        "        half_width = self.levels // 2\n",
        "        return (z_q - half_width) / half_width\n",
        "\n",
        "    def code_to_idxs(self, z_q):\n",
        "        z_q = self.scale_and_shift(z_q)\n",
        "        return (z_q * self.basis).sum(dim=-1).to(torch.int32)\n",
        "\n",
        "    def idxs_to_code(self, idxs):\n",
        "        idxs = idxs.unsqueeze(-1)\n",
        "        codes_not_centered = (idxs // self.basis) % self.levels\n",
        "        return self.scale_and_shift_inverse(codes_not_centered)\n",
        "\n",
        "    def forward(self, z):\n",
        "        # TODO: make this work for generic tensor sizes\n",
        "        # TODO: use einops to clean up\n",
        "\n",
        "        if len(z.shape) == 5: # video\n",
        "            B, C, T, H, W = z.shape\n",
        "            # (B, C, T, H, W) -> (B, T, H, W, C)\n",
        "            z_c_last = z.permute(0, 2, 3, 4, 1).contiguous()\n",
        "            # (B, T, H, W, C) -> (BTHW, C)\n",
        "            z_flatten = z_c_last.reshape(-1, C)\n",
        "            z_flatten_q = self.quantize(z_flatten)\n",
        "            # (BTHW, C) -> (B, T, H, W, C) -> (B, C, T, H, W)\n",
        "            z_q = z_flatten_q.reshape(B, T, H, W, C).permute(0, 4, 1, 2, 3).contiguous()\n",
        "\n",
        "        elif len(z.shape) == 4: # image\n",
        "            B, C, H, W = z.shape\n",
        "            # (B, C, H, W) -> (B, H, W, C)\n",
        "            z_c_last = z.permute(0, 2, 3, 1).contiguous()\n",
        "            # (B, H, W, C) -> (BHW, C)\n",
        "            z_flatten = z_c_last.reshape(-1, C)\n",
        "            z_flatten_q = self.quantize(z_flatten)\n",
        "            # (BHW, C) -> (B, H, W, C) -> (B, C, T, H, W)\n",
        "            z_q = z_flatten_q.reshape(B, H, W, C).permute(0, 3, 1, 2).contiguous()\n",
        "\n",
        "        elif len(z.shape) == 2: # vector sequence\n",
        "            # (B, C)\n",
        "            z_q = self.quantize(z)\n",
        "\n",
        "        return {'z_q': z_q}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TubeletFSQVAE(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.encoder = Encoder(\n",
        "            in_channels     = config.in_channels,\n",
        "            hidden_channels = config.hidden_channels,\n",
        "            out_channels    = config.latent_channels,\n",
        "            nlayers         = config.nlayers,\n",
        "            nblocks         = config.nblocks\n",
        "        )\n",
        "        self.decoder = Decoder(\n",
        "            in_channels     = config.latent_channels,\n",
        "            hidden_channels = config.hidden_channels,\n",
        "            out_channels    = config.in_channels,\n",
        "            nlayers         = config.nlayers,\n",
        "            nblocks         = config.nblocks\n",
        "        )\n",
        "\n",
        "        self.quantizer = FSQ(config.levels)\n",
        "\n",
        "        self.config = config\n",
        "\n",
        "    def encode(self, inputs):\n",
        "        inputs = self.extract_tubelets(inputs, self.config.t, self.config.p)\n",
        "        return self.encoder(inputs)\n",
        "\n",
        "    def quantize(self, z):\n",
        "        return self.quantizer(z)\n",
        "\n",
        "    def decode(self, z_q):\n",
        "        return self.decoder(z_q)\n",
        "\n",
        "    def extract_tubelets(self, x, t, p):\n",
        "        '''\n",
        "        extract tubelet sequence of shape ((B * n_t * n_h * n_w), C, t, p, p)\n",
        "        from a video tensor of shape (B, C, T, H, W)\n",
        "        where n_t = T // t, n_h = H // p, and n_w = W // p\n",
        "        '''\n",
        "        assert len(x.shape) == 5, 'x.shape must be (B, C, T, H, W)'\n",
        "\n",
        "        B, C, T, H, W = x.shape\n",
        "\n",
        "        assert T % t == 0, 't must divide T (x.shape[2]) evenly'\n",
        "        assert H % p == 0, 'p must divide H (x.shape[3]) evenly'\n",
        "        assert W % p == 0, 'p must divide W (x.shape[4]) evenly'\n",
        "\n",
        "        n_t, n_h, n_w = T // t, H // p, W // p\n",
        "\n",
        "        # (B, C, T, H, W) -> (B, C, n_t, t, n_h, p, n_w, p)\n",
        "        x = x.reshape(B, C, n_t, t, n_h, p, n_w, p)\n",
        "        # (B, C, n_t, t, n_h, p, n_w, p) -> (B, n_t, n_h, n_w, C, t, p, p)\n",
        "        x = x.permute(0, 2, 4, 6, 1, 3, 5, 7).contiguous()\n",
        "        # (B, n_t, n_h, n_w, C, t, p, p) -> ((B * n_t * n_h * n_w), C, t, p, p)\n",
        "        x = x.reshape(-1, C, t, p, p)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def assemble_tubelets(self, x, B, C, T, H, W, t, p):\n",
        "        '''\n",
        "        reassemble tubelet sequence of shape ((B * n_t * n_h * n_w), C, t, p, p)\n",
        "        into a video tensor of shape (B, C, T, H, W)\n",
        "        '''\n",
        "        assert len(x.shape) == 5, 'x.shape must be ((B * n_t * n_h * n_w), C, t, p, p)'\n",
        "\n",
        "        assert T % t == 0, 't must divide T (x.shape[2]) evenly'\n",
        "        assert H % p == 0, 'p must divide H (x.shape[3]) evenly'\n",
        "        assert W % p == 0, 'p must divide W (x.shape[4]) evenly'\n",
        "\n",
        "        n_t, n_h, n_w = T // t, H // p, W // p\n",
        "\n",
        "        # ((B * n_t * n_h * n_w), C, t, p, p) -> (B, n_t, n_h, n_w, C, t, p, p)\n",
        "        x = x.reshape(B, n_t, n_h, n_w, C, t, p, p)\n",
        "        # (B, n_t, n_h, n_w, C, t, p, p) -> (B, C, n_t, t, n_h, p, n_w, p)\n",
        "        x = x.permute(0, 4, 1, 5, 2, 6, 3, 7).contiguous()\n",
        "        # (B, C, n_t, t, n_h, p, n_w, p) -> (B, C, T, H, W)\n",
        "        x = x.reshape(B, C, T, H, W)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def latent_img_to_seq(self, latent_img):\n",
        "        '''\n",
        "        convert latent image from encoder of shape (B, latent_channels, 1, 4, 4) into sequence\n",
        "        of latent vectors of shape ((B * 1 * 4 * 4), latent_channels)\n",
        "        '''\n",
        "        return latent_img.permute(0, 2, 3, 4, 1).contiguous().reshape(-1, self.config.latent_channels)\n",
        "\n",
        "    def latent_seq_to_img(self, latent_seq):\n",
        "        '''\n",
        "        convert latent sequence of shape ((B * 1 * 4 * 4), latent_channels) into latent images\n",
        "        of shape (B, latent_channels, 1, 4, 4)\n",
        "        '''\n",
        "        return latent_seq.reshape(-1, 1, 4, 4, self.config.latent_channels).permute(0, 4, 1, 2, 3).contiguous()\n",
        "\n",
        "\n",
        "    def loss(self, x_hat, x, quantized):\n",
        "        MSE = F.mse_loss(x_hat, x)\n",
        "\n",
        "        return {\n",
        "            'loss': MSE,\n",
        "            **quantized\n",
        "        }\n",
        "\n",
        "    def forward(self, x):\n",
        "        z = self.encode(x)\n",
        "        quantized = self.quantize(z)\n",
        "        x_hat = self.decode(quantized['z_q'])\n",
        "\n",
        "        # ((B * n_t * n_h * n_w), C, t, p, p)\n",
        "        losses = self.loss(x_hat, self.extract_tubelets(x, self.config.t, self.config.p), quantized)\n",
        "\n",
        "        return {'x_hat': x_hat, 'z': z, 'z_q': quantized['z_q'], **losses}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Transformer Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ALiBiSelfAttention(nn.Module):\n",
        "    def __init__(self, emb_dim, qkv_dim, nheads, ctx_size, window_size, dropout=0.0, use_flash_attn=True):\n",
        "        super().__init__()\n",
        "        assert qkv_dim % nheads == 0\n",
        "        # TODO: sweep over attn_emb_dim values\n",
        "\n",
        "        # project latent embedding to higher qkv dim to increased attention capacity\n",
        "        self.W_Q = nn.Linear(emb_dim, qkv_dim, bias=False)\n",
        "        self.W_K = nn.Linear(emb_dim, qkv_dim, bias=False)\n",
        "        self.W_V = nn.Linear(emb_dim, qkv_dim, bias=False)\n",
        "\n",
        "        # project latent embedding back to emb_dim after attention has been computed\n",
        "        self.W_O = nn.Linear(qkv_dim, emb_dim, bias=False)\n",
        "\n",
        "        if not use_flash_attn:\n",
        "            self.register_buffer(\n",
        "                \"mask\",\n",
        "                torch.tril(torch.ones((ctx_size, ctx_size))).reshape(\n",
        "                    1, 1, ctx_size, ctx_size\n",
        "                ),\n",
        "            )\n",
        "\n",
        "        self.qkv_dim = qkv_dim\n",
        "        self.head_dim = qkv_dim // nheads\n",
        "        self.nheads = nheads\n",
        "        self.window_size = window_size\n",
        "\n",
        "        self.dropout_p = dropout\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        self.use_flash_attn = use_flash_attn\n",
        "\n",
        "    def forward(self, x, slopes):\n",
        "        B, T, D = x.size()\n",
        "\n",
        "        # (B, T, D) -> (B, T, qkv_dim) -> (B, T, H, H_D)\n",
        "        Q = self.W_Q(x).reshape(B, T, self.nheads, self.head_dim)\n",
        "        K = self.W_K(x).reshape(B, T, self.nheads, self.head_dim)\n",
        "        V = self.W_V(x).reshape(B, T, self.nheads, self.head_dim)\n",
        "\n",
        "        if self.use_flash_attn:\n",
        "            out = flash_attn_func(\n",
        "                Q, K, V,\n",
        "                dropout_p=self.dropout_p if self.training else 0.0,\n",
        "                softmax_scale=None,\n",
        "                causal=True,\n",
        "                window_size=(self.window_size, 0),\n",
        "                alibi_slopes=slopes.to(torch.float32),\n",
        "                deterministic=False\n",
        "            )\n",
        "        else:\n",
        "            # (B, T, H, H_D) -> (B, H, T, H_D)\n",
        "            Q = Q.transpose(1, 2)\n",
        "            K = K.transpose(1, 2)\n",
        "            V = V.transpose(1, 2)\n",
        "\n",
        "            # (B, H, T, H_D) @ (B, H, H_D, T) -> (B, H, T, T)\n",
        "            attn = (Q @ K.transpose(-2, -1)) / (1.0 * math.sqrt(self.head_dim))\n",
        "            # attn = attn + bias\n",
        "            attn = attn.masked_fill(self.mask[:,:,:T,:T]==0, float('-inf'))\n",
        "            attn = F.softmax(attn, dim=-1)\n",
        "\n",
        "            attn = self.dropout(attn)\n",
        "\n",
        "            # ((B, H, T, T)) @ (B, H, T, H_D) -> (B, H, T, H_D)\n",
        "            out = attn @ V\n",
        "\n",
        "        # (B, H, T, H_D) -> (B, T, H, H_D) -> (B, T, D)\n",
        "        out = out.transpose(1, 2).reshape(B, T, self.qkv_dim)\n",
        "\n",
        "        # (B, T, D)\n",
        "        out = self.W_O(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, emb_dim, dropout=0.0, fan_out=100):\n",
        "        super().__init__()\n",
        "        # TODO: sweep over fan_out\n",
        "        self.fc1 = nn.Linear(emb_dim, fan_out * emb_dim)\n",
        "        self.act = nn.GELU()\n",
        "        self.fc2 = nn.Linear(fan_out * emb_dim, emb_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.act(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, emb_dim, qkv_dim, nheads, ctx_size, window_size, dropout=0.0, fan_out=100, use_flash_attn=True):\n",
        "        super().__init__()\n",
        "        self.ln_1 = nn.LayerNorm(emb_dim)\n",
        "        self.attn = ALiBiSelfAttention(emb_dim, qkv_dim, nheads, ctx_size, window_size, dropout, use_flash_attn)\n",
        "        self.ln_2 = nn.LayerNorm(emb_dim)\n",
        "        self.mlp = MLP(emb_dim, dropout, fan_out)\n",
        "\n",
        "    def forward(self, x, slopes):\n",
        "        x = x + self.attn(self.ln_1(x), slopes)\n",
        "        x = x + self.mlp(self.ln_2(x))\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class TransformerDecoder(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.blocks = nn.ModuleList([\n",
        "            TransformerBlock(\n",
        "                config.emb_dim,\n",
        "                config.qkv_dim,\n",
        "                config.nheads,\n",
        "                config.ctx_size,\n",
        "                config.window_size,\n",
        "                config.dropout,\n",
        "                config.fan_out,\n",
        "                config.use_flash_attn\n",
        "            ) for _ in range(config.ntlayers)\n",
        "        ])\n",
        "        self.pred_head = nn.Linear(config.emb_dim, config.vocab_size, bias=False)\n",
        "        self.ln_f = nn.LayerNorm(config.emb_dim)\n",
        "\n",
        "        self.register_buffer(\"m\", self.get_alibi_slope(config.nheads))\n",
        "        self.window_size = config.window_size\n",
        "\n",
        "        self.use_flash_attn = config.use_flash_attn\n",
        "\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, D = x.size()\n",
        "\n",
        "        if not self.use_flash_attn:\n",
        "            bias = (self.m * self.get_relative_positions(self.window_size).to(x.device)).unsqueeze(0)\n",
        "\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        for block in self.blocks:\n",
        "            x = block(x, self.m)\n",
        "\n",
        "        logits = self.pred_head(self.ln_f(x))\n",
        "\n",
        "        return logits\n",
        "\n",
        "    def get_relative_positions(self, seq_len: int) -> torch.tensor:\n",
        "        x = torch.arange(seq_len)[None, :]\n",
        "        y = torch.arange(seq_len)[:, None]\n",
        "        return (x - y).clamp_max_(0)\n",
        "\n",
        "\n",
        "    def get_alibi_slope(self, num_heads):\n",
        "        x = (2 ** 8) ** (1 / num_heads)\n",
        "        return (\n",
        "            torch.tensor([1 / x ** (i + 1) for i in range(num_heads)])\n",
        "            # .unsqueeze(-1)\n",
        "            # .unsqueeze(-1)\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_tubelets(x, t=8, p=16):\n",
        "    '''\n",
        "    extract tubelet sequence of shape ((B * n_t * n_h * n_w), C, t, p, p)\n",
        "    from a video tensor of shape (B, C, T, H, W)\n",
        "    where n_t = T // t, n_h = H // p, and n_w = W // p\n",
        "    '''\n",
        "    assert len(x.shape) == 5, 'vid.shape must be (B, C, T, H, W)'\n",
        "\n",
        "    B, C, T, H, W = x.shape\n",
        "\n",
        "    assert T % t == 0, 't must divide T (vid.shape[2]) evenly'\n",
        "    assert H % p == 0, 'p must divide H (vid.shape[3]) evenly'\n",
        "    assert W % p == 0, 'p must divide W (vid.shape[4]) evenly'\n",
        "\n",
        "    n_t, n_h, n_w = T // t, H // p, W // p\n",
        "\n",
        "    # (B, C, T, H, W) -> (B, C, n_t, t, n_h, p, n_w, p)\n",
        "    x = x.reshape(B, C, n_t, t, n_h, p, n_w, p)\n",
        "    # (B, C, n_t, t, n_h, p, n_w, p) -> (B, n_t, n_h, n_w, C, t, p, p)\n",
        "    x = x.permute(0, 2, 4, 6, 1, 3, 5, 7).contiguous()\n",
        "    # (B, n_t, n_h, n_w, C, t, p, p) -> ((B * n_t * n_h * n_w), C, t, p, p)\n",
        "    x = x.reshape(-1, C, t, p, p)\n",
        "\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Lightning Module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class LitTubeletFSQVAE(pl.LightningModule):\n",
        "    def __init__(self, model, config):\n",
        "        super().__init__()\n",
        "        self.model = model\n",
        "        self.config = config\n",
        "        self.lr = config.lr\n",
        "\n",
        "        if self.logger:\n",
        "            self.logger.experiment.config.update(config)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        x = batch\n",
        "        out = self(x)\n",
        "\n",
        "        self.log('train/loss', out['loss'], prog_bar=True)\n",
        "\n",
        "        return out['loss']\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        x = batch\n",
        "        out = self(x)\n",
        "\n",
        "        self.log('val/loss', out['loss'], prog_bar=True)\n",
        "\n",
        "        if batch_idx == 0:\n",
        "            # ((B * n_t * n_h * n_w), C, t, p, p) -> (B, C, T, H, W)\n",
        "            t, p = out['x_hat'].shape[-3], out['x_hat'].shape[-2]\n",
        "            out['x_hat'] = self.model.assemble_tubelets(out['x_hat'], *x.shape, t, p)\n",
        "            self.log_val_clips(x, out)\n",
        "\n",
        "        return out['loss']\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.AdamW(\n",
        "            self.parameters(),\n",
        "            lr=self.lr,\n",
        "            betas=(self.config.beta1, self.config.beta2),\n",
        "            weight_decay=self.config.weight_decay\n",
        "        )\n",
        "\n",
        "        if self.config.use_lr_schedule:\n",
        "            scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=self.config.max_epochs)\n",
        "            return [optimizer], [scheduler]\n",
        "\n",
        "        return optimizer\n",
        "\n",
        "    def log_val_clips(self, x, out, num_clips=2):\n",
        "        n_clips = min(x.size(0), num_clips)\n",
        "\n",
        "        for i in range(n_clips):\n",
        "            # extract the ith original and reconstructed clip\n",
        "            original_clip = x[i]  # (C, T, H, W)\n",
        "            reconstructed_clip = out['x_hat'][i]  # (C, T, H, W)\n",
        "\n",
        "            # convert tensors to numpy arrays and transpose to (T, H, W, C) for GIF creation\n",
        "            original_clip_np = original_clip.permute(1, 2, 3, 0).cpu().numpy()\n",
        "            reconstructed_clip_np = reconstructed_clip.permute(1, 2, 3, 0).cpu().numpy()\n",
        "\n",
        "            original_clip_np = (original_clip_np - original_clip_np.min()) / (original_clip_np.max() - original_clip_np.min())\n",
        "            reconstructed_clip_np = (reconstructed_clip_np - reconstructed_clip_np.min()) / (reconstructed_clip_np.max() - reconstructed_clip_np.min())\n",
        "\n",
        "            original_clip_np = (original_clip_np * 255).astype(np.uint8)\n",
        "            reconstructed_clip_np = (reconstructed_clip_np * 255).astype(np.uint8)\n",
        "\n",
        "            # grayscale videos need to be of shape (T, H, W)\n",
        "            if original_clip_np.shape[-1] == 1:\n",
        "                original_clip_np = original_clip_np.squeeze(-1)\n",
        "\n",
        "            if reconstructed_clip_np.shape[-1] == 1:\n",
        "                reconstructed_clip_np = reconstructed_clip_np.squeeze(-1)\n",
        "\n",
        "            # create GIFs for the original and reconstructed clips\n",
        "            original_gif_path = f'/tmp/original_clip_{i}.gif'\n",
        "            reconstructed_gif_path = f'/tmp/reconstructed_clip_{i}.gif'\n",
        "            imageio.mimsave(original_gif_path, original_clip_np, fps=10)\n",
        "            imageio.mimsave(reconstructed_gif_path, reconstructed_clip_np, fps=10)\n",
        "\n",
        "            # log the GIFs to wandb\n",
        "            self.logger.experiment.log({\n",
        "                f\"val/original_clip_{i}\": wandb.Video(original_gif_path, fps=10, format=\"gif\", caption=\"Original\"),\n",
        "                f\"val/reconstructed_clip_{i}\": wandb.Video(reconstructed_gif_path, fps=10, format=\"gif\", caption=\"Reconstructed\")\n",
        "            })"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class LitTransformerDecoder(pl.LightningModule):\n",
        "    def __init__(self, model, config):\n",
        "        super().__init__()\n",
        "        self.model = model\n",
        "        self.config = config\n",
        "        self.lr = config.lr\n",
        "\n",
        "        self.fsq = FSQ(config.levels) # need codes -> idxs\n",
        "\n",
        "        if self.logger:\n",
        "            self.logger.experiment.config.update(config)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        logits = self(x)\n",
        "\n",
        "        y_idxs = self.fsq.codes_to_idxs(y).to(torch.long)\n",
        "        loss = F.cross_entropy(logits.reshape(-1, logits.size(-1)), y_idxs.reshape(-1))\n",
        "\n",
        "        self.log('train/loss', loss, prog_bar=True)\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        logits = self(x)\n",
        "\n",
        "        y_idxs = self.fsq.codes_to_idxs(y).to(torch.long)\n",
        "        loss = F.cross_entropy(logits.reshape(-1, logits.size(-1)), y_idxs.reshape(-1))\n",
        "\n",
        "        self.log('val/loss', loss, prog_bar=True)\n",
        "\n",
        "        if batch_idx == 0:\n",
        "            self.log_val_clips(x)\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.AdamW(\n",
        "            self.parameters(),\n",
        "            lr=self.lr,\n",
        "            betas=(self.config.beta1, self.config.beta2),\n",
        "            weight_decay=self.config.weight_decay\n",
        "        )\n",
        "\n",
        "        if self.config.use_lr_schedule:\n",
        "            scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=self.config.max_epochs)\n",
        "            return [optimizer], [scheduler]\n",
        "\n",
        "        return optimizer\n",
        "\n",
        "    def log_val_clips(self, lat_seq):\n",
        "        pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_num_downsample_layers(img_size, target_size=2):\n",
        "    \"\"\"\n",
        "    get the number of strided Conv layers\n",
        "    required to produce a target output volume.\n",
        "\n",
        "    The minimum number of downsample layers required for achieving the target output volume.\n",
        "    \"\"\"\n",
        "    if img_size < target_size:\n",
        "        raise ValueError(f\"Image size must be at least {target_size}x{target_size}.\")\n",
        "\n",
        "    # Calculate the minimum number of downsample layers required for the target final size\n",
        "    num_layers = math.ceil(math.log2(img_size / target_size))\n",
        "    return num_layers\n",
        "\n",
        "class VideoGenConfig:\n",
        "    def __init__(self):\n",
        "        self.project_name = 'Steamboat Willie Latent Model'#'Tubelet FSQ-VAE Steamboat Willie'\n",
        "        # dataset properties\n",
        "        self.paths = ['/content/drive/My Drive/SteamboatWillie/SteamboatWillie.mp4']\n",
        "        self.clip_dest_dir = './clips/'\n",
        "        self.latent_seqs_dest_dir = './latent_seqs/'\n",
        "        # model checkpoints\n",
        "        self.checkpoint_path = \"./checkpoints\"\n",
        "        self.fsq_vae_checkpoint_path = \"./fsq-vae-model.ckpt\"\n",
        "        self.save_top_k = 1\n",
        "        # FSQ-VAE training\n",
        "        self.train_split = 0.8\n",
        "        self.batch_size = 10\n",
        "        self.max_epochs = 1000\n",
        "        self.num_workers = 2\n",
        "        # FSQ-VAE optimizer\n",
        "        self.lr = 5e-4\n",
        "        self.beta1 = 0.9\n",
        "        self.beta2 = 0.999\n",
        "        self.weight_decay = 0.0 # 1e-2\n",
        "        self.use_wd_schedule = False\n",
        "        self.use_lr_schedule = True\n",
        "        # FSQ-VAE input properties\n",
        "        self.clip_length = 16\n",
        "        self.img_size = 256\n",
        "        self.tubelet_size = 16\n",
        "        self.in_channels = 1\n",
        "        # quantization\n",
        "        self.quant_mode = 'fsq' # 'vq'\n",
        "        self.latent_channels = 5\n",
        "        self.codebook_size = 512\n",
        "        self.commit_loss_beta = 0.25\n",
        "        self.track_codebook = True\n",
        "        self.use_ema = True\n",
        "        self.ema_gamma = 0.99\n",
        "        self.level = 11\n",
        "        self.levels = [self.level for _ in range(self.latent_channels)]\n",
        "        # encoder/decoder\n",
        "        self.start_channels = 32\n",
        "        self.nlayers = get_num_downsample_layers(self.tubelet_size, 4)\n",
        "        self.nblocks = 4\n",
        "        self.hidden_channels = 256\n",
        "        # self.nlayers = 3\n",
        "        # tubelet\n",
        "        self.t = 4\n",
        "        self.p = 16\n",
        "        # transformer\n",
        "        self.emb_dim=5\n",
        "        self.qkv_dim=2048\n",
        "        self.nheads=8\n",
        "        self.ntlayers=12\n",
        "        self.ctx_size=16383,\n",
        "        self.window_size=8192#16383,\n",
        "        self.vocab_size=math.prod(self.levels)\n",
        "        self.dropout=0.0\n",
        "        self.fan_out=410\n",
        "        self.use_flash_attn=True\n",
        "        self.t_batch_size=1\n",
        "        self.t_num_workers=2\n",
        "        self.grad_acc_steps=2\n",
        "\n",
        "\n",
        "    def update(self, updates):\n",
        "        for key, value in updates.items():\n",
        "            if hasattr(self, key):\n",
        "                setattr(self, key, value)\n",
        "                if key == 'level' or key == 'latent_channels':\n",
        "                    self.levels = [self.level for _ in range(self.latent_channels)]\n",
        "\n",
        "    def to_dict(self):\n",
        "        return {k: v for k, v in self.__dict__.items()}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Component Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "config = VideoGenConfig()\n",
        "model = TubeletFSQVAE(config)\n",
        "dataset = SteamboatWillieDataset(config, mode='full')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "checkpoint = torch.load('fsq-vae-model.ckpt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "new_state_dict = {}\n",
        "for key, value in checkpoint['state_dict'].items():\n",
        "    new_key = key.replace('model.', '')\n",
        "    new_state_dict[new_key] = value\n",
        "\n",
        "model.load_state_dict(new_state_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "clips = {}\n",
        "q_latents = {}\n",
        "reconstructions = {}\n",
        "model.eval()\n",
        "model = model.to('cuda:0')\n",
        "with torch.no_grad():\n",
        "    for i, clip in tqdm(enumerate(dataset)):\n",
        "        # collect original clips to display\n",
        "        clips[i] = clip\n",
        "        clip = clip.to('cuda:0')\n",
        "        \n",
        "        # collect clips to display\n",
        "        out = model(clip.unsqueeze(0))\n",
        "\n",
        "        # collect latents\n",
        "        q_latents[i] = out['z_q'].cpu().numpy()\n",
        "        \n",
        "        # collect reconstructions\n",
        "        x_hat = model.assemble_tubelets(out['x_hat'], *clip.unsqueeze(0).shape, out['x_hat'].shape[-3], out['x_hat'].shape[-2])\n",
        "        reconstructions[i] = x_hat.cpu().numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "reconstructions[0].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "display_clip(torch.tensor(reconstructions[0][0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "batch = next(iter(train_loader))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    batch = batch.to('cuda:0')\n",
        "    model = model.to('cuda:0')\n",
        "    \n",
        "    latents = model.encode(batch)\n",
        "    print(latents.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "latents = latents.permute(0, 2, 3, 4, 1).contiguous()\n",
        "print(latents.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "latents = latents.reshape(-1, latents.shape[-1])\n",
        "print(latents.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tublets = extract_tubelets(train_batch)\n",
        "print(tublets.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "enc=Encoder(in_channels=config.in_channels,\n",
        "            hidden_channels=config.hidden_channels,\n",
        "            nlayers=config.nlayers,\n",
        "            nblocks=config.nblocks,\n",
        "            out_channels=config.latent_channels)\n",
        "\n",
        "fsq=FSQ(levels=[config.level for _ in range(config.latent_channels*4)])\n",
        "\n",
        "dec=Decoder(in_channels=config.latent_channels,\n",
        "            hidden_channels=config.hidden_channels,\n",
        "            nlayers=config.nlayers,\n",
        "            nblocks=config.nblocks,\n",
        "            out_channels=config.in_channels)\n",
        "\n",
        "enc, fsq, dec, tublets = enc.to('cuda:0'), fsq.to('cuda:0'), dec.to('cuda:0'), tublets.to('cuda:0')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tubelets_enc = enc(tublets)\n",
        "print(tubelets_enc.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tubelets_fsq = fsq(tubelets_enc)\n",
        "print(tubelets_fsq['z_q'].shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tubelets_dec = dec(tubelets_fsq['z_q'])\n",
        "print(tubelets_dec.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Display Clips"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_tubelets(x, t=8, p=16):\n",
        "    '''\n",
        "    extract tubelet sequence of shape ((B * n_t * n_h * n_w), C, t, p, p)\n",
        "    from a video tensor of shape (B, C, T, H, W)\n",
        "    where n_t = T // t, n_h = H // p, and n_w = W // p\n",
        "    '''\n",
        "    assert len(x.shape) == 5, 'vid.shape must be (B, C, T, H, W)'\n",
        "\n",
        "    B, C, T, H, W = x.shape\n",
        "\n",
        "    assert T % t == 0, 't must divide T (vid.shape[2]) evenly'\n",
        "    assert H % p == 0, 'p must divide H (vid.shape[3]) evenly'\n",
        "    assert W % p == 0, 'p must divide W (vid.shape[4]) evenly'\n",
        "\n",
        "    n_t, n_h, n_w = T // t, H // p, W // p\n",
        "\n",
        "    # (B, C, T, H, W) -> (B, C, n_t, t, n_h, p, n_w, p)\n",
        "    x = x.reshape(B, C, n_t, t, n_h, p, n_w, p)\n",
        "    # (B, C, n_t, t, n_h, p, n_w, p) -> (B, n_t, n_h, n_w, C, t, p, p)\n",
        "    x = x.permute(0, 2, 4, 6, 1, 3, 5, 7).contiguous()\n",
        "    # (B, n_t, n_h, n_w, C, t, p, p) -> ((B * n_t * n_h * n_w), C, t, p, p)\n",
        "    x = x.reshape(-1, C, t, p, p)\n",
        "\n",
        "    # ((B * n_t * n_h * n_w), C, t, p, p) -> (B, n_t, n_h, n_w, C, t, p, p)\n",
        "    x_ = x.reshape(B, n_t, n_h, n_w, C, t, p, p)\n",
        "    # (B, n_t, n_h, n_w, C, t, p, p) -> (B, C, n_t, t, n_h, p, n_w, p)\n",
        "    x_ = x_.permute(0, 4, 1, 5, 2, 6, 3, 7).contiguous()\n",
        "    # (B, C, n_t, t, n_h, p, n_w, p) -> (B, C, T, H, W)\n",
        "    x_ = x_.reshape(B, C, T, H, W)\n",
        "\n",
        "    return x, x_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def display_clip(clip):\n",
        "    def update(frame_idx):\n",
        "        ax.clear()\n",
        "        ax.imshow(video_clip_np[frame_idx], cmap='gray')\n",
        "        ax.axis('off')\n",
        "\n",
        "    video_clip_np = clip.permute(1, 2, 3, 0).numpy()\n",
        "    fig, ax = plt.subplots()\n",
        "    ani = FuncAnimation(fig, update, frames=range(video_clip_np.shape[0]), interval=50)\n",
        "    plt.close()\n",
        "    display(HTML(ani.to_html5_video()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tublets, tublets_ = extract_tubelets(train_batch)\n",
        "print(tublets.shape)\n",
        "print(tublets_.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "display_clip(tublets_[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Initial Tubelet Shape Exploration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# (B, C, T, H, W) -> (B, C, n_t, t, n_h, p, n_w, p)\n",
        "tubelets = vid.reshape(B, C, n_t, t, n_h, p, n_w, p)\n",
        "print(tubelets.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# (B, C, n_t, t, n_h, p, n_w, p) -> (B, n_t, n_h, n_w, C, t, p, p)\n",
        "tubelets = tubelets.permute(0, 2, 4, 6, 1, 3, 5, 7).contiguous()\n",
        "print(tubelets.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# (B, n_t, n_h, n_w, C, t, p, p) -> ((B * n_t * n_h * n_w), C, t, p, p)\n",
        "tubelets = tubelets.reshape(-1, C, t, p, p)\n",
        "print(tubelets.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "conv3d_1 = nn.Conv3d(in_channels=C, out_channels=32, kernel_size=3, stride=2, padding=1)\n",
        "out1 = conv3d_1(tubelets)\n",
        "print(out1.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "conv3d_2 = nn.Conv3d(in_channels=32, out_channels=32, kernel_size=3, stride=2, padding=1)\n",
        "out2 = conv3d_2(out1)\n",
        "print(out2.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "conv3d_3 = nn.Conv3d(in_channels=32, out_channels=32, kernel_size=3, stride=2, padding=1)\n",
        "out3 = conv3d_3(out2)\n",
        "print(out3.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "conv3d_4 = nn.Conv3d(in_channels=32, out_channels=32, kernel_size=3, stride=(1, 2, 2), padding=1)\n",
        "out4 = conv3d_4(out3)\n",
        "print(out4.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "out5 = out4.reshape(-1, 32)\n",
        "print(out5.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# separate out batch dimension post VAE encoding since dim=0 is of shape (B * n_t * n_h * n_w)\n",
        "out6 = out5.reshape(out5.shape[0] // (n_t * n_h * n_w), -1, 32)\n",
        "print(out6.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "out7 = out6.reshape(-1, 32)\n",
        "print(out7.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "out7 = out7.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1)\n",
        "print(out7.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "decoder1 = nn.ConvTranspose3d(in_channels=32, out_channels=32, kernel_size=3, stride=(1, 2, 2), padding=1, output_padding=(0, 1, 1))\n",
        "out8 = decoder1(out7)\n",
        "print(out8.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "decoder2 = nn.ConvTranspose3d(in_channels=32, out_channels=32, kernel_size=3, stride=2, padding=1, output_padding=(1, 1, 1))\n",
        "out9 = decoder2(out8)\n",
        "print(out9.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "decoder3 = nn.ConvTranspose3d(in_channels=32, out_channels=32, kernel_size=3, stride=2, padding=1, output_padding=(1, 1, 1))\n",
        "out10 = decoder2(out9)\n",
        "print(out10.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "decoder3 = nn.ConvTranspose3d(in_channels=32, out_channels=32, kernel_size=3, stride=2, padding=1, output_padding=(1, 1, 1))\n",
        "out11 = decoder2(out10)\n",
        "print(out11.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# FSQ-VAE Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "resume_training = True\n",
        "\n",
        "config = VideoGenConfig()\n",
        "\n",
        "model = TubeletFSQVAE(config)\n",
        "lit_model = LitTubeletFSQVAE(model, config)\n",
        "\n",
        "steamboat_willie_data = SteamboatWillieDataModule(config)\n",
        "\n",
        "if resume_training:\n",
        "    run = wandb.init(project=config.project_name, config=config.to_dict(), resume=True)\n",
        "    artifact = run.use_artifact('CKPT NAME HERE', type='model')\n",
        "    artifact_dir = artifact.download()\n",
        "else:\n",
        "    wandb.init(project=config.project_name, config=config.to_dict())\n",
        "\n",
        "wandb_logger = WandbLogger(project=config.project_name, log_model=True)\n",
        "wandb_logger.watch(lit_model, log=\"all\")\n",
        "\n",
        "lr_monitor = LearningRateMonitor(logging_interval='step')\n",
        "\n",
        "checkpoint_callback = ModelCheckpoint(\n",
        "    dirpath=config.checkpoint_path,\n",
        "    filename='model-{epoch:02d}-{val_loss:.2f}',\n",
        "    every_n_epochs=5,\n",
        "    save_top_k=config.save_top_k,\n",
        "    monitor='val/loss',\n",
        "    mode='min',\n",
        "    save_last=True\n",
        ")\n",
        "\n",
        "# Define the EarlyStopping callback\n",
        "early_stop_callback = EarlyStopping(\n",
        "    monitor='val/loss',\n",
        "    min_delta=0.0000000,\n",
        "    patience=100,\n",
        "    verbose=True,\n",
        "    check_finite=True\n",
        ")\n",
        "\n",
        "trainer = pl.Trainer(\n",
        "    max_epochs=config.max_epochs,\n",
        "    devices=1,\n",
        "    accelerator=\"gpu\",\n",
        "    precision=\"16-mixed\",\n",
        "    logger=wandb_logger,\n",
        "    callbacks=[\n",
        "        lr_monitor,\n",
        "        early_stop_callback,\n",
        "        checkpoint_callback\n",
        "    ],\n",
        "    log_every_n_steps=1,\n",
        "    # gradient_clip_val=1.0,\n",
        "    # overfit_batches=1,\n",
        ")\n",
        "\n",
        "# tuner = Tuner(trainer)\n",
        "# lr_result = tuner.lr_find(lit_model, datamodule=steamboat_willie_data, max_lr=100)\n",
        "# lr_result.plot(show=True, suggest=True)\n",
        "if resume_training:\n",
        "    trainer.fit(lit_model, steamboat_willie_data, ckpt_path=f'{artifact_dir}/model.ckpt')\n",
        "else:\n",
        "    trainer.fit(lit_model, steamboat_willie_data)\n",
        "\n",
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# FSQ-VAE Sweeps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "config = VideoGenConfig()\n",
        "\n",
        "sweep_config = {\n",
        "    'method': 'random',\n",
        "    'metric': {\n",
        "        'name': 'val/loss',\n",
        "        'goal': 'minimize'\n",
        "    },\n",
        "    'parameters': {\n",
        "        'lr': {\n",
        "            'min': 1e-4,\n",
        "            'max': 2e-4,\n",
        "            'distribution': 'log_uniform_values'\n",
        "        },\n",
        "\n",
        "    }\n",
        "}\n",
        "\n",
        "sweep_id = wandb.sweep(sweep_config, project=config.project_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train():\n",
        "    with wandb.init() as run:\n",
        "        config = VideoGenConfig()\n",
        "        config.update(wandb.config)\n",
        "\n",
        "        model = TubeletFSQVAE(config)\n",
        "        lit_model = LitTubeletFSQVAE(model, config)\n",
        "        steamboat_willie_data = SteamboatWillieDataModule(config)\n",
        "        wandb_logger = WandbLogger(project=config.project_name, log_model=False)\n",
        "\n",
        "        early_stop_callback = EarlyStopping(\n",
        "            monitor='val/loss',\n",
        "            min_delta=0.00,\n",
        "            patience=10,\n",
        "            verbose=True,\n",
        "            check_finite=True\n",
        "        )\n",
        "\n",
        "        trainer = pl.Trainer(\n",
        "            max_epochs=20,\n",
        "            devices=1,\n",
        "            accelerator=\"gpu\",\n",
        "            precision=\"16-mixed\",\n",
        "            logger=wandb_logger,\n",
        "            callbacks=[early_stop_callback]\n",
        "        )\n",
        "\n",
        "        # tuner = Tuner(trainer)\n",
        "        # lr_result = tuner.lr_find(lit_model, datamodule=steamboat_willie_data, max_lr=1)\n",
        "        # lr_result.plot(show=True, suggest=True)\n",
        "\n",
        "        trainer.fit(lit_model, steamboat_willie_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# wandb.agent(sweep_id, train, count=25)\n",
        "wandb.agent(sweep_id, train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Latent Sequence Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class LatentSequenceDataset(Dataset):\n",
        "    def __init__(self, config, mode='train', train_split=0.8):\n",
        "        '''\n",
        "        We want to ensure that the transformer is only tasked with learning\n",
        "        relationships at the tubelet level for faster convergence, so we must\n",
        "        ensure chunks sampled are aligned with tubelet boundries\n",
        "        '''\n",
        "        self.config = config\n",
        "\n",
        "        if not os.path.exists(config.latent_seqs_dest_dir):\n",
        "            print('Building latent sequences...')\n",
        "            fsq_vae = TubeletFSQVAE(config)\n",
        "            checkpoint = torch.load(config.fsq_vae_checkpoint_path)\n",
        "            new_state_dict = {}\n",
        "            for key, value in checkpoint['state_dict'].items():\n",
        "                new_key = key.replace('model.', '')\n",
        "                new_state_dict[new_key] = value\n",
        "\n",
        "            fsq_vae.load_state_dict(new_state_dict)\n",
        "\n",
        "            clip_train_dataset = SteamboatWillieDataset(config, mode='train')\n",
        "            clip_val_dataset = SteamboatWillieDataset(config, mode='val')\n",
        "\n",
        "            clip_lat_seq_shape = (config.latents_per_clip, config.latent_channels)\n",
        "\n",
        "            self.build_latent_datasets(fsq_vae, clip_train_dataset, clip_val_dataset, clip_lat_seq_shape, config.latent_seqs_dest_dir)\n",
        "\n",
        "        if mode in ['train', 'val']: self.mode = mode\n",
        "    \n",
        "    def build_latent_datasets(self, enc_model, clip_train_dataset, clip_val_dataset, clip_lat_seq_shape, dest_dir):\n",
        "        \"\"\"\n",
        "        Build set of binary files to store encoded/quantized clip latents sequences\n",
        "        returns dict of latent_seq_idx -> mmapped file path\n",
        "        \"\"\"\n",
        "        if not os.path.exists(dest_dir):\n",
        "            os.makedirs(dest_dir)\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            enc_model.cuda()\n",
        "        \n",
        "        enc_model.eval()\n",
        "        for split, dataset in {'train': clip_train_dataset, 'val': clip_val_dataset}.items():\n",
        "            mmapped_file_path = os.path.join(dest_dir, f'{split}.bin')\n",
        "            # (number of clips x latents per clip, latent dim)\n",
        "            shape = (len(dataset) * clip_lat_seq_shape[0], clip_lat_seq_shape[1])\n",
        "            fp = np.memmap(mmapped_file_path, dtype='float32', mode='w+', shape=shape)\n",
        "            \n",
        "            idx = 0\n",
        "            for clip in tqdm(dataset, desc='Creating latent sequence .bin files'):\n",
        "                if torch.cuda.is_available():\n",
        "                    clip = clip.cuda()\n",
        "                with torch.no_grad():\n",
        "                    quantized = enc_model.quantize(enc_model.encode(clip.unsqueeze(0)))\n",
        "\n",
        "                # (B, 5, 1, 4, 4) -> (B, 1, 4, 4, 5) -> ((B*1*4*4), 5)\n",
        "                latent_seq_np = enc_model.latent_img_to_seq(quantized['z_q']).cpu().numpy()\n",
        "                fp[idx:idx + latent_seq_np.shape[0]] = latent_seq_np[:]\n",
        "                idx += latent_seq_np.shape[0]\n",
        "            \n",
        "            fp.flush()\n",
        "\n",
        "    def __len__(self):\n",
        "        '''\n",
        "        we need to ensure our idxs are aligned with tubelet boundaries (which contain\n",
        "        16 vectors each)\n",
        "        '''\n",
        "        data_file_path = os.path.join(self.config.latent_seqs_dest_dir, f'{self.mode}.bin')\n",
        "        total_sequences, _ = np.memmap(data_file_path, dtype='float32', mode='r').shape\n",
        "        return (total_sequences - (self.config.chunk_size - 1)) // 16\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        '''\n",
        "        we need to ensure our idxs are aligned with tubelet boundaries (which contain\n",
        "        16 vectors each)\n",
        "        '''\n",
        "        dataset = np.memmap(\n",
        "            os.path.join(self.config.latent_seqs_dest_dir, f'{self.mode}.bin'),\n",
        "            dtype='float32',\n",
        "            mode='r'\n",
        "        ).reshape(-1, self.config.latent_channels)\n",
        "\n",
        "        aligned_start_idx = idx * 16\n",
        "        \n",
        "        latent_seq = dataset[aligned_start_idx : aligned_start_idx + self.config.chunk_size]\n",
        "\n",
        "        return torch.tensor(latent_seq[:-1]), torch.tensor(latent_seq[1:])\n",
        "\n",
        "\n",
        "class LatentSequenceDataModule(pl.LightningDataModule):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.t_batch_size = config.t_batch_size\n",
        "        self.config = config\n",
        "\n",
        "    def setup(self, stage=None):\n",
        "        if stage == 'fit' or stage is None:\n",
        "            self.train_dataset = LatentSequenceDataset(self.config, mode='train')\n",
        "            self.val_dataset = LatentSequenceDataset(self.config, mode='val')\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return torch.utils.data.DataLoader(self.train_dataset, batch_size=self.t_batch_size, shuffle=True, num_workers=self.config.num_workers)\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return torch.utils.data.DataLoader(self.val_dataset, batch_size=self.t_batch_size, shuffle=False, num_workers=self.config.num_workers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "config = VideoGenConfig()\n",
        "train_dataset = LatentSequenceDataset(config, mode='train')\n",
        "val_dataset = LatentSequenceDataset(config, mode='val')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_batch = train_dataset[0]\n",
        "val_batch = val_dataset[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Transformer Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# verify clip sequence by decoding each latent sequence back to clips and display\n",
        "config = VideoGenConfig()\n",
        "latent_dataset = LatentSequenceDataset(config, mode='full')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "latent_dataset[0].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fsq_vae = TubeletFSQVAE(config)\n",
        "checkpoint = torch.load(config.fsq_vae_checkpoint_path)\n",
        "new_state_dict = {}\n",
        "for key, value in checkpoint['state_dict'].items():\n",
        "    new_key = key.replace('model.', '')\n",
        "    new_state_dict[new_key] = value\n",
        "\n",
        "fsq_vae.load_state_dict(new_state_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    fsq_vae.eval()\n",
        "    x, y = latent_dataset[-2]\n",
        "    x = torch.cat((x, y[-1].unsqueeze(0)), dim=0)\n",
        "    print(x.shape)\n",
        "    if torch.cuda.is_available():\n",
        "        fsq_vae = fsq_vae.cuda()\n",
        "        x = x.cuda()\n",
        "        x = x.reshape(-1, 1, 4, 4, 5).permute(0, 4, 1, 2, 3).contiguous()\n",
        "        print(x.shape)\n",
        "    clip_tubelets = fsq_vae.decode(x)\n",
        "    print(clip_tubelets.shape)\n",
        "    clip = fsq_vae.assemble_tubelets(\n",
        "        clip_tubelets,\n",
        "        B=1,\n",
        "        C=1,\n",
        "        T=config.clip_length*4,\n",
        "        H=config.img_size,\n",
        "        W=config.img_size,\n",
        "        t=config.t,\n",
        "        p=config.p\n",
        "    )\n",
        "    print(clip.shape)\n",
        "    x = x.cpu()\n",
        "    clip = clip.cpu()\n",
        "\n",
        "display_clip(clip.squeeze(0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "transformer = TransformerDecoder(\n",
        "    emb_dim=5,\n",
        "    qkv_dim=128,\n",
        "    nheads=8,\n",
        "    nlayers=5,\n",
        "    ctx_size=16383,\n",
        "    vocab_size=fsq_vae.quantizer.codebook_size\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "x, y = latent_dataset[-2]\n",
        "if torch.cuda.is_available():\n",
        "    transformer = transformer.to(torch.float16).cuda()\n",
        "    x = x.to(torch.float16).cuda()\n",
        "logits = transformer(x.unsqueeze(0))\n",
        "print(logits.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Transformer Decoder Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "resume_training = False\n",
        "\n",
        "config = VideoGenConfig()\n",
        "model = TransformerDecoder(config)\n",
        "lit_model = LitTransformerDecoder(model, config)\n",
        "latent_seq_data = LatentSequenceDataModule(config)\n",
        "\n",
        "if resume_training:\n",
        "    run = wandb.init(project=config.project_name, config=config.to_dict(), resume=True)\n",
        "    artifact = run.use_artifact('', type='model')\n",
        "    artifact_dir = artifact.download()\n",
        "else:\n",
        "    wandb.init(project=config.project_name, config=config.to_dict())\n",
        "\n",
        "wandb_logger = WandbLogger(project=config.project_name, log_model=True)\n",
        "wandb_logger.watch(lit_model, log=\"all\")\n",
        "\n",
        "lr_monitor = LearningRateMonitor(logging_interval='step')\n",
        "\n",
        "checkpoint_callback = ModelCheckpoint(\n",
        "    dirpath=config.checkpoint_path,\n",
        "    filename='model-{epoch:02d}-{val_loss:.2f}',\n",
        "    every_n_epochs=5,\n",
        "    save_top_k=config.save_top_k,\n",
        "    monitor='val/loss',\n",
        "    mode='min'\n",
        ")\n",
        "\n",
        "# Define the EarlyStopping callback\n",
        "early_stop_callback = EarlyStopping(\n",
        "    monitor='val/loss',\n",
        "    min_delta=0.0000000,\n",
        "    patience=100,\n",
        "    verbose=True,\n",
        "    check_finite=True\n",
        ")\n",
        "\n",
        "trainer = pl.Trainer(\n",
        "    max_epochs=config.max_epochs,\n",
        "    devices=1,\n",
        "    accelerator=\"gpu\",\n",
        "    precision=\"16-mixed\",\n",
        "    logger=wandb_logger,\n",
        "    callbacks=[\n",
        "        lr_monitor,\n",
        "        early_stop_callback,\n",
        "        # checkpoint_callback\n",
        "    ],\n",
        "    log_every_n_steps=1,\n",
        "    accumulate_grad_batches=config.grad_acc_steps\n",
        "    # gradient_clip_val=1.0,\n",
        "    # overfit_batches=1,\n",
        ")\n",
        "\n",
        "# tuner = Tuner(trainer)\n",
        "# lr_result = tuner.lr_find(lit_model, datamodule=LatentSequenceDataModule, max_lr=10)\n",
        "# lr_result.plot(show=True, suggest=True)\n",
        "if resume_training:\n",
        "    trainer.fit(lit_model, latent_seq_data, ckpt_path='')\n",
        "else:\n",
        "    trainer.fit(lit_model, latent_seq_data)\n",
        "\n",
        "wandb.finish()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
