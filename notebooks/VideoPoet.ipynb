{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JordanLazzaro/VideoGen/blob/main/notebooks/VideoPoet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kkynPQlHEnv3"
      },
      "source": [
        "# VideoPoet Implementation\n",
        "\n",
        "![](https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgxFblHaHRJNH7Oi2_oOTosGN9XrjgjhWmnfADchMT8WR0XAo6SxiUfpUmn5R6akciiRduaKIMdgwHZzK3xW8mErarQ_ugx41ctQAMK08O9UMVevgkk-AgFI1xYFWAomd16OcOh0R-XpyZVLQXncpk2SHf-RmPzrqBbIWZc-nUG2TH6nC2R7qyHXn8eTC-u/s2680/image21.png)\n",
        "\n",
        "[**VideoPoet**](https://research.google/blog/videopoet-a-large-language-model-for-zero-shot-video-generation/) is an autoregressive transformer decoder which models sequences of discrete, multimodal tokens produced by modality specific tokenizers; namely [MAGVIT-V2](https://arxiv.org/abs/2310.05737) for images/video, and [SoundStream](https://arxiv.org/abs/2107.03312) for audio. For text, the model leverages the T5 tokenizer as well as frozen T5 token embeddings. The set of these tokens together represent the model's full vocabulary. VideoPoet also employs a custom Super Resolution model to allow for lower resolution videos to be produced by the transformer for efficiency.\n",
        "\n",
        "Implementing the video portion of this model will require the following checkpoints:\n",
        "\n",
        "- [ ] Implement MAGVIT-V2 tokenizer\n",
        "    - [ ] Dialated Causal Convolution (in time dim)\n",
        "    - [ ] Blur Pool\n",
        "    - [ ] LFQ (can replace with fsq?)\n",
        "    - [ ] VQVAE (FSQVAE)\n",
        "    - [ ] Descriminator / GAN Loss\n",
        "- [ ] Implement the Transformer Decoder\n",
        "- [ ] Implement the Super Resolution model\n",
        "- [ ] Incorporate audio (optional/if feasible)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RuKdds-UMYoC"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i9W-O3PBEdrQ"
      },
      "outputs": [],
      "source": [
        "!pip install -q wandb pytorch_lightning av imageio\n",
        "!pip install -q flash-attn --no-build-isolation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hNaAMfnNORFg"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import gc\n",
        "import math\n",
        "import wandb\n",
        "import imageio\n",
        "import torch\n",
        "import torchvision\n",
        "import numpy as np\n",
        "import pytorch_lightning as pl\n",
        "from google.colab import drive\n",
        "from tqdm.notebook import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.animation import FuncAnimation\n",
        "from IPython.display import display, HTML\n",
        "from torch import nn, optim\n",
        "from torch.nn import functional as F\n",
        "from torchvision.datasets.video_utils import VideoClips\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torchvision.transforms import Compose, Lambda, Resize, ToTensor, CenterCrop, Grayscale\n",
        "import torchvision.transforms.functional as TF\n",
        "\n",
        "from pytorch_lightning import Callback\n",
        "from pytorch_lightning.loggers import WandbLogger\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint, LearningRateMonitor, EarlyStopping\n",
        "from pytorch_lightning.tuner import Tuner\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR, OneCycleLR\n",
        "\n",
        "from flash_attn import flash_attn_qkvpacked_func, flash_attn_func\n",
        "\n",
        "pl.seed_everything(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jxQSxmeSOV7q"
      },
      "outputs": [],
      "source": [
        "drive.mount('/content/drive')\n",
        "steamboat_willie_gdrive_path = '/content/drive/My Drive/SteamboatWillie/SteamboatWillie.mp4'\n",
        "!cp -r /content/drive/My\\ Drive/SteamboatWillie/clips ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MELKvzCbOfeZ"
      },
      "outputs": [],
      "source": [
        "def display_clip(clip):\n",
        "    '''\n",
        "    util method for displaying tensors as video\n",
        "\n",
        "    expects clip.shape = (C,T,H,W)\n",
        "    '''\n",
        "    assert len(clip.shape) == 4, 'clip shape must be PyTorch Tensor of shape (C,T,H,W)'\n",
        "\n",
        "    def update(frame_idx):\n",
        "        ax.clear()\n",
        "        ax.imshow(video_clip_np[frame_idx], cmap='gray')\n",
        "        ax.axis('off')\n",
        "\n",
        "    video_clip_np = clip.permute(1, 2, 3, 0).numpy()\n",
        "    fig, ax = plt.subplots()\n",
        "    ani = FuncAnimation(fig, update, frames=range(video_clip_np.shape[0]), interval=60)\n",
        "    plt.close()\n",
        "    display(HTML(ani.to_html5_video()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "config = {\n",
        "    \"project\": \"SteamboatWillie VideoPoet\",\n",
        "    \"data\": {\n",
        "        \"paths\": [steamboat_willie_gdrive_path],\n",
        "        \"clip_length\": 16,\n",
        "        \"clip_dest_dir\": \"clips\"\n",
        "    },\n",
        "    \"model\": {\n",
        "        \"magvit_v2\": {\n",
        "            \"checkpoint_dir\": \"./checkpoints/magvit_v2\"\n",
        "        },\n",
        "        \"transformer\": {\n",
        "            \"emb_dim\": 512,\n",
        "            \"nheads\": 8,\n",
        "            \"ctx_size\": 2048,\n",
        "            \"window_size\": 1024,\n",
        "            \"fan_out\": 4,\n",
        "            \"nlayers\": 6,\n",
        "            \"dropout\": 0.0,\n",
        "            \"use_flash_attn\": True,\n",
        "            \"checkpoint_dir\": \"./checkpoints/transformer\"\n",
        "        },\n",
        "        \"super_resolution\": {\n",
        "            \"checkpoint_dir\": \"./checkpoints/super_resolution\"\n",
        "        }\n",
        "    },\n",
        "    \"training\": {\n",
        "        \"magvit_v2\": {},\n",
        "        \"transformer\": {},\n",
        "        \"super_resolution\": {}\n",
        "    },\n",
        "    \"logging\": {}\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uF_3GgiCM2_o"
      },
      "source": [
        "# Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fxbeDgZAN1fJ"
      },
      "source": [
        "## Clips Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cVyXbEcTM5lG"
      },
      "outputs": [],
      "source": [
        "class RandomHorizontalFlipVideo(torch.nn.Module):\n",
        "    def __init__(self, p=0.5):\n",
        "        super().__init__()\n",
        "        self.p = p\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape is expected to be (C, T, H, W)\n",
        "        if torch.rand(1) < self.p:\n",
        "            # flip all frames in the clip\n",
        "            return x.flip(-1)\n",
        "        return x\n",
        "\n",
        "\n",
        "class SteamboatWillieDataset(Dataset):\n",
        "    def __init__(self, config, mode='train', train_split=0.8, shuffle=True, augment=True):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.train_split = train_split\n",
        "        self.mode = mode\n",
        "\n",
        "        self.preprocess_transforms = Compose([\n",
        "                Lambda(lambda x: x.permute(0, 3, 1, 2)),   # (T, H, W, C) to (T, C, H, W) for Greyscale\n",
        "                Grayscale(num_output_channels=1),          # Convert to grayscale\n",
        "                Lambda(lambda x: x.permute(1, 0, 2, 3)),   # (T, C, H, W) to (C, T, H, W) for Conv3d\n",
        "                CenterCrop((480, 575)),                    # Center crop to remove virtical bars\n",
        "                Resize((config.img_size, config.img_size))\n",
        "        ])\n",
        "\n",
        "        self.postprocess_transforms = Compose([\n",
        "            Lambda(lambda x: x / 255.),\n",
        "            Lambda(lambda x: x.view(self.config.in_channels, self.config.clip_length, self.config.img_size, self.config.img_size))\n",
        "        ])\n",
        "\n",
        "        if self.mode == 'train' and augment:\n",
        "            self.postprocess_transforms.transforms.append(RandomHorizontalFlipVideo(p=0.5))\n",
        "\n",
        "        if os.path.exists(config.clip_dest_dir):\n",
        "            clip_paths = self.build_existing_clip_paths(config.clip_dest_dir)\n",
        "            self.clips = self.build_clip_refs(clip_paths)\n",
        "        else:\n",
        "            video_clips = VideoClips(\n",
        "                config.paths,\n",
        "                clip_length_in_frames=config.clip_length,\n",
        "                frames_between_clips=config.clip_length\n",
        "            )\n",
        "\n",
        "            self.clips = self.build_clip_refs(self.build_clip_paths(video_clips, self.preprocess_transforms, config.clip_dest_dir))\n",
        "\n",
        "        if mode in ['train', 'val']:\n",
        "            total_clips = len(self.clips)\n",
        "            if shuffle:\n",
        "                indices = torch.randperm(total_clips).tolist()\n",
        "            else:\n",
        "                indices = list(range(total_clips))\n",
        "\n",
        "            train_size = int(total_clips * train_split)\n",
        "\n",
        "            if mode == 'train':\n",
        "                self.clip_indices = indices[:train_size]\n",
        "            else:\n",
        "                self.clip_indices = indices[train_size:]\n",
        "        elif mode == 'full':\n",
        "            self.clip_indices = list(range(len(self.clips)))\n",
        "\n",
        "    def build_clip_paths(self, video_clips, transforms, clip_dest_dir):\n",
        "        \"\"\"\n",
        "        Build set of binary files to store processed video clips\n",
        "        returns dict of clip_idx -> mmapped file path\n",
        "        \"\"\"\n",
        "        clip_paths = {}\n",
        "\n",
        "        if not os.path.exists(clip_dest_dir):\n",
        "            os.makedirs(clip_dest_dir)\n",
        "\n",
        "        for idx in tqdm(range(video_clips.num_clips()), desc='Creating clip .bin files'):\n",
        "            # transform clips and write to mmap file\n",
        "            clip, _, _, _ = video_clips.get_clip(idx)\n",
        "            clip = self.preprocess_transforms(clip)\n",
        "            clip_np = clip.numpy().astype(np.uint8)\n",
        "\n",
        "            mmapped_file_path = os.path.join(clip_dest_dir, f'clip_{idx}.bin')\n",
        "            fp = np.memmap(mmapped_file_path, dtype='uint8', mode='w+', shape=clip_np.shape)\n",
        "            fp[:] = clip_np[:]\n",
        "            fp.flush()\n",
        "            del fp\n",
        "            clip_paths[idx] = mmapped_file_path\n",
        "\n",
        "        return clip_paths\n",
        "\n",
        "    def build_existing_clip_paths(self, clip_dest_dir):\n",
        "        \"\"\"\"\n",
        "        returns dict of clip_idx -> mmapped file path\n",
        "        from existing .bin files\n",
        "        \"\"\"\n",
        "        clips_paths = {}\n",
        "        for filename in os.listdir(clip_dest_dir):\n",
        "            if filename.startswith('clip_') and filename.endswith('.bin'):\n",
        "                idx = int(filename.split('_')[1].split('.')[0])\n",
        "                file_path = os.path.join(clip_dest_dir, filename)\n",
        "                clips_paths[idx] = file_path\n",
        "\n",
        "        return clips_paths\n",
        "\n",
        "    def build_clip_refs(self, clip_paths):\n",
        "        \"\"\"\n",
        "        Build mmap reference to bin files\n",
        "        returns dict of clip_idx -> np.array mmapped to respective bin file\n",
        "        \"\"\"\n",
        "        clips = {}\n",
        "        for idx, path in tqdm(clip_paths.items(), desc='Building clip refs'):\n",
        "            clips[idx] = np.memmap(path, dtype='uint8', mode='r')\n",
        "\n",
        "        return clips\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.clip_indices)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        clip = self.clips[self.clip_indices[idx]]\n",
        "        return self.postprocess_transforms(torch.tensor(clip, dtype=torch.float32))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Lightning Datamodule"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SteamboatWillieDataModule(pl.LightningDataModule):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.batch_size = config.batch_size\n",
        "        self.config = config\n",
        "\n",
        "    def setup(self, stage=None):\n",
        "        if stage == 'fit' or stage is None:\n",
        "            self.train_dataset = SteamboatWillieDataset(self.config, mode='train')\n",
        "            self.val_dataset = SteamboatWillieDataset(self.config, mode='val')\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return torch.utils.data.DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True, num_workers=self.config.num_workers)\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return torch.utils.data.DataLoader(self.val_dataset, batch_size=self.batch_size, shuffle=False, num_workers=self.config.num_workers)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MLC7Er-SMcWj"
      },
      "source": [
        "# MAGVIT-V2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PPSOB-ZmMvm7"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Components"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YJEpGq2DMfY8"
      },
      "outputs": [],
      "source": [
        "class CausalConv3d(nn.Module):\n",
        "    '''\n",
        "    enforces causality in the time dimension (https://paperswithcode.com/method/causal-convolution)\n",
        "    https://github.com/lucidrains/magvit2-pytorch/blob/9f49074179c912736e617d61b32be367eb5f993a/magvit2_pytorch/magvit2_pytorch.py#L889\n",
        "    '''\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=(3,3,3), stride=1, dialation=1):\n",
        "        super().__init__()\n",
        "        k_t, k_h, k_w = kernel_size\n",
        "\n",
        "        # pad: (left, right, top, bottom, front, back)\n",
        "        pad_w, pad_h, pad_t = k_w//2, k_h//2, dilation * (k_t - 1) + (1 - stride)\n",
        "        self.t_causal_pad = (pad_w, pad_w, pad_h, pad_h, pad_t, 0)\n",
        "\n",
        "        stride = (stride, 1, 1)\n",
        "        dilation = (dilation, 1, 1)\n",
        "        \n",
        "        self.conv = nn.Conv3d(in_channels, out_channels, kernel_size, stride, dialation)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.pad(x, self.t_causal_pad)\n",
        "        x = self.conv(x)\n",
        "        \n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ResBlock3d(nn.Module):\n",
        "    ''' handles both same and different in/out channels '''\n",
        "    def __init__(\n",
        "            self,\n",
        "            in_channels,\n",
        "            out_channels,\n",
        "            kernel_size=(3,3,3)\n",
        "        ):\n",
        "        super().__init__()\n",
        "        if in_channels != out_channels:\n",
        "            self.identity = CausalConv3d(\n",
        "                in_channels=in_channels,\n",
        "                out_channels=out_channels,\n",
        "                kernel_size=(1, 1, 1)\n",
        "            )\n",
        "        else:\n",
        "            self.identity = nn.Identity()\n",
        "\n",
        "        self.block = nn.Sequential(\n",
        "            nn.GroupNorm(num_groups=2, num_channels=in_channels),\n",
        "            nn.SiLU(),\n",
        "            CausalConv3d(\n",
        "                in_channels=in_channels,\n",
        "                out_channels=out_channels,\n",
        "                kernel_size=kernel_size,\n",
        "                stride=1,\n",
        "                dialation=1\n",
        "            ),\n",
        "            nn.GroupNorm(num_groups=2, num_channels=out_channels),\n",
        "            nn.SiLU(),\n",
        "            CausalConv3d(\n",
        "                in_channels=out_channels,\n",
        "                out_channels=out_channels,\n",
        "                kernel_size=kernel_size,\n",
        "                stride=1,\n",
        "                dialation=1\n",
        "            ),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.block(x) + self.identity(x)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ResBlockDown3d(nn.Module):\n",
        "    ''' strided conv for down-sampling + blur pooling (https://arxiv.org/abs/1904.11486) ''' \n",
        "    def __init__(self, in_channels, out_hannels):\n",
        "        super().__init__()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class LFQ(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class FSQ(nn.Module):\n",
        "    def __init__(self, levels, eps=1e-3):\n",
        "        super().__init__()\n",
        "        self.register_buffer('levels', torch.tensor(levels))\n",
        "        # self.register_buffer(\n",
        "        #     'basis',\n",
        "        #     torch.cumprod(torch.tensor([1] + levels[:-1]), dim=0, dtype=torch.int32)\n",
        "        # )\n",
        "\n",
        "        self.register_buffer('basis', \n",
        "            torch.cat([\n",
        "                torch.tensor([1]),\n",
        "                torch.cumprod(torch.tensor(levels[:-1]), dim=0)\n",
        "            ], dim=0)\n",
        "        )\n",
        "\n",
        "        self.eps = eps\n",
        "        self.codebook_size = torch.prod(self.levels)\n",
        "\n",
        "    def round_ste(self, z):\n",
        "        z_q = torch.round(z)\n",
        "        return z + (z_q - z).detach()\n",
        "\n",
        "    def quantize(self, z):\n",
        "        # half_l is used to determine how to scale tanh; we\n",
        "        # subtract 1 from the number of levels to account for 0\n",
        "        # being a quantization bin and tanh being symmetric around 0\n",
        "        half_l = (self.levels - 1) * (1 - self.eps) / 2\n",
        "\n",
        "        # if a given level is even, it will result in a scale for tanh\n",
        "        # which is halfway between integer values, so we offset\n",
        "        # the tanh output down by 0.5 to line it with whole integers\n",
        "        offset = torch.where(self.levels % 2 == 0, 0.5, 0.0)\n",
        "\n",
        "        # if our level is even, we want to shift the tanh input to\n",
        "        # ensure the 0 quantization bin is centered\n",
        "        shift = torch.tan(offset / half_l)\n",
        "\n",
        "        # once we have our shift and offset (in the case of an even level)\n",
        "        # we can round to the nearest integer bin and allow for STE\n",
        "        z_q = self.round_ste(torch.tanh(z + shift) * half_l - offset)\n",
        "\n",
        "        # after quantization, we want to renormalize the quantized\n",
        "        # values to be within the range expected by the model (ie. [-1, 1])\n",
        "        half_width = self.levels // 2\n",
        "        return z_q / half_width\n",
        "\n",
        "    def scale_and_shift(self, z_q_normalized):\n",
        "        half_width = self.levels // 2\n",
        "        return (z_q_normalized * half_width) + half_width\n",
        "\n",
        "    def scale_and_shift_inverse(self, z_q):\n",
        "        half_width = self.levels // 2\n",
        "        return (z_q - half_width) / half_width\n",
        "\n",
        "    def codes_to_idxs(self, z_q):\n",
        "        assert z_q.shape[-1] == len(self.levels)\n",
        "        z_q = self.scale_and_shift(z_q)\n",
        "        return (z_q * self.basis).sum(dim=-1).to(torch.int32)\n",
        "\n",
        "    def idxs_to_codes(self, idxs):\n",
        "        idxs = idxs.unsqueeze(-1)\n",
        "        codes_not_centered = (idxs // self.basis) % self.levels\n",
        "        return self.scale_and_shift_inverse(codes_not_centered)\n",
        "\n",
        "    def forward(self, z):\n",
        "        # TODO: make this work for generic tensor sizes\n",
        "        # TODO: use einops to clean up\n",
        "\n",
        "        if len(z.shape) == 5: # video\n",
        "            B, C, T, H, W = z.shape\n",
        "            # (B, C, T, H, W) -> (B, T, H, W, C)\n",
        "            z_c_last = z.permute(0, 2, 3, 4, 1).contiguous()\n",
        "            # (B, T, H, W, C) -> (BTHW, C)\n",
        "            z_flatten = z_c_last.reshape(-1, C)\n",
        "            z_flatten_q = self.quantize(z_flatten)\n",
        "            # (BTHW, C) -> (B, T, H, W, C) -> (B, C, T, H, W)\n",
        "            z_q = z_flatten_q.reshape(B, T, H, W, C).permute(0, 4, 1, 2, 3).contiguous()\n",
        "\n",
        "        elif len(z.shape) == 4: # image\n",
        "            B, C, H, W = z.shape\n",
        "            # (B, C, H, W) -> (B, H, W, C)\n",
        "            z_c_last = z.permute(0, 2, 3, 1).contiguous()\n",
        "            # (B, H, W, C) -> (BHW, C)\n",
        "            z_flatten = z_c_last.reshape(-1, C)\n",
        "            z_flatten_q = self.quantize(z_flatten)\n",
        "            # (BHW, C) -> (B, H, W, C) -> (B, C, T, H, W)\n",
        "            z_q = z_flatten_q.reshape(B, H, W, C).permute(0, 3, 1, 2).contiguous()\n",
        "\n",
        "        elif len(z.shape) == 2: # vector sequence\n",
        "            # (B, C)\n",
        "            z_q = self.quantize(z)\n",
        "\n",
        "        return {'z_q': z_q}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class VQVAE(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Descriminator(nn.Module):\n",
        "    ''' bunch of downsampling resblocks + leaky relu '''\n",
        "    def __init__(self, config):\n",
        "        super().__init__()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MAGVIT2Tokenizer(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Lightning Module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VrLdChwBNNNO"
      },
      "source": [
        "## Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tYuhx1CPNPfD"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0QVpgOrNP6Q"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IlYmZCUXNTOR"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pcVSPBv2NTtF"
      },
      "source": [
        "## Sweeps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u4yMRkawNVWg"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UCDDpPuCMgAa"
      },
      "source": [
        "# Transformer Decoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gxsxnrTbM8xV"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FXyFcoc6Mj6X"
      },
      "outputs": [],
      "source": [
        "class Attention(nn.Module):\n",
        "    def __init__(\n",
        "            self,\n",
        "            emb_dim,\n",
        "            nheads,\n",
        "            ctx_size,\n",
        "            window_size,\n",
        "            dropout=0.0,\n",
        "            use_flash_attn=True\n",
        "        ):\n",
        "        super().__init__()\n",
        "        assert emb_dim % nheads == 0\n",
        "\n",
        "        self.W_Q = nn.Linear(emb_dim, emb_dim, bias=False)\n",
        "        self.W_K = nn.Linear(emb_dim, emb_dim, bias=False)\n",
        "        self.W_V = nn.Linear(emb_dim, emb_dim, bias=False)\n",
        "\n",
        "        self.W_O = nn.Linear(emb_dim, emb_dim, bias=False)\n",
        "\n",
        "        if not use_flash_attn:\n",
        "            self.register_buffer(\n",
        "                \"mask\",\n",
        "                torch.tril(torch.ones((ctx_size, ctx_size))).reshape(\n",
        "                    1, 1, ctx_size, ctx_size\n",
        "                ),\n",
        "            )\n",
        "\n",
        "        self.head_dim = emb_dim // nheads\n",
        "        self.nheads = nheads\n",
        "        self.window_size = window_size\n",
        "\n",
        "        self.dropout_p = dropout\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        self.use_flash_attn = use_flash_attn\n",
        "\n",
        "    def forward(self, x, slopes):\n",
        "        B, T, D = x.size()\n",
        "\n",
        "        # (B, T, D) -> (B, T, H, H_D)\n",
        "        Q = self.W_Q(x).reshape(B, T, self.nheads, self.head_dim)\n",
        "        K = self.W_K(x).reshape(B, T, self.nheads, self.head_dim)\n",
        "        V = self.W_V(x).reshape(B, T, self.nheads, self.head_dim)\n",
        "\n",
        "        if self.use_flash_attn:\n",
        "            out = flash_attn_func(\n",
        "                Q, K, V,\n",
        "                dropout_p=self.dropout_p if self.training else 0.0,\n",
        "                softmax_scale=None,\n",
        "                causal=True,\n",
        "                window_size=(self.window_size, 0),\n",
        "                alibi_slopes=slopes.to(torch.float32),\n",
        "                deterministic=False\n",
        "            )\n",
        "        else:\n",
        "            # (B, T, H, H_D) -> (B, H, T, H_D)\n",
        "            Q = Q.transpose(1, 2)\n",
        "            K = K.transpose(1, 2)\n",
        "            V = V.transpose(1, 2)\n",
        "\n",
        "            # (B, H, T, H_D) @ (B, H, H_D, T) -> (B, H, T, T)\n",
        "            attn = (Q @ K.transpose(-2, -1)) / (1.0 * math.sqrt(self.head_dim))\n",
        "            # attn = attn + bias\n",
        "            attn = attn.masked_fill(self.mask[:,:,:T,:T]==0, float('-inf'))\n",
        "            attn = F.softmax(attn, dim=-1)\n",
        "\n",
        "            attn = self.dropout(attn)\n",
        "\n",
        "            # ((B, H, T, T)) @ (B, H, T, H_D) -> (B, H, T, H_D)\n",
        "            out = attn @ V\n",
        "\n",
        "        # (B, H, T, H_D) -> (B, T, H, H_D) -> (B, T, D)\n",
        "        out = out.transpose(1, 2).reshape(B, T, self.emb_dim)\n",
        "\n",
        "        # (B, T, D)\n",
        "        out = self.W_O(out)\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self, emb_dim, fan_out=4, dropout=0.0):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(emb_dim, fan_out * emb_dim)\n",
        "        self.act = nn.GELU()\n",
        "        self.fc2 = nn.Linear(fan_out * emb_dim, emb_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.act(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(\n",
        "            self,\n",
        "            emb_dim,\n",
        "            nheads,\n",
        "            ctx_size,\n",
        "            window_size,\n",
        "            fan_out=4,\n",
        "            dropout=0.0,\n",
        "            use_flash_attn=True\n",
        "        ):\n",
        "        super().__init__()\n",
        "        self.ln_1 = nn.LayerNorm(emb_dim)\n",
        "        self.attn = Attention(\n",
        "            emb_dim,\n",
        "            nheads,\n",
        "            ctx_size,\n",
        "            window_size,\n",
        "            dropout,\n",
        "            use_flash_attn\n",
        "        )\n",
        "        self.ln_2 = nn.LayerNorm(emb_dim)\n",
        "        self.mlp = MLP(emb_dim, fan_out, dropout)\n",
        "\n",
        "    def forward(self, x, slopes):\n",
        "        x = x + self.attn(self.ln_1(x), slopes)\n",
        "        x = x + self.mlp(self.ln_2(x))\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TransformerDecoder(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.blocks = nn.ModuleList([\n",
        "            TransformerBlock(\n",
        "                config.emb_dim,\n",
        "                config.nheads,\n",
        "                config.ctx_size,\n",
        "                config.window_size,\n",
        "                config.fan_out,\n",
        "                config.dropout,\n",
        "                config.use_flash_attn\n",
        "            ) for _ in range(config.nlayers)\n",
        "        ])\n",
        "        self.pred_head = nn.Linear(config.emb_dim, config.vocab_size, bias=False)\n",
        "        self.ln_f = nn.LayerNorm(config.emb_dim)\n",
        "\n",
        "        self.register_buffer(\"slopes\", self.get_alibi_slope(config.nheads))\n",
        "        self.window_size = config.window_size\n",
        "\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if not self.config.use_flash_attn:\n",
        "            bias = (self.m * self.get_relative_positions(self.window_size).to(x.device)).unsqueeze(0)\n",
        "\n",
        "        x = self.emb_up_proj(x)\n",
        "\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        for block in self.blocks:\n",
        "            x = block(x, self.slopes)\n",
        "\n",
        "        logits = self.pred_head(self.ln_f(x))\n",
        "\n",
        "        return logits\n",
        "\n",
        "    def get_relative_positions(self, seq_len: int) -> torch.tensor:\n",
        "        x = torch.arange(seq_len)[None, :]\n",
        "        y = torch.arange(seq_len)[:, None]\n",
        "        return (x - y).clamp_max_(0)\n",
        "\n",
        "\n",
        "    def get_alibi_slope(self, num_heads):\n",
        "        x = (2 ** 8) ** (1 / num_heads)\n",
        "        if self.config.use_flash_attn:\n",
        "            x = torch.tensor([1 / x ** (i + 1) for i in range(num_heads)])\n",
        "        else:\n",
        "            x = torch.tensor([1 / x ** (i + 1) for i in range(num_heads)]).unsqueeze(-1).unsqueeze(-1)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Lightning Module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class StepwiseValidationScheduleCallback(Callback):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "\n",
        "    def on_batch_end(self, trainer, pl_module):\n",
        "        if (trainer.global_step + 1) % self.config.validation_step_interval == 0:\n",
        "            trainer.validate(pl_module)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class StepwiseModelCheckpoint(ModelCheckpoint):\n",
        "    def __init__(self, save_step_frequency, dirpath, filename=\"{step}\", **kwargs):\n",
        "        super().__init__(dirpath=dirpath, filename=filename, **kwargs)\n",
        "        self.save_step_frequency = save_step_frequency\n",
        "\n",
        "    def on_train_batch_end(self, trainer, pl_module, outputs, batch, batch_idx, dataloader_idx):\n",
        "        super().on_train_batch_end(trainer, pl_module, outputs, batch, batch_idx, dataloader_idx=dataloader_idx)\n",
        "\n",
        "        global_step = trainer.global_step\n",
        "        if global_step % self.save_step_frequency == 0:\n",
        "            filepath = self.format_checkpoint_name(global_step, {}, verbatim=True).format(step=global_step)\n",
        "            filepath = os.path.join(self.dirpath, f\"{filepath}-step={global_step}.ckpt\")\n",
        "            self._save_model(filepath, trainer, pl_module)\n",
        "\n",
        "            if self.save_top_k > 0:\n",
        "                self._del_model(trainer, pl_module)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class LitTransformerDecoder(pl.LightningModule):\n",
        "    def __init__(self, model, config):\n",
        "        super().__init__()\n",
        "        self.model = model\n",
        "        self.config = config\n",
        "        self.lr = config.lr\n",
        "\n",
        "        if self.logger:\n",
        "            self.logger.experiment.config.update(config)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        logits = self(x)\n",
        "        \n",
        "        loss = F.cross_entropy(logits.reshape(-1, logits.size(-1)), y.reshape(-1))\n",
        "\n",
        "        self.log('train/loss', loss, prog_bar=True)\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        logits = self(x)\n",
        "\n",
        "        loss = F.cross_entropy(logits.reshape(-1, logits.size(-1)), y.reshape(-1))\n",
        "\n",
        "        self.log('val/loss', loss, prog_bar=True)\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.AdamW(\n",
        "            self.parameters(),\n",
        "            lr=self.lr,\n",
        "            betas=(self.config.beta1, self.config.beta2),\n",
        "            weight_decay=self.config.weight_decay\n",
        "        )\n",
        "\n",
        "        if self.config.use_lr_schedule:\n",
        "            scheduler = {\n",
        "                'scheduler': CosineAnnealingLR(optimizer, T_max=self.config.training_steps, eta_min=0),\n",
        "                'interval': 'step',\n",
        "                'frequency': 1,\n",
        "            }\n",
        "            return [optimizer], [scheduler]\n",
        "\n",
        "        return optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TBX-OdpfM_IU"
      },
      "source": [
        "## Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4tHjTDGLNC7J"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M3bwY6WVND8P"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DSG27RTpNFdx"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HDLsJi3QNGjt"
      },
      "source": [
        "## Sweeps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UHoSpKtTNIWv"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AHuAfaKqMpBx"
      },
      "source": [
        "# Super Resolution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bsyvtzf6NZgX"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WQACSEuLMrnF"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Lightning Module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rBHkOWpENbnJ"
      },
      "source": [
        "## Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3FEufD9mNfP0"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wYox-rbfNftJ"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pGmjT20lNhIh"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UmjlioSINhvk"
      },
      "source": [
        "## Sweeps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pi6OQ4EeNjg7"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyO3KrMG+gJUO4IfDxp7ZPmb",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
