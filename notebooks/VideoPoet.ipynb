{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JordanLazzaro/VideoGen/blob/main/notebooks/VideoPoet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kkynPQlHEnv3"
      },
      "source": [
        "# VideoPoet Implementation\n",
        "\n",
        "![](https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgxFblHaHRJNH7Oi2_oOTosGN9XrjgjhWmnfADchMT8WR0XAo6SxiUfpUmn5R6akciiRduaKIMdgwHZzK3xW8mErarQ_ugx41ctQAMK08O9UMVevgkk-AgFI1xYFWAomd16OcOh0R-XpyZVLQXncpk2SHf-RmPzrqBbIWZc-nUG2TH6nC2R7qyHXn8eTC-u/s2680/image21.png)\n",
        "\n",
        "[**VideoPoet**](https://research.google/blog/videopoet-a-large-language-model-for-zero-shot-video-generation/) is an autoregressive transformer decoder which models sequences of discrete, multimodal tokens produced by modality specific tokenizers; namely [MAGVIT-V2](https://arxiv.org/abs/2310.05737) for images/video, and [SoundStream](https://arxiv.org/abs/2107.03312) for audio. For text, the model leverages the T5 tokenizer as well as frozen T5 token embeddings. The set of these tokens together represent the model's full vocabulary. VideoPoet also employs a custom Super Resolution model to allow for lower resolution videos to be produced by the transformer for efficiency.\n",
        "\n",
        "Implementing the video portion of this model will require the following checkpoints:\n",
        "\n",
        "- [ ] Implement MAGVIT-V2 tokenizer\n",
        "    - [ ] Dialated Causal Convolution (in time dim)\n",
        "    - [ ] Blur Pool\n",
        "    - [ ] LFQ (can replace with fsq?)\n",
        "    - [ ] VQVAE (FSQVAE)\n",
        "    - [ ] Descriminator / GAN Loss\n",
        "- [ ] Implement the Transformer Decoder\n",
        "- [ ] Implement the Super Resolution model\n",
        "- [ ] Incorporate audio (optional/if feasible)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RuKdds-UMYoC"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i9W-O3PBEdrQ"
      },
      "outputs": [],
      "source": [
        "!pip install -q wandb pytorch_lightning av imageio\n",
        "!pip install -q flash-attn --no-build-isolation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hNaAMfnNORFg"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import gc\n",
        "import math\n",
        "import wandb\n",
        "import imageio\n",
        "import torch\n",
        "import torchvision\n",
        "import numpy as np\n",
        "import pytorch_lightning as pl\n",
        "from tqdm.notebook import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.animation import FuncAnimation\n",
        "from IPython.display import display, HTML\n",
        "from torch import nn, optim, einsum\n",
        "from torch.nn import functional as F\n",
        "from torchvision.datasets.video_utils import VideoClips\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torch.autograd import grad as torch_grad\n",
        "from torch.cuda.amp import autocast\n",
        "from torchvision.transforms import Compose, Lambda, Resize, ToTensor, CenterCrop, Grayscale\n",
        "import torchvision.transforms.functional as TF\n",
        "from einops import rearrange, repeat, reduce, pack, unpack\n",
        "from einops.layers.torch import Rearrange\n",
        "\n",
        "from pytorch_lightning import Callback\n",
        "from pytorch_lightning.loggers import WandbLogger\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint, LearningRateMonitor, EarlyStopping\n",
        "from pytorch_lightning.tuner import Tuner\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR, OneCycleLR\n",
        "\n",
        "from flash_attn import flash_attn_qkvpacked_func, flash_attn_func\n",
        "\n",
        "pl.seed_everything(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jxQSxmeSOV7q"
      },
      "outputs": [],
      "source": [
        "drive.mount('/content/drive')\n",
        "steamboat_willie_gdrive_path = '/content/drive/My Drive/SteamboatWillie/SteamboatWillie.mp4'\n",
        "!cp -r /content/drive/My\\ Drive/SteamboatWillie/clips ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MELKvzCbOfeZ"
      },
      "outputs": [],
      "source": [
        "def display_clip(clip):\n",
        "    '''\n",
        "    util method for displaying tensors as video\n",
        "\n",
        "    expects clip.shape = (C,T,H,W)\n",
        "    '''\n",
        "    assert len(clip.shape) == 4, 'clip shape must be PyTorch Tensor of shape (C,T,H,W)'\n",
        "\n",
        "    def update(frame_idx):\n",
        "        ax.clear()\n",
        "        ax.imshow(video_clip_np[frame_idx], cmap='gray')\n",
        "        ax.axis('off')\n",
        "\n",
        "    video_clip_np = clip.permute(1, 2, 3, 0).numpy()\n",
        "    fig, ax = plt.subplots()\n",
        "    ani = FuncAnimation(fig, update, frames=range(video_clip_np.shape[0]), interval=60)\n",
        "    plt.close()\n",
        "    display(HTML(ani.to_html5_video()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "config = {\n",
        "    \"project\": {\n",
        "        \"magvit2\": {\n",
        "            \"name\": \"SteamboatWillie VideoPoet\"\n",
        "            \"wandb_project\": \"SteamboatWillie VideoPoet\"\n",
        "        }\n",
        "    },\n",
        "    \"data\": {\n",
        "        \"paths\": ['/content/drive/My Drive/SteamboatWillie/SteamboatWillie.mp4'],\n",
        "        \"clip_length\": 16,\n",
        "        \"clip_dest_dir\": \"clips\"\n",
        "    },\n",
        "    \"magvit2\": {\n",
        "        \"fsqvae\": {\n",
        "            \"encoder\": {\n",
        "                \"in_channels\": 1,\n",
        "                \"out_channels\": 5,\n",
        "                \"init_channels\": 32,\n",
        "                \"num_downsamples\": 3,\n",
        "                \"nblocks\": 2\n",
        "            },\n",
        "            \"fsq\": {\n",
        "                \"levels\": [9, 9, 9, 9, 9],\n",
        "            },\n",
        "            \"decoder\": {\n",
        "                \"in_channels\": 5,\n",
        "                \"out_channels\": 1,\n",
        "                \"init_channels\": 256,\n",
        "                \"num_upsamples\": 3,\n",
        "                \"nblocks\": 2\n",
        "            }\n",
        "        },\n",
        "        \"discriminator\": {\n",
        "            \"input_shape\": (1, 16, 128, 128),\n",
        "            \"in_channels\": 1,\n",
        "            \"init_channels\": 32,\n",
        "            \"num_downsamples\": 5,\n",
        "            \"use_grad_penalty\": True,\n",
        "            \"grad_penalty_weight\": 10,\n",
        "        },\n",
        "        \"gan_loss_weight\": 0.1,\n",
        "        \"recon_loss_weight\": 5,\n",
        "        \"input_shape\": (1, 16, 128, 128),\n",
        "        \"latent_shape\": (5, 2, 16, 16),\n",
        "        \"checkpoint_dir\": \"./checkpoints/magvit_v2/\"\n",
        "    },\n",
        "    \"transformer\": {\n",
        "        \"emb_dim\": 512,\n",
        "        \"nheads\": 8,\n",
        "        \"ctx_size\": 2048,\n",
        "        \"window_size\": 1024,\n",
        "        \"fan_out\": 4,\n",
        "        \"nlayers\": 6,\n",
        "        \"dropout\": 0.0,\n",
        "        \"use_flash_attn\": True,\n",
        "        \"checkpoint_dir\": \"./checkpoints/transformer/\"\n",
        "    },\n",
        "    \"super_resolution\": {\n",
        "        \"checkpoint_dir\": \"./checkpoints/super_resolution/\"\n",
        "    },\n",
        "    \"training\": {\n",
        "        \"magvit2\": {\n",
        "            \"batch_size\": 16,\n",
        "            \"num_workers\": 4,\n",
        "            \"epochs\": 1024,\n",
        "            \"lr\": 1e-4,\n",
        "            \"save_top_k\": 2,\n",
        "            \"check\": \"\"\n",
        "\n",
        "        },\n",
        "        \"transformer\": {},\n",
        "        \"super_resolution\": {}\n",
        "    },\n",
        "    \"logging\": {}\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Config:\n",
        "    def __init__(self, config_dict):\n",
        "        for key, value in config_dict.items():\n",
        "            if isinstance(value, dict):\n",
        "                value = Config(value)\n",
        "            self.__dict__[key] = value\n",
        "\n",
        "    def __getattr__(self, name):\n",
        "        return self.__dict__.get(name, None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "magvit2_config = Config(config['magvit2'])\n",
        "transformer_config = Config(config['transformer'])\n",
        "super_resolution_config = Config(config['super_resolution'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uF_3GgiCM2_o"
      },
      "source": [
        "# Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fxbeDgZAN1fJ"
      },
      "source": [
        "## Clips Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cVyXbEcTM5lG"
      },
      "outputs": [],
      "source": [
        "class RandomHorizontalFlipVideo(torch.nn.Module):\n",
        "    def __init__(self, p=0.5):\n",
        "        super().__init__()\n",
        "        self.p = p\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape is expected to be (C, T, H, W)\n",
        "        if torch.rand(1) < self.p:\n",
        "            # flip all frames in the clip\n",
        "            return x.flip(-1)\n",
        "        return x\n",
        "\n",
        "\n",
        "class SteamboatWillieDataset(Dataset):\n",
        "    def __init__(self, config, mode='train', train_split=0.8, shuffle=True, augment=True):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.train_split = train_split\n",
        "        self.mode = mode\n",
        "\n",
        "        self.preprocess_transforms = Compose([\n",
        "                Lambda(lambda x: x.permute(0, 3, 1, 2)),   # (T, H, W, C) to (T, C, H, W) for Greyscale\n",
        "                Grayscale(num_output_channels=1),          # Convert to grayscale\n",
        "                Lambda(lambda x: x.permute(1, 0, 2, 3)),   # (T, C, H, W) to (C, T, H, W) for Conv3d\n",
        "                CenterCrop((480, 575)),                    # Center crop to remove virtical bars\n",
        "                Resize((config.img_size, config.img_size))\n",
        "        ])\n",
        "\n",
        "        self.postprocess_transforms = Compose([\n",
        "            Lambda(lambda x: x / 255.),\n",
        "            Lambda(lambda x: x.view(self.config.in_channels, self.config.clip_length, self.config.img_size, self.config.img_size))\n",
        "        ])\n",
        "\n",
        "        if self.mode == 'train' and augment:\n",
        "            self.postprocess_transforms.transforms.append(RandomHorizontalFlipVideo(p=0.5))\n",
        "\n",
        "        if os.path.exists(config.clip_dest_dir):\n",
        "            clip_paths = self.build_existing_clip_paths(config.clip_dest_dir)\n",
        "            self.clips = self.build_clip_refs(clip_paths)\n",
        "        else:\n",
        "            video_clips = VideoClips(\n",
        "                config.paths,\n",
        "                clip_length_in_frames=config.clip_length,\n",
        "                frames_between_clips=config.clip_length\n",
        "            )\n",
        "\n",
        "            self.clips = self.build_clip_refs(self.build_clip_paths(video_clips, self.preprocess_transforms, config.clip_dest_dir))\n",
        "\n",
        "        if mode in ['train', 'val']:\n",
        "            total_clips = len(self.clips)\n",
        "            if shuffle:\n",
        "                indices = torch.randperm(total_clips).tolist()\n",
        "            else:\n",
        "                indices = list(range(total_clips))\n",
        "\n",
        "            train_size = int(total_clips * train_split)\n",
        "\n",
        "            if mode == 'train':\n",
        "                self.clip_indices = indices[:train_size]\n",
        "            else:\n",
        "                self.clip_indices = indices[train_size:]\n",
        "        elif mode == 'full':\n",
        "            self.clip_indices = list(range(len(self.clips)))\n",
        "\n",
        "    def build_clip_paths(self, video_clips, transforms, clip_dest_dir):\n",
        "        \"\"\"\n",
        "        Build set of binary files to store processed video clips\n",
        "        returns dict of clip_idx -> mmapped file path\n",
        "        \"\"\"\n",
        "        clip_paths = {}\n",
        "\n",
        "        if not os.path.exists(clip_dest_dir):\n",
        "            os.makedirs(clip_dest_dir)\n",
        "\n",
        "        for idx in tqdm(range(video_clips.num_clips()), desc='Creating clip .bin files'):\n",
        "            # transform clips and write to mmap file\n",
        "            clip, _, _, _ = video_clips.get_clip(idx)\n",
        "            clip = self.preprocess_transforms(clip)\n",
        "            clip_np = clip.numpy().astype(np.uint8)\n",
        "\n",
        "            mmapped_file_path = os.path.join(clip_dest_dir, f'clip_{idx}.bin')\n",
        "            fp = np.memmap(mmapped_file_path, dtype='uint8', mode='w+', shape=clip_np.shape)\n",
        "            fp[:] = clip_np[:]\n",
        "            fp.flush()\n",
        "            del fp\n",
        "            clip_paths[idx] = mmapped_file_path\n",
        "\n",
        "        return clip_paths\n",
        "\n",
        "    def build_existing_clip_paths(self, clip_dest_dir):\n",
        "        \"\"\"\"\n",
        "        returns dict of clip_idx -> mmapped file path\n",
        "        from existing .bin files\n",
        "        \"\"\"\n",
        "        clips_paths = {}\n",
        "        for filename in os.listdir(clip_dest_dir):\n",
        "            if filename.startswith('clip_') and filename.endswith('.bin'):\n",
        "                idx = int(filename.split('_')[1].split('.')[0])\n",
        "                file_path = os.path.join(clip_dest_dir, filename)\n",
        "                clips_paths[idx] = file_path\n",
        "\n",
        "        return clips_paths\n",
        "\n",
        "    def build_clip_refs(self, clip_paths):\n",
        "        \"\"\"\n",
        "        Build mmap reference to bin files\n",
        "        returns dict of clip_idx -> np.array mmapped to respective bin file\n",
        "        \"\"\"\n",
        "        clips = {}\n",
        "        for idx, path in tqdm(clip_paths.items(), desc='Building clip refs'):\n",
        "            clips[idx] = np.memmap(path, dtype='uint8', mode='r')\n",
        "\n",
        "        return clips\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.clip_indices)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        clip = self.clips[self.clip_indices[idx]]\n",
        "        return self.postprocess_transforms(torch.tensor(clip, dtype=torch.float32))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Lightning Datamodule"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SteamboatWillieDataModule(pl.LightningDataModule):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.batch_size = config.batch_size\n",
        "        self.config = config\n",
        "\n",
        "    def setup(self, stage=None):\n",
        "        if stage == 'fit' or stage is None:\n",
        "            self.train_dataset = SteamboatWillieDataset(self.config, mode='train')\n",
        "            self.val_dataset = SteamboatWillieDataset(self.config, mode='val')\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return torch.utils.data.DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True, num_workers=self.config.num_workers)\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return torch.utils.data.DataLoader(self.val_dataset, batch_size=self.batch_size, shuffle=False, num_workers=self.config.num_workers)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MLC7Er-SMcWj"
      },
      "source": [
        "# MAGVIT-V2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PPSOB-ZmMvm7"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Components"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YJEpGq2DMfY8"
      },
      "outputs": [],
      "source": [
        "class CausalConv3d(nn.Module):\n",
        "    '''\n",
        "    enforces causality in the time dimension (https://paperswithcode.com/method/causal-convolution)\n",
        "    inspired by:\n",
        "    https://github.com/lucidrains/magvit2-pytorch/blob/9f49074179c912736e617d61b32be367eb5f993a/magvit2_pytorch/magvit2_pytorch.py#L889\n",
        "    '''\n",
        "    def __init__(\n",
        "            self,\n",
        "            in_channels,\n",
        "            out_channels,\n",
        "            kernel_size=(3,3,3),\n",
        "            stride=(1, 1, 1),\n",
        "            dilation=(1, 1, 1),\n",
        "            pad_mode='constant'\n",
        "        ):\n",
        "        super().__init__()\n",
        "        k_t, k_h, k_w = kernel_size\n",
        "        pad_w, pad_h, pad_t = k_w//2, k_h//2, dilation[0] * (k_t - 1) + (1 - stride[0])\n",
        "\n",
        "        # pad: (left, right, top, bottom, front, back)\n",
        "        self.t_causal_pad = (pad_w, pad_w, pad_h, pad_h, pad_t, 0)\n",
        "        self.pad_mode = pad_mode\n",
        "\n",
        "        self.conv = nn.Conv3d(\n",
        "            in_channels  = in_channels,\n",
        "            out_channels = out_channels,\n",
        "            kernel_size  = kernel_size,\n",
        "            stride       = stride,\n",
        "            dilation     = dilation\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.pad(x, self.t_causal_pad, mode=self.pad_mode)\n",
        "        x = self.conv(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class BlurPool3d(nn.Module):\n",
        "    '''\n",
        "    https://arxiv.org/abs/1904.11486\n",
        "    inspired by:\n",
        "    https://github.com/adobe/antialiased-cnns/blob/master/antialiased_cnns/blurpool.py\n",
        "    https://github.com/lucidrains/magvit2-pytorch/blob/9f49074179c912736e617d61b32be367eb5f993a/magvit2_pytorch/magvit2_pytorch.py#L509\n",
        "    '''\n",
        "    def __init__(self, in_channels, kernel_size=3, stride=2):\n",
        "        super().__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.stride = stride\n",
        "        self.padding = (kernel_size - 1) // 2\n",
        "\n",
        "        self.register_buffer('blur_filter', self.get_blur_filter3d(kernel_size))\n",
        "\n",
        "    def forward(self, x):\n",
        "        assert len(x.shape) == 5, 'BlurPool3d only supports rank 5 tensors'\n",
        "        return F.conv3d(x, self.blur_filter, stride=(self.stride, self.stride, self.stride), padding=self.padding)\n",
        "\n",
        "    def get_blur_filter3d(self, kernel_size):\n",
        "        if kernel_size == 1:\n",
        "            filter = torch.tensor([1.,])\n",
        "        elif kernel_size == 2:\n",
        "            filter = torch.tensor([1., 1.])\n",
        "        elif kernel_size == 3:\n",
        "            filter = torch.tensor([1., 2., 1.])\n",
        "        elif kernel_size == 4:\n",
        "            filter = torch.tensor([1., 3., 3., 1.])\n",
        "        elif kernel_size == 5:\n",
        "            filter = torch.tensor([1., 4., 6., 4., 1.])\n",
        "        elif kernel_size == 6:\n",
        "            filter = torch.tensor([1., 5., 10., 10., 5., 1.])\n",
        "        elif kernel_size == 7:\n",
        "            filter = torch.tensor([1., 6., 15., 20., 15., 6., 1.])\n",
        "\n",
        "        filter = einsum('i, j, k -> i j k', filter, filter, filter)\n",
        "        filter = repeat(filter, 'd h w -> oc ic d h w', oc=self.in_channels, ic=self.in_channels)\n",
        "\n",
        "        return filter / torch.sum(filter)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ResBlock3d(nn.Module):\n",
        "    ''' handles both same and different in/out channels '''\n",
        "    def __init__(\n",
        "            self,\n",
        "            in_channels,\n",
        "            out_channels,\n",
        "            kernel_size=(3,3,3)\n",
        "        ):\n",
        "        super().__init__()\n",
        "        if in_channels != out_channels:\n",
        "            self.identity = CausalConv3d(\n",
        "                in_channels  = in_channels,\n",
        "                out_channels = out_channels,\n",
        "                kernel_size  = (1, 1, 1)\n",
        "            )\n",
        "        else:\n",
        "            self.identity = nn.Identity()\n",
        "\n",
        "        self.block = nn.Sequential(\n",
        "            nn.GroupNorm(\n",
        "                num_groups   = in_channels // 2 if in_channels >= 2 else 1,\n",
        "                num_channels = in_channels\n",
        "            ),\n",
        "            nn.SiLU(),\n",
        "            CausalConv3d(\n",
        "                in_channels  = in_channels,\n",
        "                out_channels = out_channels,\n",
        "                kernel_size  = kernel_size,\n",
        "                stride       = (1,1,1),\n",
        "                dilation     = (1,1,1)\n",
        "            ),\n",
        "            nn.GroupNorm(\n",
        "                num_groups   = out_channels // 2 if out_channels >= 2 else 1,\n",
        "                num_channels = out_channels\n",
        "            ),\n",
        "            nn.SiLU(),\n",
        "            CausalConv3d(\n",
        "                in_channels  = out_channels,\n",
        "                out_channels = out_channels,\n",
        "                kernel_size  = kernel_size,\n",
        "                stride       = (1,1,1),\n",
        "                dilation     = (1,1,1)\n",
        "            ),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.block(x) + self.identity(x)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ResBlockDown3d(nn.Module):\n",
        "    ''' strided conv for down-sampling + blur pooling (https://arxiv.org/abs/1904.11486) '''\n",
        "    def __init__(\n",
        "            self,\n",
        "            in_channels,\n",
        "            out_channels,\n",
        "            kernel_size=(3,3,3)\n",
        "        ):\n",
        "        super().__init__()\n",
        "        self.identity = nn.Sequential(\n",
        "            BlurPool3d(in_channels),\n",
        "            nn.Conv3d(\n",
        "                in_channels  = in_channels,\n",
        "                out_channels = out_channels,\n",
        "                kernel_size  = (1, 1, 1),\n",
        "                padding      = 'same'\n",
        "            )\n",
        "        )\n",
        "\n",
        "        self.block = nn.Sequential(\n",
        "            nn.Conv3d(\n",
        "                in_channels  = in_channels,\n",
        "                out_channels = out_channels,\n",
        "                kernel_size  = kernel_size,\n",
        "                padding      = 'same'\n",
        "            ),\n",
        "            nn.LeakyReLU(),\n",
        "            BlurPool3d(out_channels),\n",
        "            nn.Conv3d(\n",
        "                in_channels  = out_channels,\n",
        "                out_channels = out_channels,\n",
        "                kernel_size  = kernel_size,\n",
        "                padding      = 'same'\n",
        "            ),\n",
        "            nn.LeakyReLU(),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.block(x) + self.identity(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class PixelShuffle3d(nn.Module):\n",
        "    ''' https://arxiv.org/abs/1609.05158 '''\n",
        "    def __init__(self, r=(2, 2, 2)):\n",
        "        super().__init__()\n",
        "        self.r = r\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return rearrange(\n",
        "            x,\n",
        "            'b (c1 r1 r2 r3) d h w -> b c1 (d r1) (h r2) (w r3)',\n",
        "            r1=self.r[0], r2=self.r[1], r3=self.r[2]\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Upsample3d(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, upsample_time=True):\n",
        "        super().__init__()\n",
        "        self.conv = CausalConv3d(\n",
        "            in_channels  = in_channels,\n",
        "            out_channels = out_channels * (8 if upsample_time else 4),\n",
        "            kernel_size  = (3,3,3)\n",
        "        )\n",
        "        # TODO: should we add an activation between?\n",
        "        self.pixel_shuffle = PixelShuffle3d(\n",
        "            r=(2,2,2) if upsample_time else (1,2,2)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        x = self.pixel_shuffle(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class EncoderBlock(nn.Module):\n",
        "    def __init__(\n",
        "            self,\n",
        "            in_channels,\n",
        "            out_channels,\n",
        "            nblocks,\n",
        "            kernel_size=(3,3,3),\n",
        "            causal_stride=(2,2,2)\n",
        "        ):\n",
        "        super().__init__()\n",
        "        self.block = nn.Sequential(\n",
        "            CausalConv3d(\n",
        "                in_channels  = in_channels,\n",
        "                out_channels = out_channels,\n",
        "                kernel_size  = kernel_size,\n",
        "                stride       = causal_stride\n",
        "            ),\n",
        "            nn.Sequential(*[\n",
        "                ResBlock3d(\n",
        "                    in_channels  = out_channels,\n",
        "                    out_channels = out_channels\n",
        "                )\n",
        "                for _ in range(nblocks)\n",
        "            ])\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.block(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DecoderBlock(nn.Module):\n",
        "    def __init__(\n",
        "            self,\n",
        "            in_channels,\n",
        "            out_channels,\n",
        "            nblocks=2,\n",
        "            upsample_time=True,\n",
        "            silu=True\n",
        "        ):\n",
        "        super().__init__()\n",
        "        self.group_norm = nn.GroupNorm(\n",
        "            num_groups   = in_channels // 2 if in_channels >= 2 else 1,\n",
        "            num_channels = in_channels\n",
        "        )\n",
        "\n",
        "        self.res_blocks = nn.Sequential(*[\n",
        "            ResBlock3d(\n",
        "                in_channels  = in_channels if i==0 else out_channels,\n",
        "                out_channels = out_channels\n",
        "            )\n",
        "            for i in range(nblocks)\n",
        "        ])\n",
        "\n",
        "        self.upsample = Upsample3d(\n",
        "            in_channels   = out_channels,\n",
        "            out_channels  = out_channels,\n",
        "            upsample_time = upsample_time\n",
        "        )\n",
        "        if silu:\n",
        "            self.silu = nn.SiLU()\n",
        "        else:\n",
        "            self.silu = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.group_norm(x)\n",
        "        x = self.res_blocks(x)\n",
        "        x = self.upsample(x)\n",
        "\n",
        "        if self.silu is not None:\n",
        "            x = self.silu(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class FSQ(nn.Module):\n",
        "    def __init__(self, levels, eps=1e-3):\n",
        "        super().__init__()\n",
        "        self.register_buffer('levels', torch.tensor(levels))\n",
        "\n",
        "        self.register_buffer('basis', \n",
        "            torch.cat([\n",
        "                torch.tensor([1]),\n",
        "                torch.cumprod(torch.tensor(levels[:-1]), dim=0)\n",
        "            ], dim=0)\n",
        "        )\n",
        "\n",
        "        self.eps = eps\n",
        "        self.codebook_size = torch.prod(self.levels)\n",
        "\n",
        "    def round_ste(self, z):\n",
        "        z_q = torch.round(z)\n",
        "        return z + (z_q - z).detach()\n",
        "\n",
        "    def quantize(self, z):\n",
        "        # half_l is used to determine how to scale tanh; we\n",
        "        # subtract 1 from the number of levels to account for 0\n",
        "        # being a quantization bin and tanh being symmetric around 0\n",
        "        half_l = (self.levels - 1) * (1 - self.eps) / 2\n",
        "\n",
        "        # if a given level is even, it will result in a scale for tanh\n",
        "        # which is halfway between integer values, so we offset\n",
        "        # the tanh output down by 0.5 to line it with whole integers\n",
        "        offset = torch.where(self.levels % 2 == 0, 0.5, 0.0)\n",
        "\n",
        "        # if our level is even, we want to shift the tanh input to\n",
        "        # ensure the 0 quantization bin is centered\n",
        "        shift = torch.tan(offset / half_l)\n",
        "\n",
        "        # once we have our shift and offset (in the case of an even level)\n",
        "        # we can round to the nearest integer bin and allow for STE\n",
        "        z_q = self.round_ste(torch.tanh(z + shift) * half_l - offset)\n",
        "\n",
        "        # after quantization, we want to renormalize the quantized\n",
        "        # values to be within the range expected by the model (ie. [-1, 1])\n",
        "        half_width = self.levels // 2\n",
        "        return z_q / half_width\n",
        "\n",
        "    def scale_and_shift(self, z_q_normalized):\n",
        "        half_width = self.levels // 2\n",
        "        return (z_q_normalized * half_width) + half_width\n",
        "\n",
        "    def scale_and_shift_inverse(self, z_q):\n",
        "        half_width = self.levels // 2\n",
        "        return (z_q - half_width) / half_width\n",
        "\n",
        "    def codes_to_idxs(self, z_q):\n",
        "        assert z_q.shape[-1] == len(self.levels)\n",
        "        z_q = self.scale_and_shift(z_q)\n",
        "        return (z_q * self.basis).sum(dim=-1).to(torch.int32)\n",
        "\n",
        "    def idxs_to_codes(self, idxs):\n",
        "        idxs = idxs.unsqueeze(-1)\n",
        "        codes_not_centered = (idxs // self.basis) % self.levels\n",
        "        return self.scale_and_shift_inverse(codes_not_centered)\n",
        "\n",
        "    def forward(self, z):\n",
        "        # TODO: make this work for generic tensor sizes\n",
        "        # TODO: use einops to clean up\n",
        "\n",
        "        if len(z.shape) == 5: # video\n",
        "            B, C, T, H, W = z.shape\n",
        "            # (B, C, T, H, W) -> (B, T, H, W, C)\n",
        "            z_c_last = z.permute(0, 2, 3, 4, 1).contiguous()\n",
        "            # (B, T, H, W, C) -> (BTHW, C)\n",
        "            z_flatten = z_c_last.reshape(-1, C)\n",
        "            z_flatten_q = self.quantize(z_flatten)\n",
        "            # (BTHW, C) -> (B, T, H, W, C) -> (B, C, T, H, W)\n",
        "            z_q = z_flatten_q.reshape(B, T, H, W, C).permute(0, 4, 1, 2, 3).contiguous()\n",
        "\n",
        "        elif len(z.shape) == 4: # image\n",
        "            B, C, H, W = z.shape\n",
        "            # (B, C, H, W) -> (B, H, W, C)\n",
        "            z_c_last = z.permute(0, 2, 3, 1).contiguous()\n",
        "            # (B, H, W, C) -> (BHW, C)\n",
        "            z_flatten = z_c_last.reshape(-1, C)\n",
        "            z_flatten_q = self.quantize(z_flatten)\n",
        "            # (BHW, C) -> (B, H, W, C) -> (B, C, T, H, W)\n",
        "            z_q = z_flatten_q.reshape(B, H, W, C).permute(0, 3, 1, 2).contiguous()\n",
        "\n",
        "        elif len(z.shape) == 2: # vector sequence\n",
        "            # (B, C)\n",
        "            z_q = self.quantize(z)\n",
        "\n",
        "        return {'z_q': z_q}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(\n",
        "            self,\n",
        "            in_channels,\n",
        "            out_channels,\n",
        "            init_channels,\n",
        "            num_downsamples,\n",
        "            nblocks\n",
        "        ):\n",
        "        super().__init__()\n",
        "        channels = [init_channels * (2 ** i) for i in range(num_downsamples+1)]\n",
        "        self.in_conv = CausalConv3d(\n",
        "            in_channels=in_channels,\n",
        "            out_channels=channels[0],\n",
        "            kernel_size=(3,3,3)\n",
        "        )\n",
        "\n",
        "        self.enc_blocks = nn.Sequential(*[\n",
        "            EncoderBlock(\n",
        "                in_channels=channels[i],\n",
        "                out_channels=channels[i+1],\n",
        "                nblocks=nblocks,\n",
        "                kernel_size=(3,3,3),\n",
        "                causal_stride=(2,2,2)\n",
        "            )\n",
        "            for i in range(num_downsamples)\n",
        "        ])\n",
        "\n",
        "        self.out_block = nn.Sequential(\n",
        "            nn.GroupNorm(\n",
        "                num_groups=channels[-1] // 2,\n",
        "                num_channels=channels[-1]\n",
        "            ),\n",
        "            nn.SiLU(),\n",
        "            CausalConv3d(\n",
        "                in_channels=channels[-1],\n",
        "                out_channels=out_channels,\n",
        "                kernel_size=(1,1,1)\n",
        "            ),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.in_conv(x)\n",
        "        x = self.enc_blocks(x)\n",
        "        x = self.out_block(x)\n",
        "        \n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(\n",
        "            self,\n",
        "            in_channels,\n",
        "            out_channels,\n",
        "            init_channels,\n",
        "            num_upsamples,\n",
        "            nblocks,\n",
        "        ):\n",
        "        super().__init__()\n",
        "        channels = [init_channels // (2 ** i) for i in range(num_upsamples)]\n",
        "        self.in_block = nn.Sequential(\n",
        "            CausalConv3d(\n",
        "                in_channels=in_channels,\n",
        "                out_channels=channels[0],\n",
        "                kernel_size=(3,3,3)\n",
        "            ),\n",
        "            nn.Sequential(*[\n",
        "                ResBlock3d(\n",
        "                    in_channels=channels[0],\n",
        "                    out_channels=channels[0]\n",
        "                )\n",
        "                for i in range(nblocks)\n",
        "            ])\n",
        "        )\n",
        "\n",
        "        self.dec_blocks = nn.Sequential(*[\n",
        "            DecoderBlock(\n",
        "                in_channels=channels[i],\n",
        "                out_channels=channels[i+1] if i<num_upsamples-1 else out_channels,\n",
        "                nblocks=nblocks,\n",
        "                upsample_time=True,\n",
        "                silu=(i == num_upsamples-1)\n",
        "            )\n",
        "            for i in range(num_upsamples)\n",
        "        ])\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.in_block(x)\n",
        "        x = self.dec_blocks(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class FSQVAE(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.encoder = Encoder(\n",
        "            in_channels     = config.encoder.in_channels,\n",
        "            out_channels    = config.encoder.out_channels,\n",
        "            init_channels   = config.encoder.init_channels,\n",
        "            num_downsamples = config.encoder.num_downsamples,\n",
        "            nblocks         = config.encoder.nblocks\n",
        "        )\n",
        "\n",
        "        self.fsq = FSQ(levels = config.fsq.levels)\n",
        "\n",
        "        self.decoder = Decoder(\n",
        "            in_channels   = config.decoder.in_channels,\n",
        "            out_channels  = config.decoder.out_channels,\n",
        "            init_channels = config.decoder.init_channels,\n",
        "            num_upsamples = config.decoder.num_upsamples,\n",
        "            nblocks       = config.decoder.nblocks\n",
        "        )\n",
        "\n",
        "    def encode(self, x):\n",
        "        return self.encoder(x)\n",
        "\n",
        "    def quantize(self, z):\n",
        "        return self.fsq(z)\n",
        "\n",
        "    def decode(self, z_q):\n",
        "        return self.decoder(z_q)\n",
        "\n",
        "    def forward(self, x):\n",
        "        z = self.encode(x)\n",
        "        z_q = self.quantize(z)\n",
        "        x_hat = self.decode(z_q)\n",
        "        return { 'z': z, 'z_q': z_q, 'x_hat': x_hat }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Discriminator(nn.Module):\n",
        "    ''' bunch of downsampling resblocks + leaky relu '''\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv3d(\n",
        "                in_channels  = config.in_channels,\n",
        "                out_channels = config.init_channels,\n",
        "                kernel_size  = (3, 3, 3),\n",
        "                padding      = 'same'\n",
        "            ),\n",
        "            nn.LeakyReLU()\n",
        "        )\n",
        "\n",
        "        channels = [config.init_channels * (2 ** i) for i in range(config.num_downsamples+1)]\n",
        "        self.downsample = nn.Sequential(*[\n",
        "            ResBlockDown3d(\n",
        "                in_channels  = channels[i],\n",
        "                out_channels = channels[i+1]\n",
        "            )\n",
        "            for i in range(config.num_downsamples)\n",
        "        ])\n",
        "\n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.Conv3d(\n",
        "                in_channels  = channels[-1],\n",
        "                out_channels = channels[-1],\n",
        "                kernel_size  = (3, 3, 3),\n",
        "                padding      = 'same'\n",
        "            ),\n",
        "            nn.LeakyReLU()\n",
        "        )\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(channels[-1]*(4*4), channels[-1]), # 4x4 after downsampling\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Linear(channels[-1], 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.downsample(x)\n",
        "        x = self.conv2(x)\n",
        "        x = rearrange(x, 'b c d h w -> b (c d h w)')\n",
        "        x = self.mlp(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MAGVIT2(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.fsqvae = FSQVAE(config.fsqvae)\n",
        "        self.discriminator = Discriminator(config.discriminator)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fsqvae(x)\n",
        "\n",
        "    def tokenize(self, x):\n",
        "        z_q = self.fsqvae.quantize(self.fsqvae.encode(x))\n",
        "        z_q = rearrange(z_q, 'b c d h w -> b (d h w) c')\n",
        "        idxs = self.fsqvae.fsq.codes_to_idxs(z_q)\n",
        "        return idxs\n",
        "\n",
        "    def decode(self, idxs):\n",
        "        c, d, h, w = self.config.latent_shape\n",
        "        codes = self.fsqvae.fsq.idxs_to_codes(idxs)\n",
        "        codes = rearrange(codes, 'b (d h w) c -> b c d h w', d=d, h=h, w=w)\n",
        "        return self.fsqvae.decode(codes)\n",
        "\n",
        "    def discriminate(self, x):\n",
        "        return self.discriminator(x)\n",
        "\n",
        "    def reconstruction_loss(self, x_hat, x):\n",
        "        return F.mse_loss(x_hat, x)\n",
        "\n",
        "    def adversarial_loss(self, y_hat, y):\n",
        "        return F.binary_cross_entropy(y_hat, y)\n",
        "\n",
        "    def gradient_penalty(self, x, y_hat_real):\n",
        "        '''\n",
        "        inspired by:\n",
        "        https://github.com/lucidrains/magvit2-pytorch/blob/9f49074179c912736e617d61b32be367eb5f993a/magvit2_pytorch/magvit2_pytorch.py#L99\n",
        "        '''\n",
        "        gradients = torch_grad(\n",
        "            outputs = y_hat_real,\n",
        "            inputs = x,\n",
        "            grad_outputs = torch.ones(y_hat_real.size(), device = x.device),\n",
        "            create_graph = True,\n",
        "            retain_graph = True,\n",
        "            only_inputs = True\n",
        "        )[0]\n",
        "\n",
        "        gradients = rearrange(gradients, 'b ... -> b (...)')\n",
        "        return ((gradients.norm(2, dim = 1) - 1) ** 2).mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Lightning Module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class LitMAGVIT2(pl.LightningModule):\n",
        "    def __init__(self, magvit2, config):\n",
        "        super().__init__()\n",
        "\n",
        "        self.magvit2 = magvit2\n",
        "        self.config = config\n",
        "        self.lr = config.training.lr\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.magvit2(x)\n",
        "\n",
        "    def training_step(self, batch, batch_idx, optimizer_idx):\n",
        "        x = batch\n",
        "        out = self(x)\n",
        "\n",
        "        rec_loss = self.magvit2.reconstruction_loss(out['x_hat'], x)\n",
        "\n",
        "        # train generator\n",
        "        if optimizer_idx == 0:\n",
        "            y_hat_fake = self.magvit2.discriminate(out['x_hat'])\n",
        "            generator_loss = self.magvit2.adversarial_loss(y_hat_fake, torch.ones_like(y_hat_fake))\n",
        "            total_loss = rec_loss + self.magvit2.discriminator_weight * generator_loss\n",
        "\n",
        "            self.log('train/rec_loss',       rec_loss)\n",
        "            self.log('train/generator_loss', generator_loss)\n",
        "            self.log('train/total_loss',     total_loss)\n",
        "\n",
        "            return total_loss\n",
        "\n",
        "        # train discriminator\n",
        "        if optimizer_idx == 1:\n",
        "            y_hat_real = self.magvit2.discriminate(x)\n",
        "            y_hat_fake = self.magvit2.discriminate(out['x_hat'].detach())\n",
        "\n",
        "            real_labels = torch.ones_like(y_hat_real)\n",
        "            fake_labels = torch.zeros_like(y_hat_fake)\n",
        "\n",
        "            real_loss = self.magvit2.adversarial_loss(y_hat_real, real_labels)\n",
        "            fake_loss = self.magvit2.adversarial_loss(y_hat_fake, fake_labels)\n",
        "            disc_loss = (real_loss + fake_loss) / 2\n",
        "\n",
        "            if self.config.discriminator.use_grad_penalty:\n",
        "                grad_penalty = self.magvit2.gradient_penalty(x, out['x_hat'].detach())\n",
        "                disc_loss += self.config.discriminator.grad_penalty_weight * grad_penalty\n",
        "                self.log('train/grad_penalty', grad_penalty)\n",
        "\n",
        "            self.log('train/discriminator_loss', disc_loss)\n",
        "\n",
        "            return disc_loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        x = batch\n",
        "        out = self(x)\n",
        "\n",
        "        rec_loss = self.magvit2.reconstruction_loss(out['x_hat'], x)\n",
        "        self.log('val/rec_loss', rec_loss, prog_bar=True)\n",
        "\n",
        "        self.log_val_clips(x, out)\n",
        "\n",
        "        return rec_loss\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        fsqvae_optimizer = torch.optim.AdamW(\n",
        "            self.magvit2.fsqvae.parameters(),\n",
        "            lr=self.lr,\n",
        "            betas=(self.config.training.beta1, self.config.training.beta2),\n",
        "            weight_decay=self.config.weight_decay\n",
        "        )\n",
        "\n",
        "        disc_optimizer = torch.optim.AdamW(\n",
        "            self.self.magvit2.discriminator.parameters(),\n",
        "            lr=self.lr,\n",
        "            betas=(self.config.training.beta1, self.config.training.beta2),\n",
        "            weight_decay=self.config.weight_decay\n",
        "        )\n",
        "\n",
        "        if self.config.use_lr_schedule:\n",
        "            fsqvae_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
        "                fsqvae_optimizer, T_max=self.config.training_steps)\n",
        "\n",
        "            disc_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
        "                disc_optimizer, T_max=self.config.training_steps)\n",
        "\n",
        "            return [fsqvae_optimizer, disc_optimizer], [fsqvae_scheduler, disc_scheduler]\n",
        "\n",
        "        return [fsqvae_optimizer, disc_optimizer]\n",
        "\n",
        "    def log_val_clips(self, x, out, num_clips=2):\n",
        "        n_clips = min(x.size(0), num_clips)\n",
        "\n",
        "        for i in range(n_clips):\n",
        "            # extract the ith original and reconstructed clip\n",
        "            original_clip = x[i]  # (C, T, H, W)\n",
        "            reconstructed_clip = out['x_hat'][i]  # (C, T, H, W)\n",
        "\n",
        "            # convert tensors to numpy arrays and transpose to (T, H, W, C) for GIF creation\n",
        "            original_clip_np = original_clip.permute(1, 2, 3, 0).cpu().numpy()\n",
        "            reconstructed_clip_np = reconstructed_clip.permute(1, 2, 3, 0).cpu().numpy()\n",
        "\n",
        "            original_clip_np = (original_clip_np - original_clip_np.min()) / (original_clip_np.max() - original_clip_np.min())\n",
        "            reconstructed_clip_np = (reconstructed_clip_np - reconstructed_clip_np.min()) / (reconstructed_clip_np.max() - reconstructed_clip_np.min())\n",
        "\n",
        "            original_clip_np = (original_clip_np * 255).astype(np.uint8)\n",
        "            reconstructed_clip_np = (reconstructed_clip_np * 255).astype(np.uint8)\n",
        "\n",
        "            # grayscale videos need to be of shape (T, H, W)\n",
        "            if original_clip_np.shape[-1] == 1:\n",
        "                original_clip_np = original_clip_np.squeeze(-1)\n",
        "\n",
        "            if reconstructed_clip_np.shape[-1] == 1:\n",
        "                reconstructed_clip_np = reconstructed_clip_np.squeeze(-1)\n",
        "\n",
        "            # create GIFs for the original and reconstructed clips\n",
        "            original_gif_path = f'/tmp/original_clip_{i}.gif'\n",
        "            reconstructed_gif_path = f'/tmp/reconstructed_clip_{i}.gif'\n",
        "            imageio.mimsave(original_gif_path, original_clip_np, fps=10)\n",
        "            imageio.mimsave(reconstructed_gif_path, reconstructed_clip_np, fps=10)\n",
        "\n",
        "            # log the GIFs to wandb\n",
        "            self.logger.experiment.log({\n",
        "                f\"val/original_clip_{i}\": wandb.Video(original_gif_path, fps=10, format=\"gif\", caption=\"Original\"),\n",
        "                f\"val/reconstructed_clip_{i}\": wandb.Video(reconstructed_gif_path, fps=10, format=\"gif\", caption=\"Reconstructed\")\n",
        "            })"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "x = torch.randn(5, 3, 16, 128, 128).cuda()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "conv = CausalConv3d(in_channels=1, out_channels=16).cuda()\n",
        "out = conv(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "out.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "res_block = ResBlock3d(in_channels=16, out_channels=16).cuda()\n",
        "out = res_block(out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "out.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "res_block2 = ResBlock3d(in_channels=16, out_channels=32).cuda()\n",
        "out = res_block2(out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "out.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "blur_pool = BlurPool3d(1).cuda()\n",
        "out = blur_pool(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "out.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "res_block_down3d = ResBlockDown3d(in_channels=3, out_channels=32).cuda()\n",
        "out = res_block_down3d(out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "out.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "disc = Discriminator(in_channels=3).cuda()\n",
        "out = disc(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "out.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "enc = Encoder(in_channels=3, out_channels=5, init_channels=32, num_downsamples=3, nblocks=2).cuda()\n",
        "out = enc(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "out.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "x = torch.randn(5, 32, 16, 128, 128).cuda()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "upsample = Upsample3d(in_channels=32, out_channels=64).cuda()\n",
        "out = upsample(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "out.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "upsample = Upsample3d(in_channels=32, out_channels=64, upsample_time=False).cuda()\n",
        "out = upsample(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "out.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "x = torch.randn(5, 5, 2, 16, 16).cuda()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dec = Decoder(in_channels=5, out_channels=3, init_channels=512, num_upsamples=3, nblocks=2).cuda()\n",
        "out = dec(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "out.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0QVpgOrNP6Q"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IlYmZCUXNTOR"
      },
      "outputs": [],
      "source": [
        "resume_training = False\n",
        "\n",
        "project_config  = Config(config['project'])\n",
        "magvit2_config  = Config(config['magvit2'])\n",
        "training_config = Config(config['training'])\n",
        "data_config     = Config(config['data'])\n",
        "\n",
        "model = MAGVIT2(magvit2_config)\n",
        "lit_model = LitMAGVIT2(model, magvit2_config)\n",
        "data = SteamboatWillieDataModule(data_config)\n",
        "\n",
        "if resume_training:\n",
        "    run = wandb.init(\n",
        "        project=project_config.magvit2.project_name,\n",
        "        run=\"[RUN NAME]\",\n",
        "        config=config['magvit2'], resume=True\n",
        "    )\n",
        "    artifact = run.use_artifact('[ARTIFACT NAME]', type='model')\n",
        "    artifact_dir = artifact.download()\n",
        "else:\n",
        "    wandb.init(\n",
        "        project=project_config.magvit2.project_name,\n",
        "        run=\"[RUN NAME]\",\n",
        "        config=config.to_dict()\n",
        "    )\n",
        "\n",
        "wandb_logger = WandbLogger(\n",
        "    project=project_config.magvit2.project_name,\n",
        "    log_model=True\n",
        ")\n",
        "wandb_logger.watch(lit_model, log=\"all\")\n",
        "\n",
        "lr_monitor = LearningRateMonitor(logging_interval='step')\n",
        "\n",
        "checkpoint_callback = ModelCheckpoint(\n",
        "    dirpath=magvit2_config.checkpoint_dir,\n",
        "    filename='magvit2-{epoch:05d}',\n",
        "    every_n_epochs=5,\n",
        "    save_top_k=training_config.save_top_k,\n",
        "    monitor='val/loss',\n",
        "    mode='min'\n",
        ")\n",
        "\n",
        "# Define the EarlyStopping callback\n",
        "early_stop_callback = EarlyStopping(\n",
        "    monitor='val/loss',\n",
        "    min_delta=0.0000000,\n",
        "    patience=100,\n",
        "    verbose=True,\n",
        "    check_finite=True\n",
        ")\n",
        "\n",
        "trainer = pl.Trainer(\n",
        "    max_epochs=training_config.magvit2.max_epochs,\n",
        "    devices=1,\n",
        "    accelerator=\"gpu\",\n",
        "    precision=\"16-mixed\",\n",
        "    logger=wandb_logger,\n",
        "    callbacks=[\n",
        "        lr_monitor,\n",
        "        early_stop_callback,\n",
        "        checkpoint_callback\n",
        "    ],\n",
        "    log_every_n_steps=1,\n",
        "    # gradient_clip_val=1.0,\n",
        "    # overfit_batches=1,\n",
        ")\n",
        "\n",
        "# tuner = Tuner(trainer)\n",
        "# lr_result = tuner.lr_find(lit_model, datamodule=steamboat_willie_data, max_lr=100)\n",
        "# lr_result.plot(show=True, suggest=True)\n",
        "if resume_training:\n",
        "    trainer.fit(lit_model, data, ckpt_path='[CHECKPOINT PATH]')\n",
        "else:\n",
        "    trainer.fit(lit_model, data)\n",
        "\n",
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pcVSPBv2NTtF"
      },
      "source": [
        "## Sweeps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u4yMRkawNVWg"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UCDDpPuCMgAa"
      },
      "source": [
        "# Transformer Decoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gxsxnrTbM8xV"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FXyFcoc6Mj6X"
      },
      "outputs": [],
      "source": [
        "class Attention(nn.Module):\n",
        "    def __init__(\n",
        "            self,\n",
        "            emb_dim,\n",
        "            nheads,\n",
        "            ctx_size,\n",
        "            window_size,\n",
        "            dropout=0.0,\n",
        "            use_flash_attn=True\n",
        "        ):\n",
        "        super().__init__()\n",
        "        assert emb_dim % nheads == 0\n",
        "\n",
        "        self.W_Q = nn.Linear(emb_dim, emb_dim, bias=False)\n",
        "        self.W_K = nn.Linear(emb_dim, emb_dim, bias=False)\n",
        "        self.W_V = nn.Linear(emb_dim, emb_dim, bias=False)\n",
        "\n",
        "        self.W_O = nn.Linear(emb_dim, emb_dim, bias=False)\n",
        "\n",
        "        if not use_flash_attn:\n",
        "            self.register_buffer(\n",
        "                \"mask\",\n",
        "                torch.tril(torch.ones((ctx_size, ctx_size))).reshape(\n",
        "                    1, 1, ctx_size, ctx_size\n",
        "                ),\n",
        "            )\n",
        "\n",
        "        self.head_dim = emb_dim // nheads\n",
        "        self.nheads = nheads\n",
        "        self.window_size = window_size\n",
        "\n",
        "        self.dropout_p = dropout\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        self.use_flash_attn = use_flash_attn\n",
        "\n",
        "    def forward(self, x, slopes):\n",
        "        B, T, D = x.size()\n",
        "\n",
        "        # (B, T, D) -> (B, T, H, H_D)\n",
        "        Q = self.W_Q(x).reshape(B, T, self.nheads, self.head_dim)\n",
        "        K = self.W_K(x).reshape(B, T, self.nheads, self.head_dim)\n",
        "        V = self.W_V(x).reshape(B, T, self.nheads, self.head_dim)\n",
        "\n",
        "        if self.use_flash_attn:\n",
        "            out = flash_attn_func(\n",
        "                Q, K, V,\n",
        "                dropout_p=self.dropout_p if self.training else 0.0,\n",
        "                softmax_scale=None,\n",
        "                causal=True,\n",
        "                window_size=(self.window_size, 0),\n",
        "                alibi_slopes=slopes.to(torch.float32),\n",
        "                deterministic=False\n",
        "            )\n",
        "        else:\n",
        "            # (B, T, H, H_D) -> (B, H, T, H_D)\n",
        "            Q = Q.transpose(1, 2)\n",
        "            K = K.transpose(1, 2)\n",
        "            V = V.transpose(1, 2)\n",
        "\n",
        "            # (B, H, T, H_D) @ (B, H, H_D, T) -> (B, H, T, T)\n",
        "            attn = (Q @ K.transpose(-2, -1)) / (1.0 * math.sqrt(self.head_dim))\n",
        "            # attn = attn + bias\n",
        "            attn = attn.masked_fill(self.mask[:,:,:T,:T]==0, float('-inf'))\n",
        "            attn = F.softmax(attn, dim=-1)\n",
        "\n",
        "            attn = self.dropout(attn)\n",
        "\n",
        "            # ((B, H, T, T)) @ (B, H, T, H_D) -> (B, H, T, H_D)\n",
        "            out = attn @ V\n",
        "\n",
        "        # (B, H, T, H_D) -> (B, T, H, H_D) -> (B, T, D)\n",
        "        out = out.transpose(1, 2).reshape(B, T, self.emb_dim)\n",
        "\n",
        "        # (B, T, D)\n",
        "        out = self.W_O(out)\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self, emb_dim, fan_out=4, dropout=0.0):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(emb_dim, fan_out * emb_dim)\n",
        "        self.act = nn.GELU()\n",
        "        self.fc2 = nn.Linear(fan_out * emb_dim, emb_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.act(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(\n",
        "            self,\n",
        "            emb_dim,\n",
        "            nheads,\n",
        "            ctx_size,\n",
        "            window_size,\n",
        "            fan_out=4,\n",
        "            dropout=0.0,\n",
        "            use_flash_attn=True\n",
        "        ):\n",
        "        super().__init__()\n",
        "        self.ln_1 = nn.LayerNorm(emb_dim)\n",
        "        self.attn = Attention(\n",
        "            emb_dim,\n",
        "            nheads,\n",
        "            ctx_size,\n",
        "            window_size,\n",
        "            dropout,\n",
        "            use_flash_attn\n",
        "        )\n",
        "        self.ln_2 = nn.LayerNorm(emb_dim)\n",
        "        self.mlp = MLP(emb_dim, fan_out, dropout)\n",
        "\n",
        "    def forward(self, x, slopes):\n",
        "        x = x + self.attn(self.ln_1(x), slopes)\n",
        "        x = x + self.mlp(self.ln_2(x))\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TransformerDecoder(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.blocks = nn.ModuleList([\n",
        "            TransformerBlock(\n",
        "                config.emb_dim,\n",
        "                config.nheads,\n",
        "                config.ctx_size,\n",
        "                config.window_size,\n",
        "                config.fan_out,\n",
        "                config.dropout,\n",
        "                config.use_flash_attn\n",
        "            ) for _ in range(config.nlayers)\n",
        "        ])\n",
        "        self.pred_head = nn.Linear(config.emb_dim, config.vocab_size, bias=False)\n",
        "        self.ln_f = nn.LayerNorm(config.emb_dim)\n",
        "\n",
        "        self.register_buffer(\"slopes\", self.get_alibi_slope(config.nheads))\n",
        "        self.window_size = config.window_size\n",
        "\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if not self.config.use_flash_attn:\n",
        "            bias = (self.m * self.get_relative_positions(self.window_size).to(x.device)).unsqueeze(0)\n",
        "\n",
        "        x = self.emb_up_proj(x)\n",
        "\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        for block in self.blocks:\n",
        "            x = block(x, self.slopes)\n",
        "\n",
        "        logits = self.pred_head(self.ln_f(x))\n",
        "\n",
        "        return logits\n",
        "\n",
        "    def get_relative_positions(self, seq_len: int) -> torch.tensor:\n",
        "        x = torch.arange(seq_len)[None, :]\n",
        "        y = torch.arange(seq_len)[:, None]\n",
        "        return (x - y).clamp_max_(0)\n",
        "\n",
        "\n",
        "    def get_alibi_slope(self, num_heads):\n",
        "        x = (2 ** 8) ** (1 / num_heads)\n",
        "        if self.config.use_flash_attn:\n",
        "            x = torch.tensor([1 / x ** (i + 1) for i in range(num_heads)])\n",
        "        else:\n",
        "            x = torch.tensor([1 / x ** (i + 1) for i in range(num_heads)]).unsqueeze(-1).unsqueeze(-1)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Lightning Module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class StepwiseValidationScheduleCallback(Callback):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "\n",
        "    def on_batch_end(self, trainer, pl_module):\n",
        "        if (trainer.global_step + 1) % self.config.validation_step_interval == 0:\n",
        "            trainer.validate(pl_module)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class StepwiseModelCheckpoint(ModelCheckpoint):\n",
        "    def __init__(self, save_step_frequency, dirpath, filename=\"{step}\", **kwargs):\n",
        "        super().__init__(dirpath=dirpath, filename=filename, **kwargs)\n",
        "        self.save_step_frequency = save_step_frequency\n",
        "\n",
        "    def on_train_batch_end(self, trainer, pl_module, outputs, batch, batch_idx, dataloader_idx):\n",
        "        super().on_train_batch_end(trainer, pl_module, outputs, batch, batch_idx, dataloader_idx=dataloader_idx)\n",
        "\n",
        "        global_step = trainer.global_step\n",
        "        if global_step % self.save_step_frequency == 0:\n",
        "            filepath = self.format_checkpoint_name(global_step, {}, verbatim=True).format(step=global_step)\n",
        "            filepath = os.path.join(self.dirpath, f\"{filepath}-step={global_step}.ckpt\")\n",
        "            self._save_model(filepath, trainer, pl_module)\n",
        "\n",
        "            if self.save_top_k > 0:\n",
        "                self._del_model(trainer, pl_module)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class LitTransformerDecoder(pl.LightningModule):\n",
        "    def __init__(self, model, config):\n",
        "        super().__init__()\n",
        "        self.model = model\n",
        "        self.config = config\n",
        "        self.lr = config.lr\n",
        "\n",
        "        if self.logger:\n",
        "            self.logger.experiment.config.update(config)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        logits = self(x)\n",
        "        \n",
        "        loss = F.cross_entropy(logits.reshape(-1, logits.size(-1)), y.reshape(-1))\n",
        "\n",
        "        self.log('train/loss', loss, prog_bar=True)\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        logits = self(x)\n",
        "\n",
        "        loss = F.cross_entropy(logits.reshape(-1, logits.size(-1)), y.reshape(-1))\n",
        "\n",
        "        self.log('val/loss', loss, prog_bar=True)\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.AdamW(\n",
        "            self.parameters(),\n",
        "            lr=self.lr,\n",
        "            betas=(self.config.beta1, self.config.beta2),\n",
        "            weight_decay=self.config.weight_decay\n",
        "        )\n",
        "\n",
        "        if self.config.use_lr_schedule:\n",
        "            scheduler = {\n",
        "                'scheduler': CosineAnnealingLR(optimizer, T_max=self.config.training_steps, eta_min=0),\n",
        "                'interval': 'step',\n",
        "                'frequency': 1,\n",
        "            }\n",
        "            return [optimizer], [scheduler]\n",
        "\n",
        "        return optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TBX-OdpfM_IU"
      },
      "source": [
        "## Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4tHjTDGLNC7J"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M3bwY6WVND8P"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DSG27RTpNFdx"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HDLsJi3QNGjt"
      },
      "source": [
        "## Sweeps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UHoSpKtTNIWv"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AHuAfaKqMpBx"
      },
      "source": [
        "# Super Resolution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bsyvtzf6NZgX"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WQACSEuLMrnF"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Lightning Module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rBHkOWpENbnJ"
      },
      "source": [
        "## Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3FEufD9mNfP0"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wYox-rbfNftJ"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pGmjT20lNhIh"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UmjlioSINhvk"
      },
      "source": [
        "## Sweeps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pi6OQ4EeNjg7"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyO3KrMG+gJUO4IfDxp7ZPmb",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
