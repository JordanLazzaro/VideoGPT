{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jjF85_2XJ1f6"
      },
      "outputs": [],
      "source": [
        "!pip install -q wandb pytorch_lightning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t11yJ4V8J6qr"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "import math\n",
        "import torch\n",
        "import torch.utils.data\n",
        "import torchvision\n",
        "import wandb\n",
        "import pytorch_lightning as pl\n",
        "\n",
        "from __future__ import print_function\n",
        "from collections import OrderedDict\n",
        "from torch import nn, optim\n",
        "from torch.nn import functional as F\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.utils import save_image\n",
        "from pytorch_lightning.loggers import WandbLogger\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint, LearningRateMonitor, EarlyStopping\n",
        "from pytorch_lightning.tuner import Tuner\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR, OneCycleLR\n",
        "\n",
        "\n",
        "pl.seed_everything(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G5xTdpkbJ-ls"
      },
      "outputs": [],
      "source": [
        "class CIFAR10DataModule(pl.LightningDataModule):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.batch_size = config.batch_size\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "        ])\n",
        "\n",
        "    def prepare_data(self):\n",
        "        torchvision.datasets.CIFAR10(root='./data', train=True, download=True)\n",
        "        torchvision.datasets.CIFAR10(root='./data', train=False, download=True)\n",
        "\n",
        "    def setup(self, stage=None):\n",
        "        if stage in ('fit', None):\n",
        "            self.cifar10_train = torchvision.datasets.CIFAR10(\n",
        "                root='./data', train=True, transform=self.transform)\n",
        "            self.cifar10_val = torchvision.datasets.CIFAR10(\n",
        "                root='./data', train=False, transform=self.transform)\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return torch.utils.data.DataLoader(\n",
        "            self.cifar10_train,\n",
        "            batch_size=self.batch_size,\n",
        "            shuffle=True,\n",
        "            num_workers=2, # TODO: check if we need more workers\n",
        "            pin_memory=True\n",
        "        )\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return torch.utils.data.DataLoader(\n",
        "            self.cifar10_val,\n",
        "            batch_size=self.batch_size,\n",
        "            shuffle=False,\n",
        "            num_workers=2, # TODO: check if we need more workers\n",
        "            pin_memory=True\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
        "dataloader = torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=False, num_workers=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "img, label = next(iter(dataloader))\n",
        "img.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Quantizer(nn.Module):\n",
        "    def __init__(self, config, init_codebook=None):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.codebook = nn.Embedding(init_codebook.shape[0], init_codebook.shape[1])\n",
        "        self.codebook.weight = nn.Parameter(init_codebook, requires_grad=True)\n",
        "        if init_codebook is None:\n",
        "            self.codebook = nn.Embedding(config.codebook_size, config.latent_dim)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        inputs = inputs.permute(0, 2, 3, 1).contiguous()\n",
        "        flat_inputs = inputs.reshape(-1, self.config.latent_dim)\n",
        "\n",
        "        # Σ(x-y)^2 = Σx^2 - 2xy + Σy^2\n",
        "        dists = (\n",
        "            torch.sum(flat_inputs ** 2, dim=1, keepdim=True) - # Σx^2\n",
        "            2 * (flat_inputs @ self.codebook.weight.t()) +     # 2*xy\n",
        "            torch.sum(self.codebook.weight ** 2, dim=1)        # Σy^2\n",
        "        )\n",
        "\n",
        "        enc_idxs = torch.argmin(dists, dim=1)\n",
        "        quantized_inputs = self.codebook(enc_idxs).reshape(inputs.shape)\n",
        "\n",
        "        # \"The VQ objective uses the l2 error to move the embedding vectors\n",
        "        # e_i towards the encoder outputs z_e(x)\"\n",
        "        embedding_loss = F.mse_loss(quantized_inputs, inputs.detach())\n",
        "        \n",
        "        # \"since the volume of the embedding space is dimensionless, it can grow\n",
        "        # arbitrarily if the embeddings e_i do not train as fast as the encoder\n",
        "        # parameters. To make sure the encoder commits to an embedding and its\n",
        "        # output does not grow, we add a commitment loss\"\n",
        "        commitment_loss = F.mse_loss(quantized_inputs.detach(), inputs)\n",
        "        \n",
        "        # parts 2 & 3 of full loss (ie. not including reconstruciton loss)\n",
        "        vq_loss = commitment_loss * self.config.embed_loss_beta + embedding_loss\n",
        "\n",
        "        # sets the output to be the input plus the residual value between the\n",
        "        # quantized latents and the inputs like a resnet for Straight Through\n",
        "        # Estimation (STE)\n",
        "        quantized_inputs = inputs + (quantized_inputs - inputs).detach()\n",
        "        quantized_inputs = quantized_inputs.permute(0, 3, 1, 2).contiguous()\n",
        "\n",
        "        return quantized_inputs, vq_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Config:\n",
        "    def __init__(self):\n",
        "        self.codebook_size = 10\n",
        "        self.latent_dim = 8\n",
        "        self.embed_loss_beta = 0.25\n",
        "\n",
        "config = Config()\n",
        "codebook = torch.ones((10, 8)) * torch.arange(1, 11).unsqueeze(1)\n",
        "print(f'codebook shape: {codebook.shape}')\n",
        "input = torch.ones(1, 8, 2, 2) * torch.arange(1, 5).view(2, 2) + 0.1\n",
        "print(f'input shape: {input.shape}')\n",
        "\n",
        "quantizer, vq_loss = Quantizer(config, codebook)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "quantized_input = quantizer(input)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f'input:\\n {input}\\n')\n",
        "print(f'quantized_input:\\n {quantized_input}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L8FV0QSNKCEL"
      },
      "outputs": [],
      "source": [
        "class ResBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        '''\n",
        "        \"[...] followed by two residual 3 × 3 blocks (implemented as ReLU, 3x3 conv,\n",
        "        ReLU, 1x1 conv), all having 256 hidden units.\"\n",
        "        '''\n",
        "        super().__init__()\n",
        "        if in_channels != out_channels:\n",
        "            self.identity = nn.Conv2d(\n",
        "                in_channels=in_channels,\n",
        "                out_channels=out_channels,\n",
        "                kernel_size=1\n",
        "            )\n",
        "        else:\n",
        "            self.identity = nn.Identity()\n",
        "\n",
        "        hidden_channels = out_channels // 4\n",
        "\n",
        "        self.block = nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_channels=in_channels,\n",
        "                out_channels=hidden_channels,\n",
        "                kernel_size=3,\n",
        "                padding=1\n",
        "            ),\n",
        "            nn.BatchNorm2d(hidden_channels),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Conv2d(\n",
        "                in_channels=hidden_channels,\n",
        "                out_channels=hidden_channels,\n",
        "                kernel_size=3,\n",
        "                padding=1\n",
        "            ),\n",
        "            nn.BatchNorm2d(hidden_channels),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Conv2d(\n",
        "                in_channels=hidden_channels,\n",
        "                out_channels=hidden_channels,\n",
        "                kernel_size=3,\n",
        "                padding=1\n",
        "            ),\n",
        "            nn.BatchNorm2d(hidden_channels),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Conv2d(\n",
        "                in_channels=hidden_channels,\n",
        "                out_channels=out_channels,\n",
        "                kernel_size=1\n",
        "            )\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.block(x) + self.identity(x)\n",
        "        return F.leaky_relu(out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WPRmSrppKFUh"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        '''\n",
        "        \"The encoder consists of 2 strided convolutional layers with stride 2 and\n",
        "        window size 4 × 4, followed by two residual 3 × 3 blocks (implemented as ReLU,\n",
        "        3x3 conv, ReLU, 1x1 conv), all having 256 hidden units.\"\n",
        "        '''\n",
        "        super().__init__()\n",
        "\n",
        "        blocks = []\n",
        "        curr_channel = config.in_channels\n",
        "        for out_channel in config.channel_dims:\n",
        "            blocks.append(\n",
        "                nn.Sequential(\n",
        "                    # reduce spatial dims increase channels\n",
        "                    nn.Conv2d(\n",
        "                        in_channels=curr_channel,\n",
        "                        out_channels=out_channel,\n",
        "                        kernel_size=3,\n",
        "                        stride=2,\n",
        "                        padding=1\n",
        "                    ),\n",
        "                    nn.BatchNorm2d(out_channel),\n",
        "                    nn.LeakyReLU(),\n",
        "                    # continue at current dim and channels\n",
        "                    nn.Sequential(*[\n",
        "                        ResBlock(\n",
        "                            in_channels=out_channel,\n",
        "                            out_channels=out_channel\n",
        "                        ) for _ in range(config.nblocks)\n",
        "                    ])\n",
        "                ),\n",
        "            )\n",
        "            curr_channel = out_channel\n",
        "\n",
        "        self.blocks    = nn.Sequential(*blocks)\n",
        "        self.fc_mu     = nn.Linear(config.channel_dims[-1]*4, config.latent_dim)\n",
        "        self.fc_logvar = nn.Linear(config.channel_dims[-1]*4, config.latent_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.blocks(x)\n",
        "        x = torch.flatten(x, start_dim=1)\n",
        "        return self.fc_mu(x), self.fc_logvar(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "28MR93jRKHy9"
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        '''\n",
        "        \"The decoder similarly has two residual 3 × 3 blocks, followed by\n",
        "        two transposed convolutions with stride 2 and window size 4 × 4\"\n",
        "        '''\n",
        "        super().__init__()\n",
        "\n",
        "        self.latent_map = nn.Linear(config.latent_dim, config.channel_dims[-1]*4)\n",
        "\n",
        "        blocks = []\n",
        "        reverse_channels = config.channel_dims\n",
        "        reverse_channels = list(config.channel_dims)[::-1]\n",
        "        for i in range(len(reverse_channels)-1):\n",
        "            blocks.append(\n",
        "                nn.Sequential(\n",
        "                    # continue at current dim and channels\n",
        "                    nn.Sequential(*[\n",
        "                        ResBlock(\n",
        "                            in_channels=reverse_channels[i],\n",
        "                            out_channels=reverse_channels[i]\n",
        "                        ) for _ in range(config.nblocks)\n",
        "                    ]),\n",
        "                    # reduce channels increase spatial dims\n",
        "                    nn.ConvTranspose2d(\n",
        "                        in_channels=reverse_channels[i],\n",
        "                        out_channels=reverse_channels[i+1],\n",
        "                        kernel_size=3,\n",
        "                        stride = 2,\n",
        "                        padding=1,\n",
        "                        output_padding=1\n",
        "                    ),\n",
        "                    nn.BatchNorm2d(reverse_channels[i+1]),\n",
        "                    nn.LeakyReLU()\n",
        "                )\n",
        "            )\n",
        "        self.blocks = nn.Sequential(*blocks)\n",
        "        self.final_layer = nn.Sequential(\n",
        "            nn.ConvTranspose2d(\n",
        "                in_channels=reverse_channels[-1],\n",
        "                out_channels=config.in_channels,\n",
        "                kernel_size=3,\n",
        "                stride = 2,\n",
        "                padding=1,\n",
        "                output_padding=1\n",
        "            ),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "        self.config = config\n",
        "\n",
        "    def forward(self, z):\n",
        "        z = self.latent_map(z)\n",
        "        z = z.view(-1, self.config.channel_dims[-1], 2, 2)\n",
        "        z = self.blocks(z)\n",
        "        return self.final_layer(z)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1yawr70bKImu"
      },
      "outputs": [],
      "source": [
        "class VQVAE(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.encoder = Encoder(config)\n",
        "        self.decoder = Decoder(config)\n",
        "        self.config = config\n",
        "\n",
        "    def encode(self, x):\n",
        "        return self.encoder(x)\n",
        "\n",
        "    def decode(self, z):\n",
        "        return self.decoder(z)\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = torch.exp(0.5*logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return eps * std + mu\n",
        "\n",
        "    def loss(self, x_hat, x, mu, logvar):\n",
        "        MSE = F.mse_loss(x_hat, x)\n",
        "        MKLD = torch.mean(-0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp(), dim=1), dim=0)\n",
        "        loss = MSE + self.config.kld_weight * MKLD\n",
        "        return loss, MSE, MKLD\n",
        "\n",
        "    def forward(self, x):\n",
        "        mu, logvar = self.encode(x)\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        x_hat = self.decode(z)\n",
        "        return x_hat, mu, logvar"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Lightning Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kN5eG3HnKLEF"
      },
      "outputs": [],
      "source": [
        "class LitVQVAE(pl.LightningModule):\n",
        "    def __init__(self, model, config=None):\n",
        "        super().__init__()\n",
        "        self.model = model\n",
        "        self.config = config\n",
        "        self.lr = config.lr\n",
        "\n",
        "        if self.logger:\n",
        "            self.logger.experiment.config.update(config)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        x, _ = batch\n",
        "        x_hat, mu, logvar = self(x)\n",
        "\n",
        "        loss, MSE, MKLD = self.model.loss(x_hat, x, mu, logvar)\n",
        "        self.log('train/loss', loss, prog_bar=True)\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        x, _ = batch\n",
        "        x_hat, mu, logvar = self(x)\n",
        "\n",
        "        loss, MSE, MKLD = self.model.loss(x_hat, x, mu, logvar)\n",
        "        self.log('val/loss', loss, prog_bar=True)\n",
        "\n",
        "        if batch_idx == 0:\n",
        "            n_images = min(x.size(0), 8)\n",
        "            comparison = torch.cat([x[:n_images], x_hat[:n_images]])\n",
        "            grid = torchvision.utils.make_grid(comparison)\n",
        "            self.logger.experiment.log({\"val/reconstructions\": [wandb.Image(grid, caption=\"Top: Original, Bottom: Reconstructed\")]})\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        self.optimizer = torch.optim.AdamW(self.parameters(), lr=self.lr, betas=(self.config.beta1, self.config.beta2))\n",
        "        self.scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(self.optimizer, T_max=self.config.max_epochs)\n",
        "        return [self.optimizer], [self.scheduler]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x9uUkXe1KNf_"
      },
      "outputs": [],
      "source": [
        "def get_num_downsample_layers(img_size):\n",
        "    \"\"\"\n",
        "    Get the number of strided Conv2D layers\n",
        "    required to produce a 2x2 output volume\n",
        "    \"\"\"\n",
        "    if img_size < 2:\n",
        "        raise ValueError(\"Image size must be at least 2x2.\")\n",
        "\n",
        "    # Calculate the minimum number of downsample layers required for 2x2 final\n",
        "    num_layers = math.ceil(math.log2(img_size / 2))\n",
        "    return num_layers\n",
        "\n",
        "def build_channel_dims(start_channels, nlayers):\n",
        "    \"\"\"\n",
        "    Construct a list of channel counts for nlayers downsample layers\n",
        "    assuming the channels double as spatial dims halve\n",
        "    \"\"\"\n",
        "    channels = []\n",
        "    for _ in range(nlayers):\n",
        "        channels.append(start_channels)\n",
        "        start_channels *= 2\n",
        "    return channels\n",
        "\n",
        "class CIFAR10VQVAEConfig:\n",
        "    def __init__(self):\n",
        "        # model checkpoints\n",
        "        self.checkpoint_path = \"./checkpoints\"\n",
        "        self.save_top_k = 1\n",
        "        # training\n",
        "        self.batch_size = 128\n",
        "        self.max_epochs = 60\n",
        "        self.training_steps = 250,000\n",
        "        # optimizer\n",
        "        self.lr = 2e-4\n",
        "        self.beta1 = 0.9\n",
        "        self.beta2 = 0.999\n",
        "        # input properties\n",
        "        self.img_size = 32\n",
        "        self.in_channels = 3\n",
        "        # bottleneck/latents\n",
        "        self.latent_dim = 64\n",
        "        self.codebook_size = 512\n",
        "        self.embed_loss_beta = 0.25\n",
        "        # encoder/decoder\n",
        "        self.start_channels = 32\n",
        "        self.nblocks = 0\n",
        "        self.nlayers = get_num_downsample_layers(self.img_size)\n",
        "        self.channel_dims = build_channel_dims(self.start_channels, self.nlayers)\n",
        "\n",
        "    def update(self, updates):\n",
        "        for key, value in updates.items():\n",
        "            if hasattr(self, key):\n",
        "                setattr(self, key, value)\n",
        "        self.nlayers = get_num_downsample_layers(self.img_size)\n",
        "        self.channel_dims = build_channel_dims(self.start_channels, self.nlayers)\n",
        "\n",
        "    def to_dict(self):\n",
        "        return {k: v for k, v in self.__dict__.items()}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Oah3ySaKSQk"
      },
      "outputs": [],
      "source": [
        "config = CIFAR10VQVAEConfig()\n",
        "model = VQVAE(config)\n",
        "lit_model = LitVQVAE(model, config)\n",
        "cifar10_data = CIFAR10DataModule(config)\n",
        "\n",
        "wandb.init(project=\"VQVAE CIFAR-10\", config=config.to_dict())\n",
        "wandb_logger = WandbLogger(project=\"VQVAE CIFAR-10\", log_model=False)\n",
        "wandb_logger.watch(lit_model, log=\"all\")\n",
        "\n",
        "lr_monitor = LearningRateMonitor(logging_interval='step')\n",
        "\n",
        "checkpoint_callback = ModelCheckpoint(\n",
        "    dirpath=config.checkpoint_path,\n",
        "    filename='model-{epoch:02d}-{val_loss:.2f}',\n",
        "    every_n_epochs=5,\n",
        "    save_top_k=config.save_top_k,\n",
        "    monitor='val/loss',\n",
        "    mode='min',\n",
        "    save_last=True\n",
        ")\n",
        "\n",
        "# Define the EarlyStopping callback\n",
        "early_stop_callback = EarlyStopping(\n",
        "    monitor='val/loss',\n",
        "    min_delta=0.00,\n",
        "    patience=3,\n",
        "    verbose=True,\n",
        "    check_finite=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dFM3LWghKUOv"
      },
      "outputs": [],
      "source": [
        "#wandb.finish()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eWPlQ1PNK4bs"
      },
      "outputs": [],
      "source": [
        "trainer = pl.Trainer(\n",
        "    max_epochs=config.max_epochs,\n",
        "    devices=1,\n",
        "    accelerator=\"gpu\",\n",
        "    precision=\"16-mixed\",\n",
        "    logger=wandb_logger,\n",
        "    callbacks=[\n",
        "        lr_monitor,\n",
        "        early_stop_callback,\n",
        "        # checkpoint_callback\n",
        "    ],\n",
        "    log_every_n_steps=1,\n",
        ")\n",
        "\n",
        "# tuner = Tuner(trainer)\n",
        "# tuner.lr_find(lit_model, datamodule=cifar10_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3-swnb5PK5rK"
      },
      "outputs": [],
      "source": [
        "trainer.fit(lit_model, cifar10_data)\n",
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Sweeps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iab5_ZrqK7td"
      },
      "outputs": [],
      "source": [
        "sweep_config = {\n",
        "    'method': 'random',\n",
        "    'metric': {\n",
        "        'name': 'val/loss',\n",
        "        'goal': 'minimize'\n",
        "    },\n",
        "    'parameters': {\n",
        "        'nblocks': {\n",
        "            'values': [0, 1, 2]\n",
        "        },\n",
        "        'latent_dim': {\n",
        "            'values': [128, 256, 512]\n",
        "        },\n",
        "        'start_channels': {\n",
        "            'values': [32, 64, 128]\n",
        "        },\n",
        "        'lr': {\n",
        "            'min': 1e-5,\n",
        "            'max': 1e-2,\n",
        "            'distribution': 'uniform'\n",
        "        },\n",
        "        'beta1': {\n",
        "            'values': [0.9, 0.95, 0.99]\n",
        "        },\n",
        "        'beta2': {\n",
        "            'values': [0.999, 0.9999]\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "sweep_id = wandb.sweep(sweep_config, project=\"VQVAE CIFAR-10\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0BfB-TYDK-Ma"
      },
      "outputs": [],
      "source": [
        "def train():\n",
        "    with wandb.init() as run:\n",
        "        config = CIFAR10VQVAEConfig()\n",
        "        config.update(wandb.config)\n",
        "\n",
        "        model = VQVAE(config)\n",
        "        lit_model = LitVQVAE(model, config)\n",
        "        cifar10_data = CIFAR10DataModule(config)\n",
        "\n",
        "        wandb_logger = WandbLogger(project=\"VQVAE CIFAR-10\", log_model=False)\n",
        "\n",
        "        early_stop_callback = EarlyStopping(\n",
        "            monitor='val/loss',\n",
        "            min_delta=0.00,\n",
        "            patience=3,\n",
        "            verbose=True,\n",
        "            check_finite=True\n",
        "        )\n",
        "\n",
        "        trainer = pl.Trainer(\n",
        "            max_epochs=config.max_epochs,\n",
        "            devices=1,\n",
        "            accelerator=\"gpu\",\n",
        "            precision=\"16-mixed\",\n",
        "            logger=wandb_logger,\n",
        "            callbacks=[early_stop_callback]\n",
        "        )\n",
        "\n",
        "        trainer.fit(lit_model, cifar10_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cV_krC5nLB2W"
      },
      "outputs": [],
      "source": [
        "wandb.agent(sweep_id, train, count=5)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
