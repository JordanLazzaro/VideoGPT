{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jjF85_2XJ1f6"
      },
      "outputs": [],
      "source": [
        "!pip install -q wandb pytorch_lightning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t11yJ4V8J6qr"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "import math\n",
        "import torch\n",
        "import torch.utils.data\n",
        "import torchvision\n",
        "import wandb\n",
        "import pytorch_lightning as pl\n",
        "\n",
        "from __future__ import print_function\n",
        "from collections import OrderedDict\n",
        "from torch import nn, optim\n",
        "from torch.nn import functional as F\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.utils import save_image\n",
        "from pytorch_lightning.loggers import WandbLogger\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint, LearningRateMonitor\n",
        "from pytorch_lightning.tuner import Tuner\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR, OneCycleLR\n",
        "\n",
        "\n",
        "pl.seed_everything(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G5xTdpkbJ-ls"
      },
      "outputs": [],
      "source": [
        "class CIFAR10DataModule(pl.LightningDataModule):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.batch_size = config.batch_size\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "        ])\n",
        "\n",
        "    def prepare_data(self):\n",
        "        torchvision.datasets.CIFAR10(root='./data', train=True, download=True)\n",
        "        torchvision.datasets.CIFAR10(root='./data', train=False, download=True)\n",
        "\n",
        "    def setup(self, stage=None):\n",
        "        if stage in ('fit', None):\n",
        "            self.cifar10_train = torchvision.datasets.CIFAR10(\n",
        "                root='./data', train=True, transform=self.transform)\n",
        "            self.cifar10_val = torchvision.datasets.CIFAR10(\n",
        "                root='./data', train=False, transform=self.transform)\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return torch.utils.data.DataLoader(\n",
        "            self.cifar10_train,\n",
        "            batch_size=self.batch_size,\n",
        "            shuffle=True,\n",
        "            num_workers=2, # TODO: check if we need more workers\n",
        "            pin_memory=True\n",
        "        )\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return torch.utils.data.DataLoader(\n",
        "            self.cifar10_val,\n",
        "            batch_size=self.batch_size,\n",
        "            shuffle=False,\n",
        "            num_workers=2, # TODO: check if we need more workers\n",
        "            pin_memory=True\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
        "dataloader = torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=False, num_workers=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "img, label = next(iter(dataloader))\n",
        "img.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "K, D = 100, 10\n",
        "codes = nn.Embedding(K, D)\n",
        "latents = torch.randn(1, D, 2, 2)\n",
        "print(latents.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "latents = latents.permute(0, 2, 3, 1).contiguous()\n",
        "print(latents.shape)\n",
        "flat_latents = latents.view(-1, D)\n",
        "print(flat_latents.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Σ(x-y)^2 = Σx^2 - 2xy + Σy^2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "latent_dists = torch.sum(flat_latents ** 2, dim=1, keepdim=True)\n",
        "print(f'||latents||^2: {latent_dists.shape}')\n",
        "\n",
        "code_dists = torch.sum(codes.weight ** 2, dim=1)\n",
        "print(f'||codes||^2: {code_dists.shape}\\n')\n",
        "\n",
        "lat_code_dists = 2 * (flat_latents @ codes.weight.t())\n",
        "print(f'2*lats@codes: {lat_code_dists.shape}\\n')\n",
        "\n",
        "dist = latent_dists + code_dists - lat_code_dists  # [BHW x K]\n",
        "print(f'dist.shape: {dist.shape} = {latent_dists.shape} + {code_dists.shape} - {lat_code_dists.shape}\\n')\n",
        "\n",
        "# Get the encoding that has the min distance\n",
        "encoding_inds = torch.argmin(dist, dim=1).unsqueeze(1)  # [BHW, 1]\n",
        "print(f'encoding_inds.shape: {encoding_inds.shape}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L8FV0QSNKCEL"
      },
      "outputs": [],
      "source": [
        "class ResBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        if in_channels != out_channels:\n",
        "            self.identity = nn.Conv2d(\n",
        "                in_channels=in_channels,\n",
        "                out_channels=out_channels,\n",
        "                kernel_size=1\n",
        "            )\n",
        "        else:\n",
        "            self.identity = nn.Identity()\n",
        "\n",
        "        hidden_channels = out_channels // 4\n",
        "\n",
        "        self.block = nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_channels=in_channels,\n",
        "                out_channels=hidden_channels,\n",
        "                kernel_size=3,\n",
        "                padding=1\n",
        "            ),\n",
        "            nn.BatchNorm2d(hidden_channels),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Conv2d(\n",
        "                in_channels=hidden_channels,\n",
        "                out_channels=hidden_channels,\n",
        "                kernel_size=3,\n",
        "                padding=1\n",
        "            ),\n",
        "            nn.BatchNorm2d(hidden_channels),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Conv2d(\n",
        "                in_channels=hidden_channels,\n",
        "                out_channels=hidden_channels,\n",
        "                kernel_size=3,\n",
        "                padding=1\n",
        "            ),\n",
        "            nn.BatchNorm2d(hidden_channels),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Conv2d(\n",
        "                in_channels=hidden_channels,\n",
        "                out_channels=out_channels,\n",
        "                kernel_size=1\n",
        "            )\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.block(x) + self.identity(x)\n",
        "        return F.leaky_relu(out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WPRmSrppKFUh"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "\n",
        "        blocks = []\n",
        "        curr_channel = config.in_channels\n",
        "        for out_channel in config.channel_dims:\n",
        "            blocks.append(\n",
        "                nn.Sequential(\n",
        "                    # reduce spatial dims increase channels\n",
        "                    nn.Conv2d(\n",
        "                        in_channels=curr_channel,\n",
        "                        out_channels=out_channel,\n",
        "                        kernel_size=3,\n",
        "                        stride=2,\n",
        "                        padding=1\n",
        "                    ),\n",
        "                    nn.BatchNorm2d(out_channel),\n",
        "                    nn.LeakyReLU(),\n",
        "                    # continue at current dim and channels\n",
        "                    nn.Sequential(*[\n",
        "                        ResBlock(\n",
        "                            in_channels=out_channel,\n",
        "                            out_channels=out_channel\n",
        "                        ) for _ in range(config.nblocks)\n",
        "                    ])\n",
        "                ),\n",
        "            )\n",
        "            curr_channel = out_channel\n",
        "\n",
        "        self.blocks    = nn.Sequential(*blocks)\n",
        "        self.fc_mu     = nn.Linear(config.channel_dims[-1]*4, config.latent_dim)\n",
        "        self.fc_logvar = nn.Linear(config.channel_dims[-1]*4, config.latent_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.blocks(x)\n",
        "        x = torch.flatten(x, start_dim=1)\n",
        "        return self.fc_mu(x), self.fc_logvar(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "28MR93jRKHy9"
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "\n",
        "        self.latent_map = nn.Linear(config.latent_dim, config.channel_dims[-1]*4)\n",
        "\n",
        "        blocks = []\n",
        "        reverse_channels = config.channel_dims\n",
        "        reverse_channels = list(config.channel_dims)[::-1]\n",
        "        for i in range(len(reverse_channels)-1):\n",
        "            blocks.append(\n",
        "                nn.Sequential(\n",
        "                    # continue at current dim and channels\n",
        "                    nn.Sequential(*[\n",
        "                        ResBlock(\n",
        "                            in_channels=reverse_channels[i],\n",
        "                            out_channels=reverse_channels[i]\n",
        "                        ) for _ in range(config.nblocks)\n",
        "                    ]),\n",
        "                    # reduce channels increase spatial dims\n",
        "                    nn.ConvTranspose2d(\n",
        "                        in_channels=reverse_channels[i],\n",
        "                        out_channels=reverse_channels[i+1],\n",
        "                        kernel_size=3,\n",
        "                        stride = 2,\n",
        "                        padding=1,\n",
        "                        output_padding=1\n",
        "                    ),\n",
        "                    nn.BatchNorm2d(reverse_channels[i+1]),\n",
        "                    nn.LeakyReLU()\n",
        "                )\n",
        "            )\n",
        "        self.blocks = nn.Sequential(*blocks)\n",
        "        self.final_layer = nn.Sequential(\n",
        "            nn.ConvTranspose2d(\n",
        "                in_channels=reverse_channels[-1],\n",
        "                out_channels=config.in_channels,\n",
        "                kernel_size=3,\n",
        "                stride = 2,\n",
        "                padding=1,\n",
        "                output_padding=1\n",
        "            ),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "        self.config = config\n",
        "\n",
        "    def forward(self, z):\n",
        "        z = self.latent_map(z)\n",
        "        z = z.view(-1, self.config.channel_dims[-1], 2, 2)\n",
        "        z = self.blocks(z)\n",
        "        return self.final_layer(z)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nJQXezyYLP08"
      },
      "outputs": [],
      "source": [
        "class Quantizer(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.codebook = nn.Embedding(config.codebook_size, config.latent_dim)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1yawr70bKImu"
      },
      "outputs": [],
      "source": [
        "class VQVAE(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.encoder = Encoder(config)\n",
        "        self.decoder = Decoder(config)\n",
        "        self.config = config\n",
        "\n",
        "    def encode(self, x):\n",
        "        return self.encoder(x)\n",
        "\n",
        "    def decode(self, z):\n",
        "        return self.decoder(z)\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = torch.exp(0.5*logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return eps * std + mu\n",
        "\n",
        "    def loss(self, x_hat, x, mu, logvar):\n",
        "        MSE = F.mse_loss(x_hat, x)\n",
        "        MKLD = torch.mean(-0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp(), dim=1), dim=0)\n",
        "        loss = MSE + self.config.kld_weight * MKLD\n",
        "        return loss, MSE, MKLD\n",
        "\n",
        "    def forward(self, x):\n",
        "        mu, logvar = self.encode(x)\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        x_hat = self.decode(z)\n",
        "        return x_hat, mu, logvar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kN5eG3HnKLEF"
      },
      "outputs": [],
      "source": [
        "class LitVQVAE(pl.LightningModule):\n",
        "    def __init__(self, model, config=None):\n",
        "        super().__init__()\n",
        "        self.model = model\n",
        "        self.config = config\n",
        "        self.lr = config.lr\n",
        "\n",
        "        if self.logger:\n",
        "            self.logger.experiment.config.update(config)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        x, _ = batch\n",
        "        x_hat, mu, logvar = self(x)\n",
        "\n",
        "        loss, MSE, MKLD = self.model.loss(x_hat, x, mu, logvar)\n",
        "        self.log('train/loss', loss, prog_bar=True)\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        x, _ = batch\n",
        "        x_hat, mu, logvar = self(x)\n",
        "\n",
        "        loss, MSE, MKLD = self.model.loss(x_hat, x, mu, logvar)\n",
        "        self.log('val/loss', loss, prog_bar=True)\n",
        "\n",
        "        if batch_idx == 0:\n",
        "            n_images = min(x.size(0), 8)\n",
        "            comparison = torch.cat([x[:n_images], x_hat[:n_images]])\n",
        "            grid = torchvision.utils.make_grid(comparison)\n",
        "            self.logger.experiment.log({\"val/reconstructions\": [wandb.Image(grid, caption=\"Top: Original, Bottom: Reconstructed\")]})\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        self.optimizer = torch.optim.AdamW(self.parameters(), lr=self.lr, betas=(self.config.beta1, self.config.beta2))\n",
        "        self.scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(self.optimizer, T_max=self.config.max_epochs)\n",
        "        return [self.optimizer], [self.scheduler]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x9uUkXe1KNf_"
      },
      "outputs": [],
      "source": [
        "def get_num_downsample_layers(img_size):\n",
        "    \"\"\"\n",
        "    Get the number of strided Conv2D layers\n",
        "    required to produce a 2x2 output volume\n",
        "    \"\"\"\n",
        "    if img_size < 2:\n",
        "        raise ValueError(\"Image size must be at least 2x2.\")\n",
        "\n",
        "    # Calculate the minimum number of downsample layers required for 2x2 final\n",
        "    num_layers = math.ceil(math.log2(img_size / 2))\n",
        "    return num_layers\n",
        "\n",
        "def build_channel_dims(start_channels, nlayers):\n",
        "    \"\"\"\n",
        "    Construct a list of channel counts for nlayers downsample layers\n",
        "    assuming the channels double as spatial dims halve\n",
        "    \"\"\"\n",
        "    channels = []\n",
        "    for _ in range(nlayers):\n",
        "        channels.append(start_channels)\n",
        "        start_channels *= 2\n",
        "    return channels\n",
        "\n",
        "class CIFAR10VAEConfig:\n",
        "    def __init__(self):\n",
        "        self.checkpoint_path = \"./checkpoints\"\n",
        "        self.save_top_k = 1\n",
        "        self.batch_size = 2048 # TODO: maxout for max throughput\n",
        "        self.max_epochs = 60\n",
        "        self.lr = 3e-4\n",
        "        self.beta1 = 0.9\n",
        "        self.beta2 = 0.999\n",
        "        self.img_size = 32\n",
        "        self.in_channels = 3\n",
        "        self.latent_dim = 128\n",
        "        self.kld_weight = 0.000025\n",
        "        self.start_channels = 32\n",
        "        self.nblocks = 0\n",
        "        self.nlayers = get_num_downsample_layers(self.img_size)\n",
        "        self.channel_dims = build_channel_dims(self.start_channels, self.nlayers)\n",
        "\n",
        "    def update(self, updates):\n",
        "        for key, value in updates.items():\n",
        "            if hasattr(self, key):\n",
        "                setattr(self, key, value)\n",
        "        self.nlayers = get_num_downsample_layers(self.img_size)\n",
        "        self.channel_dims = build_channel_dims(self.start_channels, self.nlayers)\n",
        "\n",
        "    def to_dict(self):\n",
        "        return {k: v for k, v in self.__dict__.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Oah3ySaKSQk"
      },
      "outputs": [],
      "source": [
        "config = CIFAR10VAEConfig()\n",
        "model = VAE(config)\n",
        "lit_model = LitVAE(model, config)\n",
        "cifar10_data = CIFAR10DataModule(config)\n",
        "\n",
        "wandb.init(project=\"VAE CIFAR-10\", config=config.to_dict())\n",
        "wandb_logger = WandbLogger(project=\"VAE CIFAR-10\", log_model=False)\n",
        "wandb_logger.watch(lit_model, log=\"all\")\n",
        "\n",
        "lr_monitor = LearningRateMonitor(logging_interval='step')\n",
        "\n",
        "checkpoint_callback = ModelCheckpoint(\n",
        "    dirpath=config.checkpoint_path,\n",
        "    filename='model-{epoch:02d}-{val_loss:.2f}',\n",
        "    every_n_epochs=5,\n",
        "    save_top_k=config.save_top_k,\n",
        "    monitor='val/loss',\n",
        "    mode='min',\n",
        "    save_last=True\n",
        ")\n",
        "\n",
        "# Define the EarlyStopping callback\n",
        "early_stop_callback = EarlyStopping(\n",
        "    monitor='val/loss',\n",
        "    min_delta=0.00,\n",
        "    patience=3,\n",
        "    verbose=True,\n",
        "    check_finite=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dFM3LWghKUOv"
      },
      "outputs": [],
      "source": [
        "#wandb.finish()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eWPlQ1PNK4bs"
      },
      "outputs": [],
      "source": [
        "trainer = pl.Trainer(\n",
        "    max_epochs=config.max_epochs,\n",
        "    devices=1,\n",
        "    accelerator=\"gpu\",\n",
        "    precision=\"16-mixed\",\n",
        "    logger=wandb_logger,\n",
        "    callbacks=[\n",
        "        lr_monitor,\n",
        "        early_stop_callback,\n",
        "        # checkpoint_callback\n",
        "    ],\n",
        "    log_every_n_steps=1,\n",
        ")\n",
        "\n",
        "# tuner = Tuner(trainer)\n",
        "# tuner.lr_find(lit_model, datamodule=cifar10_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3-swnb5PK5rK"
      },
      "outputs": [],
      "source": [
        "trainer.fit(lit_model, cifar10_data)\n",
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iab5_ZrqK7td"
      },
      "outputs": [],
      "source": [
        "sweep_config = {\n",
        "    'method': 'random',\n",
        "    'metric': {\n",
        "        'name': 'val/loss',\n",
        "        'goal': 'minimize'\n",
        "    },\n",
        "    'parameters': {\n",
        "        'nblocks': {\n",
        "            'values': [0, 1, 2]\n",
        "        },\n",
        "        'latent_dim': {\n",
        "            'values': [128, 256, 512]\n",
        "        },\n",
        "        'start_channels': {\n",
        "            'values': [32, 64, 128]\n",
        "        },\n",
        "        'lr': {\n",
        "            'min': 1e-5,\n",
        "            'max': 1e-2,\n",
        "            'distribution': 'uniform'\n",
        "        },\n",
        "        'beta1': {\n",
        "            'values': [0.9, 0.95, 0.99]\n",
        "        },\n",
        "        'beta2': {\n",
        "            'values': [0.999, 0.9999]\n",
        "        },\n",
        "        'kld_weight': {\n",
        "            'min': 0.0000025,\n",
        "            'max': 0.00025,\n",
        "            'distribution': 'uniform'\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "sweep_id = wandb.sweep(sweep_config, project=\"VAE CIFAR-10\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0BfB-TYDK-Ma"
      },
      "outputs": [],
      "source": [
        "def train():\n",
        "    with wandb.init() as run:\n",
        "        config = CIFAR10VAEConfig()\n",
        "        config.update(wandb.config)\n",
        "\n",
        "        model = VAE(config)\n",
        "        lit_model = LitVAE(model, config)\n",
        "        cifar10_data = CIFAR10DataModule(config)\n",
        "\n",
        "        wandb_logger = WandbLogger(project=\"VAE CIFAR-10\", log_model=False)\n",
        "\n",
        "        early_stop_callback = EarlyStopping(\n",
        "            monitor='val/loss',\n",
        "            min_delta=0.00,\n",
        "            patience=3,\n",
        "            verbose=True,\n",
        "            check_finite=True\n",
        "        )\n",
        "\n",
        "        trainer = pl.Trainer(\n",
        "            max_epochs=config.max_epochs,\n",
        "            devices=1,\n",
        "            accelerator=\"gpu\",\n",
        "            precision=\"16-mixed\",\n",
        "            logger=wandb_logger,\n",
        "            callbacks=[early_stop_callback]\n",
        "        )\n",
        "\n",
        "        trainer.fit(lit_model, cifar10_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cV_krC5nLB2W"
      },
      "outputs": [],
      "source": [
        "wandb.agent(sweep_id, train, count=5)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
