{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Inspired by:\n",
        "\n",
        "https://github.com/google-deepmind/sonnet/blob/v2/sonnet/src/nets/vqvae.py\n",
        "\n",
        "Config values from paper plus:\n",
        "\n",
        "https://github.com/google-deepmind/sonnet/blob/v2/examples/vqvae_example.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jjF85_2XJ1f6"
      },
      "outputs": [],
      "source": [
        "!pip install -q wandb pytorch_lightning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t11yJ4V8J6qr"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "import math\n",
        "import torch\n",
        "import torch.utils.data\n",
        "import torchvision\n",
        "import wandb\n",
        "import pytorch_lightning as pl\n",
        "\n",
        "from __future__ import print_function\n",
        "from collections import OrderedDict\n",
        "from torch import nn, optim\n",
        "from torch.nn import functional as F\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.utils import save_image\n",
        "from pytorch_lightning.loggers import WandbLogger\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint, LearningRateMonitor, EarlyStopping\n",
        "from pytorch_lightning.tuner import Tuner\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR, OneCycleLR\n",
        "\n",
        "\n",
        "pl.seed_everything(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G5xTdpkbJ-ls"
      },
      "outputs": [],
      "source": [
        "class CIFAR10DataModule(pl.LightningDataModule):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.batch_size = config.batch_size\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "        ])\n",
        "\n",
        "    def prepare_data(self):\n",
        "        torchvision.datasets.CIFAR10(root='./data', train=True, download=True)\n",
        "        torchvision.datasets.CIFAR10(root='./data', train=False, download=True)\n",
        "\n",
        "    def setup(self, stage=None):\n",
        "        if stage in ('fit', None):\n",
        "            self.cifar10_train = torchvision.datasets.CIFAR10(\n",
        "                root='./data', train=True, transform=self.transform)\n",
        "            self.cifar10_val = torchvision.datasets.CIFAR10(\n",
        "                root='./data', train=False, transform=self.transform)\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return torch.utils.data.DataLoader(\n",
        "            self.cifar10_train,\n",
        "            batch_size=self.batch_size,\n",
        "            shuffle=True,\n",
        "            num_workers=2, # TODO: check if we need more workers\n",
        "            pin_memory=True\n",
        "        )\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return torch.utils.data.DataLoader(\n",
        "            self.cifar10_val,\n",
        "            batch_size=self.batch_size,\n",
        "            shuffle=False,\n",
        "            num_workers=2, # TODO: check if we need more workers\n",
        "            pin_memory=True\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L8FV0QSNKCEL"
      },
      "outputs": [],
      "source": [
        "class ResBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        '''\n",
        "        \"[...] followed by two residual 3 × 3 blocks (implemented as ReLU, 3x3 conv,\n",
        "        ReLU, 1x1 conv), all having 256 hidden units.\"\n",
        "        '''\n",
        "        super().__init__()\n",
        "        if in_channels != out_channels:\n",
        "            self.identity = nn.Conv2d(\n",
        "                in_channels=in_channels,\n",
        "                out_channels=out_channels,\n",
        "                kernel_size=1\n",
        "            )\n",
        "        else:\n",
        "            self.identity = nn.Identity()\n",
        "\n",
        "        self.block = nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_channels=in_channels,\n",
        "                out_channels=in_channels,\n",
        "                kernel_size=3,\n",
        "                padding=1\n",
        "            ),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(\n",
        "                in_channels=in_channels,\n",
        "                out_channels=out_channels,\n",
        "                kernel_size=1\n",
        "            )\n",
        "        )\n",
        "        self.res_act = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.block(x) + self.identity(x)\n",
        "        return self.res_act(out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WPRmSrppKFUh"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        '''\n",
        "        \"The encoder consists of 2 strided convolutional layers with stride 2 and\n",
        "        window size 4 × 4, followed by two residual 3 × 3 blocks (implemented as ReLU,\n",
        "        3x3 conv, ReLU, 1x1 conv), all having 256 hidden units.\"\n",
        "        '''\n",
        "        super().__init__()\n",
        "        self.strided_blocks = nn.Sequential(*[\n",
        "            nn.Sequential(\n",
        "                nn.Conv2d(\n",
        "                    in_channels=config.in_channels if i == 0 else config.hidden_channels,\n",
        "                    out_channels=config.hidden_channels,\n",
        "                    kernel_size=4,\n",
        "                    stride=2,\n",
        "                    padding=1\n",
        "                ),\n",
        "                nn.ReLU()\n",
        "            ) for i in range(config.nlayers)\n",
        "        ])\n",
        "\n",
        "        self.res_blocks = nn.Sequential(*[\n",
        "            ResBlock(\n",
        "                in_channels=config.hidden_channels,\n",
        "                out_channels=config.hidden_channels if i < config.nblocks-1 else config.latent_channels\n",
        "            ) for i in range(config.nblocks)\n",
        "        ])\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.strided_blocks(x)\n",
        "        x = self.res_blocks(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Quantizer(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.codebook = nn.Embedding(config.codebook_size, config.latent_dim)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        inputs = inputs.permute(0, 2, 3, 1).contiguous()\n",
        "        flat_inputs = inputs.reshape(-1, self.config.latent_dim)\n",
        "\n",
        "        # Σ(x-y)^2 = Σx^2 - 2xy + Σy^2\n",
        "        dists = (\n",
        "            torch.sum(flat_inputs ** 2, dim=1, keepdim=True) - # Σx^2\n",
        "            2 * (flat_inputs @ self.codebook.weight.t()) +     # 2*xy\n",
        "            torch.sum(self.codebook.weight ** 2, dim=1)        # Σy^2\n",
        "        )\n",
        "\n",
        "        enc_idxs = torch.argmin(dists, dim=1)\n",
        "        quantized_inputs = self.codebook(enc_idxs).reshape(inputs.shape)\n",
        "\n",
        "        # \"The VQ objective uses the l2 error to move the embedding vectors\n",
        "        # e_i towards the encoder outputs z_e(x)\"\n",
        "        embedding_loss = F.mse_loss(quantized_inputs, inputs.detach())\n",
        "        \n",
        "        # \"since the volume of the embedding space is dimensionless, it can grow\n",
        "        # arbitrarily if the embeddings e_i do not train as fast as the encoder\n",
        "        # parameters. To make sure the encoder commits to an embedding and its\n",
        "        # output does not grow, we add a commitment loss\"\n",
        "        commitment_loss = F.mse_loss(quantized_inputs.detach(), inputs)\n",
        "        \n",
        "        # parts 2 & 3 of full loss (ie. not including reconstruciton loss)\n",
        "        vq_loss = commitment_loss * self.config.embed_loss_beta + embedding_loss\n",
        "\n",
        "        # sets the output to be the input plus the residual value between the\n",
        "        # quantized latents and the inputs like a resnet for Straight Through\n",
        "        # Estimation (STE)\n",
        "        quantized_inputs = inputs + (quantized_inputs - inputs).detach()\n",
        "        quantized_inputs = quantized_inputs.permute(0, 3, 1, 2).contiguous()\n",
        "\n",
        "        return (quantized_inputs, vq_loss, embedding_loss, commitment_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "28MR93jRKHy9"
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        '''\n",
        "        \"The decoder similarly has two residual 3 × 3 blocks, followed by\n",
        "        two transposed convolutions with stride 2 and window size 4 × 4\"\n",
        "        '''\n",
        "        super().__init__()\n",
        "\n",
        "        self.res_blocks = nn.Sequential(*[\n",
        "            ResBlock(\n",
        "                in_channels=config.latent_channels if i==0 else config.hidden_channels,\n",
        "                out_channels=config.hidden_channels\n",
        "            ) for i in range(config.nblocks)\n",
        "        ])\n",
        "\n",
        "        self.transposed_blocks = nn.Sequential(*[\n",
        "            nn.Sequential(\n",
        "                nn.ConvTranspose2d(\n",
        "                    in_channels=config.hidden_channels,\n",
        "                    out_channels=config.hidden_channels,\n",
        "                    kernel_size=4,\n",
        "                    stride=2,\n",
        "                    padding=1\n",
        "                ),\n",
        "                nn.ReLU()\n",
        "            ) for _ in range(config.nlayers-1)\n",
        "        ])\n",
        "\n",
        "        self.out_layer = nn.ConvTranspose2d(\n",
        "            in_channels=config.hidden_channels,\n",
        "            out_channels=config.in_channels,\n",
        "            kernel_size=4,\n",
        "            stride=2,\n",
        "            padding=1\n",
        "        )\n",
        "\n",
        "    def forward(self, z_q):\n",
        "        out = self.res_blocks(z_q)\n",
        "        out = self.transposed_blocks(out)\n",
        "        out = self.out_layer(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1yawr70bKImu"
      },
      "outputs": [],
      "source": [
        "class VQVAE(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.encoder =   Encoder(config)\n",
        "        self.decoder =   Decoder(config)\n",
        "        self.quantizer = Quantizer(config)\n",
        "        self.config =    config\n",
        "\n",
        "    def encode(self, x):\n",
        "        return self.encoder(x)\n",
        "\n",
        "    def quantize(self, z):\n",
        "        return self.quantizer(z)\n",
        "\n",
        "    def decode(self, q_z):\n",
        "        return self.decoder(q_z)\n",
        "\n",
        "    def loss(self, x_hat, x, quantized):\n",
        "        MSE = F.mse_loss(x_hat, x)\n",
        "        loss = MSE + quantized['vq_loss']\n",
        "        \n",
        "        return {\n",
        "            'MSE':  MSE,\n",
        "            'loss': loss,\n",
        "            **quantized\n",
        "        }\n",
        "\n",
        "    def forward(self, x):\n",
        "        z = self.encode(x)\n",
        "        quantized = self.quantize(z)\n",
        "        x_hat = self.decode(quantized['quantized_inputs'])\n",
        "        losses = self.loss(x_hat, x, quantized)\n",
        "        \n",
        "        return {'x_hat': x_hat, **losses}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Lightning Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kN5eG3HnKLEF"
      },
      "outputs": [],
      "source": [
        "class LitVQVAE(pl.LightningModule):\n",
        "    def __init__(self, model, config):\n",
        "        super().__init__()\n",
        "        self.model = model\n",
        "        self.config = config\n",
        "        self.lr = config.lr\n",
        "\n",
        "        if self.logger:\n",
        "            self.logger.experiment.config.update(config)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        x, _ = batch\n",
        "        out = self(x)\n",
        "\n",
        "        self.log('train/loss',            out['loss'],            prog_bar=True)\n",
        "        self.log('train/MSE',             out['MSE'],             prog_bar=True)\n",
        "        self.log('train/vq_loss',         out['vq_loss'],         prog_bar=True)\n",
        "        self.log('train/embedding_loss',  out['embedding_loss'],  prog_bar=True)\n",
        "        self.log('train/commitment_loss', out['commitment_loss'], prog_bar=True)\n",
        "\n",
        "        return out['loss']\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        x, _ = batch\n",
        "        out = self(x)\n",
        "\n",
        "        self.log('val/loss',            out['loss'],            prog_bar=True)\n",
        "        self.log('val/MSE',             out['MSE'],             prog_bar=True)\n",
        "        self.log('val/vq_loss',         out['vq_loss'],         prog_bar=True)\n",
        "        self.log('val/embedding_loss',  out['embedding_loss'],  prog_bar=True)\n",
        "        self.log('val/commitment_loss', out['commitment_loss'], prog_bar=True)\n",
        "\n",
        "        if batch_idx == 0:\n",
        "            n_images = min(x.size(0), 8)\n",
        "            comparison = torch.cat([x[:n_images], out['x_hat'][:n_images]])\n",
        "            grid = torchvision.utils.make_grid(comparison)\n",
        "            self.logger.experiment.log({\"val/reconstructions\": [wandb.Image(grid, caption=\"Top: Original, Bottom: Reconstructed\")]})\n",
        "\n",
        "        return out['loss']\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.AdamW(\n",
        "            self.parameters(),\n",
        "            lr=self.lr,\n",
        "            betas=(self.config.beta1, self.config.beta2),\n",
        "            weight_decay=self.config.weight_decay\n",
        "        )\n",
        "        \n",
        "        if self.config.use_lr_schedule:\n",
        "            scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=self.config.max_epochs)\n",
        "            return [optimizer], [scheduler]\n",
        "        \n",
        "        return optimizer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x9uUkXe1KNf_"
      },
      "outputs": [],
      "source": [
        "def get_num_downsample_layers(img_size):\n",
        "    \"\"\"\n",
        "    Get the number of strided Conv2D layers\n",
        "    required to produce a 2x2 output volume\n",
        "    \"\"\"\n",
        "    if img_size < 2:\n",
        "        raise ValueError(\"Image size must be at least 2x2.\")\n",
        "\n",
        "    # Calculate the minimum number of downsample layers required for 2x2 final\n",
        "    num_layers = math.ceil(math.log2(img_size / 2))\n",
        "    return num_layers\n",
        "\n",
        "def build_channel_dims(start_channels, nlayers):\n",
        "    \"\"\"\n",
        "    Construct a list of channel counts for nlayers downsample layers\n",
        "    assuming the channels double as spatial dims halve\n",
        "    \"\"\"\n",
        "    channels = []\n",
        "    for _ in range(nlayers):\n",
        "        channels.append(start_channels)\n",
        "        start_channels *= 2\n",
        "    return channels\n",
        "\n",
        "class CIFAR10VQVAEConfig:\n",
        "    def __init__(self):\n",
        "        # model checkpoints\n",
        "        self.checkpoint_path = \"./checkpoints\"\n",
        "        self.save_top_k = 1\n",
        "        # training\n",
        "        self.batch_size = 128\n",
        "        self.max_epochs = 120\n",
        "        self.training_steps = 250000\n",
        "        self.num_workers = 2\n",
        "        # optimizer\n",
        "        self.lr = 2e-4\n",
        "        self.beta1 = 0.9\n",
        "        self.beta2 = 0.999\n",
        "        self.weight_decay = 0.0 # 1e-2\n",
        "        self.use_wd_schedule = False\n",
        "        self.use_lr_schedule = False\n",
        "        # input properties\n",
        "        self.img_size = 32\n",
        "        self.in_channels = 3\n",
        "        # latents / quantization\n",
        "        self.latent_channels = 64\n",
        "        self.codebook_size = 512\n",
        "        self.embed_loss_beta = 0.25\n",
        "        # encoder/decoder\n",
        "        self.hidden_channels = 128\n",
        "        self.nblocks = 2\n",
        "        self.nlayers = 2\n",
        "\n",
        "    def update(self, updates):\n",
        "        for key, value in updates.items():\n",
        "            if hasattr(self, key):\n",
        "                setattr(self, key, value)\n",
        "        self.nlayers = get_num_downsample_layers(self.img_size)\n",
        "        self.channel_dims = build_channel_dims(self.start_channels, self.nlayers)\n",
        "\n",
        "    def to_dict(self):\n",
        "        return {k: v for k, v in self.__dict__.items()}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# COMPONENT TESTING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
        "dataloader = torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=False, num_workers=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "img, label = next(iter(dataloader))\n",
        "img.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "config = CIFAR10VQVAEConfig()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "encoder = Encoder(config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "z = encoder(img)\n",
        "print(z.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "quantizer = Quantizer(config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "z_q, vq_loss, embedding_loss, commitment_loss = quantizer(z)\n",
        "print(z_q.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "decoder = Decoder(config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "x_hat = decoder(z_q)\n",
        "print(x_hat.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "vqvae = VQVAE(config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "vqvae(img)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Oah3ySaKSQk"
      },
      "outputs": [],
      "source": [
        "config = CIFAR10VQVAEConfig()\n",
        "model = VQVAE(config)\n",
        "lit_model = LitVQVAE(model, config)\n",
        "cifar10_data = CIFAR10DataModule(config)\n",
        "\n",
        "wandb.init(project=\"VQ-VAE CIFAR-10\", config=config.to_dict())\n",
        "wandb_logger = WandbLogger(project=\"VQ-VAE CIFAR-10\", log_model=False)\n",
        "wandb_logger.watch(lit_model, log=\"all\")\n",
        "\n",
        "lr_monitor = LearningRateMonitor(logging_interval='step')\n",
        "\n",
        "checkpoint_callback = ModelCheckpoint(\n",
        "    dirpath=config.checkpoint_path,\n",
        "    filename='model-{epoch:02d}-{val_loss:.2f}',\n",
        "    every_n_epochs=5,\n",
        "    save_top_k=config.save_top_k,\n",
        "    monitor='val/loss',\n",
        "    mode='min',\n",
        "    save_last=True\n",
        ")\n",
        "\n",
        "# Define the EarlyStopping callback\n",
        "early_stop_callback = EarlyStopping(\n",
        "    monitor='val/loss',\n",
        "    min_delta=0.00,\n",
        "    patience=3,\n",
        "    verbose=True,\n",
        "    check_finite=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dFM3LWghKUOv"
      },
      "outputs": [],
      "source": [
        "#wandb.finish()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eWPlQ1PNK4bs"
      },
      "outputs": [],
      "source": [
        "trainer = pl.Trainer(\n",
        "    max_epochs=config.max_epochs,\n",
        "    devices=1,\n",
        "    accelerator=\"gpu\",\n",
        "    precision=\"16-mixed\",\n",
        "    logger=wandb_logger,\n",
        "    callbacks=[\n",
        "        lr_monitor,\n",
        "        early_stop_callback,\n",
        "        # checkpoint_callback\n",
        "    ],\n",
        "    log_every_n_steps=1,\n",
        "    # overfit_batches=1,\n",
        ")\n",
        "\n",
        "# tuner = Tuner(trainer)\n",
        "# tuner.lr_find(lit_model, datamodule=cifar10_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3-swnb5PK5rK"
      },
      "outputs": [],
      "source": [
        "trainer.fit(lit_model, cifar10_data)\n",
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Sweeps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iab5_ZrqK7td"
      },
      "outputs": [],
      "source": [
        "sweep_config = {\n",
        "    'method': 'random',\n",
        "    'metric': {\n",
        "        'name': 'val/loss',\n",
        "        'goal': 'minimize'\n",
        "    },\n",
        "    'parameters': {\n",
        "        'nblocks': {\n",
        "            'values': [0, 1, 2]\n",
        "        },\n",
        "        'latent_dim': {\n",
        "            'values': [128, 256, 512]\n",
        "        },\n",
        "        'start_channels': {\n",
        "            'values': [32, 64, 128]\n",
        "        },\n",
        "        'lr': {\n",
        "            'min': 1e-5,\n",
        "            'max': 1e-2,\n",
        "            'distribution': 'uniform'\n",
        "        },\n",
        "        'beta1': {\n",
        "            'values': [0.9, 0.95, 0.99]\n",
        "        },\n",
        "        'beta2': {\n",
        "            'values': [0.999, 0.9999]\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "sweep_id = wandb.sweep(sweep_config, project=\"VQVAE CIFAR-10\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0BfB-TYDK-Ma"
      },
      "outputs": [],
      "source": [
        "def train():\n",
        "    with wandb.init() as run:\n",
        "        config = CIFAR10VQVAEConfig()\n",
        "        config.update(wandb.config)\n",
        "\n",
        "        model = VQVAE(config)\n",
        "        lit_model = LitVQVAE(model, config)\n",
        "        cifar10_data = CIFAR10DataModule(config)\n",
        "\n",
        "        wandb_logger = WandbLogger(project=\"VQVAE CIFAR-10\", log_model=False)\n",
        "\n",
        "        early_stop_callback = EarlyStopping(\n",
        "            monitor='val/loss',\n",
        "            min_delta=0.00,\n",
        "            patience=3,\n",
        "            verbose=True,\n",
        "            check_finite=True\n",
        "        )\n",
        "\n",
        "        trainer = pl.Trainer(\n",
        "            max_epochs=config.max_epochs,\n",
        "            devices=1,\n",
        "            accelerator=\"gpu\",\n",
        "            precision=\"16-mixed\",\n",
        "            logger=wandb_logger,\n",
        "            callbacks=[early_stop_callback]\n",
        "        )\n",
        "\n",
        "        trainer.fit(lit_model, cifar10_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cV_krC5nLB2W"
      },
      "outputs": [],
      "source": [
        "wandb.agent(sweep_id, train, count=5)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
