{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y178Rjq-7R1B"
      },
      "source": [
        "Inspired by:\n",
        "\n",
        "https://github.com/google-deepmind/sonnet/blob/v2/sonnet/src/nets/vqvae.py\n",
        "\n",
        "Config values from paper plus:\n",
        "\n",
        "https://github.com/google-deepmind/sonnet/blob/v2/examples/vqvae_example.ipynb\n",
        "\n",
        "TODO:\n",
        "- [ ] sample / interpolate latent space\n",
        "- [X] try training without weight decay\n",
        "- [ ] VQ-VAE 2 hierarchical encoding / decoding\n",
        "- [X] EMA VQ Loss\n",
        "- [ ] try to identify the optimal learning rate (is this more complicated due to training dynamics?)\n",
        "- [X] try varifying that the codebook is initialized correctly\n",
        "- [X] try to understand and log codebook usage and other important model dinamics\n",
        "- [ ] try training with lr warmup\n",
        "- [ ] try training with 1 cycle + warmup  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jjF85_2XJ1f6",
        "outputId": "7239ac3b-c692-45cb-fd80-12d786baf10c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m777.7/777.7 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.4/196.4 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m254.1/254.1 kB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m840.2/840.2 kB\u001b[0m \u001b[31m31.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q wandb pytorch_lightning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t11yJ4V8J6qr",
        "outputId": "af9618a8-92a9-47be-ca00-a627c50fc762"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:lightning_fabric.utilities.seed:Seed set to 42\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "42"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import argparse\n",
        "import math\n",
        "import torch\n",
        "import torch.utils.data\n",
        "import torchvision\n",
        "import wandb\n",
        "import pytorch_lightning as pl\n",
        "import torch.nn.init as init\n",
        "\n",
        "from __future__ import print_function\n",
        "from collections import OrderedDict\n",
        "from torch import nn, optim\n",
        "from torch.nn import functional as F\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.utils import save_image\n",
        "from pytorch_lightning.loggers import WandbLogger\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint, LearningRateMonitor, EarlyStopping\n",
        "from pytorch_lightning.tuner import Tuner\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR, OneCycleLR\n",
        "\n",
        "\n",
        "pl.seed_everything(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7phHAga3Or6w"
      },
      "source": [
        "# Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "G5xTdpkbJ-ls"
      },
      "outputs": [],
      "source": [
        "class CIFAR10DataModule(pl.LightningDataModule):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "        ])\n",
        "\n",
        "    def prepare_data(self):\n",
        "        torchvision.datasets.CIFAR10(root='./data', train=True, download=True)\n",
        "        torchvision.datasets.CIFAR10(root='./data', train=False, download=True)\n",
        "\n",
        "    def setup(self, stage=None):\n",
        "        if stage in ('fit', None):\n",
        "            self.cifar10_train = torchvision.datasets.CIFAR10(\n",
        "                root='./data', train=True, transform=self.transform)\n",
        "            self.cifar10_val = torchvision.datasets.CIFAR10(\n",
        "                root='./data', train=False, transform=self.transform)\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return torch.utils.data.DataLoader(\n",
        "            self.cifar10_train,\n",
        "            batch_size=self.config.batch_size,\n",
        "            shuffle=True,\n",
        "            num_workers=self.config.num_workers, # TODO: check if we need more workers\n",
        "            pin_memory=True\n",
        "        )\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return torch.utils.data.DataLoader(\n",
        "            self.cifar10_val,\n",
        "            batch_size=self.config.batch_size,\n",
        "            shuffle=False,\n",
        "            num_workers=self.config.num_workers, # TODO: check if we need more workers\n",
        "            pin_memory=True\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iXaJCQ8yOxNT"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L8FV0QSNKCEL"
      },
      "outputs": [],
      "source": [
        "class ResBlock(nn.Module):\n",
        "    def __init__(\n",
        "            self,\n",
        "            in_channels,\n",
        "            out_channels\n",
        "        ):\n",
        "        '''\n",
        "        \"[...] followed by two residual 3 × 3 blocks (implemented as ReLU, 3x3 conv,\n",
        "        ReLU, 1x1 conv), all having 256 hidden units.\"\n",
        "        '''\n",
        "        super().__init__()\n",
        "        if in_channels != out_channels:\n",
        "            self.identity = nn.Conv2d(\n",
        "                in_channels=in_channels,\n",
        "                out_channels=out_channels,\n",
        "                kernel_size=1\n",
        "            )\n",
        "        else:\n",
        "            self.identity = nn.Identity()\n",
        "\n",
        "        self.block = nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_channels=in_channels,\n",
        "                out_channels=in_channels,\n",
        "                kernel_size=3,\n",
        "                padding=1\n",
        "            ),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(in_channels),\n",
        "            nn.Conv2d(\n",
        "                in_channels=in_channels,\n",
        "                out_channels=out_channels,\n",
        "                kernel_size=1\n",
        "            ),\n",
        "            nn.BatchNorm2d(out_channels)\n",
        "        )\n",
        "        self.res_act = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.block(x) + self.identity(x)\n",
        "        return self.res_act(out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WPRmSrppKFUh"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(\n",
        "            self,\n",
        "            in_channels,\n",
        "            hidden_channels,\n",
        "            out_channels,\n",
        "            nlayers,\n",
        "            nblocks\n",
        "        ):\n",
        "        '''\n",
        "        \"For 256 × 256 images, we use a two level latent hierarchy. [...] the encoder\n",
        "        network first transforms and downsamples the image by a factor of 4 to a 64 × 64 representation\n",
        "        which is quantized to our bottom level latent map. Another stack of residual blocks then further\n",
        "        scales down the representations by a factor of two, yielding a top-level 32 × 32 latent map after\n",
        "        quantization\"\n",
        "        '''\n",
        "        super().__init__()\n",
        "        self.strided_blocks = nn.Sequential(*[\n",
        "            nn.Sequential(\n",
        "                nn.Conv2d(\n",
        "                    in_channels=in_channels if i == 0 else hidden_channels,\n",
        "                    out_channels=hidden_channels,\n",
        "                    kernel_size=4,\n",
        "                    stride=2,\n",
        "                    padding=1\n",
        "                ),\n",
        "                nn.BatchNorm2d(hidden_channels),\n",
        "                nn.ReLU()\n",
        "            ) for i in range(nlayers)\n",
        "        ])\n",
        "\n",
        "        self.res_blocks = nn.Sequential(*[\n",
        "            ResBlock(\n",
        "                in_channels=hidden_channels,\n",
        "                out_channels=hidden_channels if i < nblocks-1 else out_channels\n",
        "            ) for i in range(nblocks)\n",
        "        ])\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.strided_blocks(x)\n",
        "        x = self.res_blocks(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nJQXezyYLP08"
      },
      "outputs": [],
      "source": [
        "class Quantizer(nn.Module):\n",
        "    def __init__(\n",
        "            self,\n",
        "            codebook_size,\n",
        "            latent_channels,\n",
        "            commit_loss_beta,\n",
        "            track_codebook\n",
        "        ):\n",
        "        super().__init__()\n",
        "        self.codebook_size = codebook_size\n",
        "        self.latent_channels = latent_channels\n",
        "        self.commit_loss_beta = commit_loss_beta\n",
        "        self.track_codebook = track_codebook\n",
        "\n",
        "        self.codebook = nn.Embedding(codebook_size, latent_channels)\n",
        "        init.uniform_(self.codebook.weight, -1/codebook_size, 1/codebook_size)\n",
        "\n",
        "        if track_codebook:\n",
        "            self.register_buffer('codebook_usage', torch.zeros(codebook_size, dtype=torch.float))\n",
        "            self.register_buffer('total_usage', torch.tensor(0, dtype=torch.float))\n",
        "\n",
        "    def reset_usage_stats(self):\n",
        "        self.codebook_usage.zero_()\n",
        "        self.total_usage.zero_()\n",
        "\n",
        "    def calculate_perplexity(self, enc_idxs):\n",
        "        unique_indices, counts = torch.unique(enc_idxs, return_counts=True)\n",
        "        self.codebook_usage.index_add_(0, unique_indices, counts.float())\n",
        "        self.total_usage += torch.sum(counts)\n",
        "\n",
        "        if self.total_usage > 0:\n",
        "            probs = self.codebook_usage / self.total_usage\n",
        "            perplexity = torch.exp(-torch.sum(probs * torch.log(probs + 1e-10)))\n",
        "            return perplexity\n",
        "        else:\n",
        "            return torch.tensor([0.0])\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        inputs = inputs.permute(0, 2, 3, 1).contiguous()\n",
        "        flat_inputs = inputs.reshape(-1, self.latent_channels)\n",
        "\n",
        "        # Σ(x-y)^2 = Σx^2 - 2xy + Σy^2\n",
        "        dists = (\n",
        "            torch.sum(flat_inputs ** 2, dim=1, keepdim=True) - # Σx^2\n",
        "            2 * (flat_inputs @ self.codebook.weight.t()) +     # 2*xy\n",
        "            torch.sum(self.codebook.weight ** 2, dim=1)        # Σy^2\n",
        "        )\n",
        "\n",
        "        code_idxs = torch.argmin(dists, dim=1)\n",
        "        quantized_inputs = self.codebook(code_idxs).reshape(inputs.shape)\n",
        "\n",
        "        # \"The VQ objective uses the l2 error to move the embedding vectors\n",
        "        # e_i towards the encoder outputs z_e(x)\"\n",
        "        embedding_loss = F.mse_loss(quantized_inputs, inputs.detach())\n",
        "\n",
        "        # \"since the volume of the embedding space is dimensionless, it can grow\n",
        "        # arbitrarily if the embeddings e_i do not train as fast as the encoder\n",
        "        # parameters. To make sure the encoder commits to an embedding and its\n",
        "        # output does not grow, we add a commitment loss\"\n",
        "        commitment_loss = F.mse_loss(quantized_inputs.detach(), inputs)\n",
        "\n",
        "        # parts 2 & 3 of full loss (ie. not including reconstruciton loss)\n",
        "        vq_loss = commitment_loss * self.config.embed_loss_beta + embedding_loss\n",
        "\n",
        "        # sets the output to be the input plus the residual value between the\n",
        "        # quantized latents and the inputs like a resnet for Straight Through\n",
        "        # Estimation (STE)\n",
        "        quantized_inputs = inputs + (quantized_inputs - inputs).detach()\n",
        "        quantized_inputs = quantized_inputs.permute(0, 3, 1, 2).contiguous()\n",
        "\n",
        "        if self.track_codebook:\n",
        "            perplexity = self.calculate_perplexity(code_idxs)\n",
        "\n",
        "        return {\n",
        "            'quantized_inputs': quantized_inputs,\n",
        "            'vq_loss':          vq_loss,\n",
        "            'embedding_loss':   embedding_loss,\n",
        "            'commitment_loss':  commitment_loss,\n",
        "            'perplexity':       perplexity if self.track_codebook else torch.tensor([0.0])\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wY0t2U17Zwtx"
      },
      "outputs": [],
      "source": [
        "class QuantizerEMA(nn.Module):\n",
        "    def __init__(\n",
        "            self,\n",
        "            codebook_size,\n",
        "            latent_channels,\n",
        "            ema_gamma,\n",
        "            commit_loss_beta,\n",
        "            track_codebook\n",
        "        ):\n",
        "        super().__init__()\n",
        "        self.codebook_size = codebook_size\n",
        "        self.latent_channels = latent_channels\n",
        "        self.ema_gamma = ema_gamma\n",
        "        self.commit_loss_beta = commit_loss_beta\n",
        "        self.track_codebook = track_codebook\n",
        "\n",
        "        self.codebook = nn.Embedding(codebook_size, latent_channels)\n",
        "        init.uniform_(self.codebook.weight, -1/codebook_size, 1/codebook_size)\n",
        "\n",
        "        self.register_buffer('N', torch.zeros(codebook_size) + 1e-6)\n",
        "        self.register_buffer('m', torch.zeros(codebook_size, latent_channels))\n",
        "        init.uniform_(self.m, -1/codebook_size, 1/codebook_size)\n",
        "\n",
        "        if track_codebook:\n",
        "            self.register_buffer('codebook_usage', torch.zeros(codebook_size, dtype=torch.float))\n",
        "            self.register_buffer('total_usage', torch.tensor(0, dtype=torch.float))\n",
        "\n",
        "    def ema_update(self, code_idxs, flat_inputs):\n",
        "        # we don't want to track grads for ops in EMA calculation\n",
        "        code_idxs, flat_inputs = code_idxs.detach(), flat_inputs.detach()\n",
        "\n",
        "        # number of vectors for each i which quantize to e_i\n",
        "        n = torch.bincount(code_idxs, minlength=self.codebook_size)\n",
        "\n",
        "        # sum of vectors for each i which quantize to code e_i\n",
        "        one_hot_indices = F.one_hot(code_idxs, num_classes=self.codebook_size).type(flat_inputs.dtype)\n",
        "        embed_sums = one_hot_indices.T @ flat_inputs\n",
        "\n",
        "        # update EMA of code usage and sum of codes\n",
        "        self.N = self.N * self.ema_gamma + n * (1 - self.ema_gamma)\n",
        "        self.m = self.m * self.ema_gamma + embed_sums * (1 - self.ema_gamma)\n",
        "\n",
        "        self.codebook.weight.data.copy_(self.m / self.N.unsqueeze(-1))\n",
        "\n",
        "    def reset_usage_stats(self):\n",
        "        self.codebook_usage.zero_()\n",
        "        self.total_usage.zero_()\n",
        "\n",
        "    def calculate_perplexity(self, enc_idxs):\n",
        "        unique_indices, counts = torch.unique(enc_idxs, return_counts=True)\n",
        "        self.codebook_usage.index_add_(0, unique_indices, counts.float())\n",
        "        self.total_usage += torch.sum(counts)\n",
        "\n",
        "        if self.total_usage > 0:\n",
        "            probs = self.codebook_usage / self.total_usage\n",
        "            perplexity = torch.exp(-torch.sum(probs * torch.log(probs + 1e-10)))\n",
        "            return perplexity\n",
        "        else:\n",
        "            return torch.tensor([0.0])\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        inputs = inputs.permute(0, 2, 3, 1).contiguous()\n",
        "        flat_inputs = inputs.reshape(-1, self.latent_channels)\n",
        "\n",
        "        # Σ(x-y)^2 = Σx^2 - 2xy + Σy^2\n",
        "        dists = (\n",
        "            torch.sum(flat_inputs ** 2, dim=1, keepdim=True) - # Σx^2\n",
        "            2 * (flat_inputs @ self.codebook.weight.t()) +     # 2*xy\n",
        "            torch.sum(self.codebook.weight ** 2, dim=1)        # Σy^2\n",
        "        )\n",
        "\n",
        "        code_idxs = torch.argmin(dists, dim=1)\n",
        "        quantized_inputs = self.codebook(code_idxs).reshape(inputs.shape)\n",
        "\n",
        "        if self.training:\n",
        "            # perform exponential moving average update for codebook\n",
        "            self.ema_update(code_idxs, flat_inputs)\n",
        "\n",
        "        # \"since the volume of the embedding space is dimensionless, it can grow\n",
        "        # arbitrarily if the embeddings e_i do not train as fast as the encoder\n",
        "        # parameters. To make sure the encoder commits to an embedding and its\n",
        "        # output does not grow, we add a commitment loss\"\n",
        "        commitment_loss = F.mse_loss(quantized_inputs.detach(), inputs)\n",
        "\n",
        "        # parts 2 & 3 of full loss (ie. not including reconstruciton loss)\n",
        "        vq_loss = commitment_loss * self.commit_loss_beta\n",
        "\n",
        "        # sets the output to be the input plus the residual value between the\n",
        "        # quantized latents and the inputs like a resnet for Straight Through\n",
        "        # Estimation (STE)\n",
        "        quantized_inputs = inputs + (quantized_inputs - inputs).detach()\n",
        "        quantized_inputs = quantized_inputs.permute(0, 3, 1, 2).contiguous()\n",
        "\n",
        "        if self.track_codebook:\n",
        "            perplexity = self.calculate_perplexity(code_idxs)\n",
        "\n",
        "        return {\n",
        "            'quantized_inputs': quantized_inputs,\n",
        "            'vq_loss':          vq_loss,\n",
        "            'commitment_loss':  commitment_loss,\n",
        "            'perplexity':       perplexity if self.track_codebook else torch.tensor([0.0])\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "28MR93jRKHy9"
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(\n",
        "            self,\n",
        "            in_channels,\n",
        "            hidden_channels,\n",
        "            out_channels,\n",
        "            nblocks,\n",
        "            nlayers\n",
        "        ):\n",
        "        '''\n",
        "        \"The decoder similarly has two residual 3 × 3 blocks, followed by\n",
        "        two transposed convolutions with stride 2 and window size 4 × 4\"\n",
        "        '''\n",
        "        super().__init__()\n",
        "        self.res_blocks = nn.Sequential(*[\n",
        "            ResBlock(\n",
        "                in_channels=in_channels if i==0 else hidden_channels,\n",
        "                out_channels=hidden_channels\n",
        "            ) for i in range(nblocks)\n",
        "        ])\n",
        "\n",
        "        self.transposed_blocks = nn.Sequential(*[\n",
        "            nn.Sequential(\n",
        "                nn.ConvTranspose2d(\n",
        "                    in_channels=hidden_channels,\n",
        "                    out_channels=hidden_channels,\n",
        "                    kernel_size=4,\n",
        "                    stride=2,\n",
        "                    padding=1\n",
        "                ),\n",
        "                nn.BatchNorm2d(hidden_channels),\n",
        "                nn.ReLU()\n",
        "            ) for _ in range(nlayers-1)\n",
        "        ])\n",
        "\n",
        "        self.out_layer = nn.ConvTranspose2d(\n",
        "            in_channels=hidden_channels,\n",
        "            out_channels=out_channels,\n",
        "            kernel_size=4,\n",
        "            stride=2,\n",
        "            padding=1\n",
        "        )\n",
        "\n",
        "    def forward(self, z_q):\n",
        "        out = self.res_blocks(z_q)\n",
        "        out = self.transposed_blocks(out)\n",
        "        out = self.out_layer(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1yawr70bKImu"
      },
      "outputs": [],
      "source": [
        "class VQVAE2(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.bottom_encoder = Encoder(\n",
        "            in_channels     = config.in_channels,\n",
        "            hidden_channels = config.hidden_channels,\n",
        "            out_channels    = config.latent_channels, # bottom codebook latent_channels\n",
        "            nlayers         = config.nlayers,\n",
        "            nblocks         = config.nblocks\n",
        "        )\n",
        "        self.top_encoder = Encoder(\n",
        "            in_channels     = config.latent_channels,\n",
        "            hidden_channels = config.hidden_channels,\n",
        "            out_channels    = config.latent_channels, # top codebook latent_channels\n",
        "            nlayers         = config.nlayers,\n",
        "            nblocks         = config.nblocks\n",
        "        )\n",
        "        \n",
        "        self.top_decoder = Decoder(\n",
        "            in_channels     = config.latent_channels, # top codebook latent_channels\n",
        "            hidden_channels = config.hidden_channels,\n",
        "            out_channels    = config.latent_channels,\n",
        "            nlayers         = config.nlayers,\n",
        "            nblocks         = config.nblocks\n",
        "        )\n",
        "        self.bottom_decoder = Decoder(\n",
        "            in_channels     = config.latent_channels * 2, # top codebook latent_channels\n",
        "            hidden_channels = config.hidden_channels,\n",
        "            out_channels    = config.in_channels,\n",
        "            nlayers         = config.nlayers,\n",
        "            nblocks         = config.nblocks\n",
        "        )\n",
        "        \n",
        "        if config.use_ema:\n",
        "            self.top_quantizer = QuantizerEMA(\n",
        "                codebook_size    = config.codebook_size,\n",
        "                latent_channels  = config.latent_channels,\n",
        "                ema_gamma        = config.ema_gamma,\n",
        "                commit_loss_beta = config.commit_loss_beta,\n",
        "                track_codebook   = config.track_codebook\n",
        "            )\n",
        "            self.bottom_quantizer = QuantizerEMA(\n",
        "                codebook_size    = config.codebook_size,\n",
        "                latent_channels  = config.latent_channels * 2, # we concat two bottom z's together\n",
        "                ema_gamma        = config.ema_gamma,\n",
        "                commit_loss_beta = config.commit_loss_beta,\n",
        "                track_codebook   = config.track_codebook\n",
        "            )\n",
        "        else:\n",
        "            self.top_quantizer = Quantizer(\n",
        "                codebook_size    = config.codebook_size,\n",
        "                latent_channels  = config.latent_channels,\n",
        "                commit_loss_beta = config.commit_loss_beta,\n",
        "                track_codebook   = config.track_codebook\n",
        "            )\n",
        "            self.bottom_quantizer = Quantizer(\n",
        "                codebook_size    = config.codebook_size,\n",
        "                latent_channels  = config.latent_channels * 2, # we concat two bottom z's together\n",
        "                commit_loss_beta = config.commit_loss_beta,\n",
        "                track_codebook   = config.track_codebook\n",
        "            )\n",
        "        \n",
        "        self.config = config\n",
        "\n",
        "    def loss(self, x_hat, x, quantized):\n",
        "        MSE = F.mse_loss(x_hat, x)\n",
        "        loss = MSE + quantized['bottom_quantized']['vq_loss'] + quantized['top_quantized']['vq_loss']\n",
        "\n",
        "        return {\n",
        "            'MSE':  MSE,\n",
        "            'loss': loss,\n",
        "            **quantized\n",
        "        }\n",
        "\n",
        "    def forward(self, x):\n",
        "        bottom_z = self.bottom_encoder(x)\n",
        "        top_z = self.top_encoder(bottom_z)\n",
        "        \n",
        "        top_quantized = self.top_quantizer(top_z)\n",
        "        bottom_z_hat = self.top_decoder(top_quantized['quantized_inputs'])\n",
        "        \n",
        "        bottom_z_cat = torch.cat([bottom_z, bottom_z_hat], dim=1)\n",
        "        bottom_quantized = self.bottom_quantizer(bottom_z_cat)\n",
        "\n",
        "        x_hat = self.bottom_decoder(bottom_quantized['quantized_inputs'])\n",
        "        losses = self.loss(x_hat, x, {'bottom_quantized': bottom_quantized, 'top_quantized': top_quantized})\n",
        "\n",
        "        return {'x_hat': x_hat, **losses}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1vhADlRoO0sR"
      },
      "source": [
        "# Lightning Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kN5eG3HnKLEF"
      },
      "outputs": [],
      "source": [
        "class LitVQVAE2(pl.LightningModule):\n",
        "    def __init__(self, model, config):\n",
        "        super().__init__()\n",
        "        self.model = model\n",
        "        self.config = config\n",
        "        self.lr = config.lr\n",
        "\n",
        "        if self.logger:\n",
        "            self.logger.experiment.config.update(config)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        x, _ = batch\n",
        "        out = self(x)\n",
        "\n",
        "        self.log('train/loss', out['loss'], prog_bar=True)\n",
        "        self.log('train/MSE',  out['MSE'],  prog_bar=True)\n",
        "        \n",
        "        self.log('train/top/vq_loss',    out['top_quantized']['vq_loss'],    prog_bar=True)\n",
        "        self.log('train/bottom/vq_loss', out['bottom_quantized']['vq_loss'], prog_bar=True)\n",
        "        \n",
        "        self.log('train/top/commitment_loss',    out['top_quantized']['commitment_loss'],    prog_bar=True)\n",
        "        self.log('train/bottom/commitment_loss', out['bottom_quantized']['commitment_loss'], prog_bar=True)\n",
        "\n",
        "        if not self.config.use_ema:\n",
        "            self.log('train/top/embedding_loss',    out['embedding_loss'], prog_bar=True)\n",
        "            self.log('train/bottom/embedding_loss', out['embedding_loss'], prog_bar=True)\n",
        "\n",
        "        if self.config.track_codebook:\n",
        "            self.log('train/top/perplexity',    out['top_quantized']['perplexity'],    prog_bar=True)\n",
        "            self.log('train/bottom/perplexity', out['bottom_quantized']['perplexity'], prog_bar=True)\n",
        "\n",
        "        return out['loss']\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        x, _ = batch\n",
        "        out = self(x)\n",
        "\n",
        "        self.log('val/loss', out['loss'], prog_bar=True)\n",
        "        self.log('val/MSE',  out['MSE'],  prog_bar=True)\n",
        "        \n",
        "        self.log('val/top/vq_loss',    out['top_quantized']['vq_loss'],    prog_bar=True)\n",
        "        self.log('val/bottom/vq_loss', out['bottom_quantized']['vq_loss'], prog_bar=True)\n",
        "        \n",
        "        self.log('val/top/commitment_loss',    out['top_quantized']['commitment_loss'],    prog_bar=True)\n",
        "        self.log('val/bottom/commitment_loss', out['bottom_quantized']['commitment_loss'], prog_bar=True)\n",
        "\n",
        "        if not self.config.use_ema:\n",
        "            self.log('val/top/embedding_loss',    out['top_quantized']['embedding_loss'],    prog_bar=True)\n",
        "            self.log('val/bottom/embedding_loss', out['bottom_quantized']['embedding_loss'], prog_bar=True)\n",
        "\n",
        "        if self.config.track_codebook:\n",
        "            self.log('val/top/perplexity',    out['top_quantized']['perplexity'],    prog_bar=True)\n",
        "            self.log('val/bottom/perplexity', out['bottom_quantized']['perplexity'], prog_bar=True)\n",
        "\n",
        "        if batch_idx == 0:\n",
        "            n_images = min(x.size(0), 8)\n",
        "            comparison = torch.cat([x[:n_images], out['x_hat'][:n_images]])\n",
        "            grid = torchvision.utils.make_grid(comparison)\n",
        "            self.logger.experiment.log({\"val/reconstructions\": [wandb.Image(grid, caption=\"Top: Original, Bottom: Reconstructed\")]})\n",
        "\n",
        "        return out['loss']\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        # tracking perplexity per epoch\n",
        "        self.model.top_quantizer.reset_usage_stats()\n",
        "        self.model.bottom_quantizer.reset_usage_stats()\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.AdamW(\n",
        "            self.parameters(),\n",
        "            lr=self.lr,\n",
        "            betas=(self.config.beta1, self.config.beta2),\n",
        "            weight_decay=self.config.weight_decay\n",
        "        )\n",
        "\n",
        "        if self.config.use_lr_schedule:\n",
        "            scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=self.config.max_epochs)\n",
        "            return [optimizer], [scheduler]\n",
        "\n",
        "        return optimizer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gfu2yUW2O6SN"
      },
      "source": [
        "# Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x9uUkXe1KNf_"
      },
      "outputs": [],
      "source": [
        "def get_num_downsample_layers(img_size):\n",
        "    \"\"\"\n",
        "    Get the number of strided Conv2D layers\n",
        "    required to produce a 2x2 output volume\n",
        "    \"\"\"\n",
        "    if img_size < 2:\n",
        "        raise ValueError(\"Image size must be at least 2x2.\")\n",
        "\n",
        "    # Calculate the minimum number of downsample layers required for 2x2 final\n",
        "    num_layers = math.ceil(math.log2(img_size / 2))\n",
        "    return num_layers\n",
        "\n",
        "def build_channel_dims(start_channels, nlayers):\n",
        "    \"\"\"\n",
        "    Construct a list of channel counts for nlayers downsample layers\n",
        "    assuming the channels double as spatial dims halve\n",
        "    \"\"\"\n",
        "    channels = []\n",
        "    for _ in range(nlayers):\n",
        "        channels.append(start_channels)\n",
        "        start_channels *= 2\n",
        "    return channels\n",
        "\n",
        "class CIFAR10VQVAE2Config:\n",
        "    def __init__(self):\n",
        "        # model checkpoints\n",
        "        self.checkpoint_path = \"./checkpoints\"\n",
        "        self.save_top_k = 1\n",
        "        # training\n",
        "        self.batch_size = 128\n",
        "        self.max_epochs = 120\n",
        "        self.training_steps = 250000\n",
        "        self.num_workers = 2\n",
        "        # optimizer\n",
        "        self.lr = 2e-4\n",
        "        self.beta1 = 0.9\n",
        "        self.beta2 = 0.999\n",
        "        self.weight_decay = 0.0 # 1e-2\n",
        "        self.use_wd_schedule = False\n",
        "        self.use_lr_schedule = False\n",
        "        # input properties\n",
        "        self.img_size = 32\n",
        "        self.in_channels = 3\n",
        "        # latents / quantization\n",
        "        self.latent_channels = 10\n",
        "        self.top_codebook_size = 256\n",
        "        self.bottom_codebook_size = 256\n",
        "        self.codebook_size = 512\n",
        "        self.commit_loss_beta = 0.25\n",
        "        self.track_codebook = True\n",
        "        self.use_ema = True\n",
        "        self.ema_gamma = 0.99\n",
        "        # encoder/decoder\n",
        "        self.hidden_channels = 256\n",
        "        self.nblocks = 1\n",
        "        self.nlayers = 1\n",
        "\n",
        "    def update(self, updates):\n",
        "        for key, value in updates.items():\n",
        "            if hasattr(self, key):\n",
        "                setattr(self, key, value)\n",
        "        self.nlayers = get_num_downsample_layers(self.img_size)\n",
        "        self.channel_dims = build_channel_dims(self.start_channels, self.nlayers)\n",
        "\n",
        "    def to_dict(self):\n",
        "        return {k: v for k, v in self.__dict__.items()}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QaKxwooJy5ae"
      },
      "source": [
        "# COMPONENT TESTING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8lGBq2MA9K0E",
        "outputId": "42db88dc-6b9d-4720-aa1d-2d2d2392b57d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
        "dataloader = torch.utils.data.DataLoader(dataset, batch_size=3, shuffle=False, num_workers=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gEj4nMOGybju",
        "outputId": "d0fd00ae-9cd7-44f2-9f59-6952ef76cd46"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([3, 3, 32, 32])"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "img, label = next(iter(dataloader))\n",
        "img.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TjTSwUbAzEBj"
      },
      "outputs": [],
      "source": [
        "config = CIFAR10VQVAE2Config()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Di8siejD1vTD"
      },
      "outputs": [],
      "source": [
        "vqvae2 = VQVAE2(config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Aw6tGkYs1xEQ"
      },
      "outputs": [],
      "source": [
        "out = vqvae2(img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(out['top_quantized']['quantized_inputs'].shape)\n",
        "print(out['bottom_quantized']['quantized_inputs'].shape)\n",
        "print(out['x_hat'].shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CJBt4mU3O_1Q"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "id": "2Oah3ySaKSQk",
        "outputId": "e384e014-2cc7-4951-a02f-c1afcb960973"
      },
      "outputs": [
        {
          "data": {
            "application/javascript": "\n        window._wandbApiKey = new Promise((resolve, reject) => {\n            function loadScript(url) {\n            return new Promise(function(resolve, reject) {\n                let newScript = document.createElement(\"script\");\n                newScript.onerror = reject;\n                newScript.onload = resolve;\n                document.body.appendChild(newScript);\n                newScript.src = url;\n            });\n            }\n            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n            const iframe = document.createElement('iframe')\n            iframe.style.cssText = \"width:0;height:0;border:none\"\n            document.body.appendChild(iframe)\n            const handshake = new Postmate({\n                container: iframe,\n                url: 'https://wandb.ai/authorize'\n            });\n            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n            handshake.then(function(child) {\n                child.on('authorize', data => {\n                    clearTimeout(timeout)\n                    resolve(data)\n                });\n            });\n            })\n        });\n    ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.16.2"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20240127_182906-del2dptb</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jordanlazzaro/VQ-VAE%20CIFAR-10/runs/del2dptb' target=\"_blank\">pretty-sun-44</a></strong> to <a href='https://wandb.ai/jordanlazzaro/VQ-VAE%20CIFAR-10' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/jordanlazzaro/VQ-VAE%20CIFAR-10' target=\"_blank\">https://wandb.ai/jordanlazzaro/VQ-VAE%20CIFAR-10</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/jordanlazzaro/VQ-VAE%20CIFAR-10/runs/del2dptb' target=\"_blank\">https://wandb.ai/jordanlazzaro/VQ-VAE%20CIFAR-10/runs/del2dptb</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loggers/wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: logging graph, to disable use `wandb.watch(log_graph=False)`\n"
          ]
        }
      ],
      "source": [
        "config = CIFAR10VQVAE2Config()\n",
        "model = VQVAE2(config)\n",
        "lit_model = LitVQVAE2(model, config)\n",
        "cifar10_data = CIFAR10DataModule(config)\n",
        "\n",
        "wandb.init(project=\"VQ-VAE-2 CIFAR-10\", config=config.to_dict())\n",
        "wandb_logger = WandbLogger(project=\"VQ-VAE-2 CIFAR-10\", log_model=False)\n",
        "wandb_logger.watch(lit_model, log=\"all\")\n",
        "\n",
        "lr_monitor = LearningRateMonitor(logging_interval='step')\n",
        "\n",
        "checkpoint_callback = ModelCheckpoint(\n",
        "    dirpath=config.checkpoint_path,\n",
        "    filename='model-{epoch:02d}-{val_loss:.2f}',\n",
        "    every_n_epochs=5,\n",
        "    save_top_k=config.save_top_k,\n",
        "    monitor='val/loss',\n",
        "    mode='min',\n",
        "    save_last=True\n",
        ")\n",
        "\n",
        "# Define the EarlyStopping callback\n",
        "early_stop_callback = EarlyStopping(\n",
        "    monitor='val/loss',\n",
        "    min_delta=0.00,\n",
        "    patience=15,\n",
        "    verbose=True,\n",
        "    check_finite=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dFM3LWghKUOv"
      },
      "outputs": [],
      "source": [
        "# wandb.finish()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eWPlQ1PNK4bs",
        "outputId": "4eff6a09-1cb8-4d78-e3f5-441ed0e53888"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:pytorch_lightning.utilities.rank_zero:Using 16bit Automatic Mixed Precision (AMP)\n",
            "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n"
          ]
        }
      ],
      "source": [
        "trainer = pl.Trainer(\n",
        "    max_epochs=config.max_epochs,\n",
        "    devices=1,\n",
        "    accelerator=\"gpu\",\n",
        "    precision=\"16-mixed\",\n",
        "    logger=wandb_logger,\n",
        "    callbacks=[\n",
        "        lr_monitor,\n",
        "        early_stop_callback,\n",
        "        # checkpoint_callback\n",
        "    ],\n",
        "    log_every_n_steps=1,\n",
        "    # overfit_batches=1,\n",
        ")\n",
        "\n",
        "# tuner = Tuner(trainer)\n",
        "# tuner.lr_find(lit_model, datamodule=cifar10_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3-swnb5PK5rK"
      },
      "outputs": [],
      "source": [
        "trainer.fit(lit_model, cifar10_data)\n",
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1BNd1kgOPCeH"
      },
      "source": [
        "# Sweeps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iab5_ZrqK7td"
      },
      "outputs": [],
      "source": [
        "sweep_config = {\n",
        "    'method': 'random',\n",
        "    'metric': {\n",
        "        'name': 'val/loss',\n",
        "        'goal': 'minimize'\n",
        "    },\n",
        "    'parameters': {\n",
        "        'nblocks': {\n",
        "            'values': [0, 1, 2]\n",
        "        },\n",
        "        'latent_dim': {\n",
        "            'values': [128, 256, 512]\n",
        "        },\n",
        "        'start_channels': {\n",
        "            'values': [32, 64, 128]\n",
        "        },\n",
        "        'lr': {\n",
        "            'min': 1e-5,\n",
        "            'max': 1e-2,\n",
        "            'distribution': 'uniform'\n",
        "        },\n",
        "        'beta1': {\n",
        "            'values': [0.9, 0.95, 0.99]\n",
        "        },\n",
        "        'beta2': {\n",
        "            'values': [0.999, 0.9999]\n",
        "        },\n",
        "        'kld_weight': {\n",
        "            'min': 0.0000025,\n",
        "            'max': 0.00025,\n",
        "            'distribution': 'uniform'\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "sweep_id = wandb.sweep(sweep_config, project=\"VQ-VAE-2 CIFAR-10\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0BfB-TYDK-Ma"
      },
      "outputs": [],
      "source": [
        "def train():\n",
        "    with wandb.init() as run:\n",
        "        config = CIFAR10VQVAE2Config()\n",
        "        config.update(wandb.config)\n",
        "\n",
        "        model = VQVAE2(config)\n",
        "        lit_model = LitVQVAE2(model, config)\n",
        "        cifar10_data = CIFAR10DataModule(config)\n",
        "\n",
        "        wandb_logger = WandbLogger(project=\"VQ-VAE-2 CIFAR-10\", log_model=False)\n",
        "\n",
        "        early_stop_callback = EarlyStopping(\n",
        "            monitor='val/loss',\n",
        "            min_delta=0.00,\n",
        "            patience=3,\n",
        "            verbose=True,\n",
        "            check_finite=True\n",
        "        )\n",
        "\n",
        "        trainer = pl.Trainer(\n",
        "            max_epochs=config.max_epochs,\n",
        "            devices=1,\n",
        "            accelerator=\"gpu\",\n",
        "            precision=\"16-mixed\",\n",
        "            logger=wandb_logger,\n",
        "            callbacks=[early_stop_callback]\n",
        "        )\n",
        "\n",
        "        trainer.fit(lit_model, cifar10_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cV_krC5nLB2W"
      },
      "outputs": [],
      "source": [
        "wandb.agent(sweep_id, train, count=5)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "7phHAga3Or6w",
        "QaKxwooJy5ae",
        "1BNd1kgOPCeH"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
