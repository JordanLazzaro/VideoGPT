# Video Generation Model
Heavy inspiration from the following work:

[VideoGPT](https://github.com/wilson1yan/VideoGPT)

[FSQ-VAE](https://arxiv.org/abs/2309.15505)

[ViViT](https://arxiv.org/pdf/2103.15691.pdf)

[Sora Tech Report](https://openai.com/research/video-generation-models-as-world-simulators)

[Sora Reverse Engineering](https://arxiv.org/abs/2402.17177)

## Goal

Use FSQ-VAE to build a sequence of latent vectors corresponding to spatio-temporal 
patches (described in Sora tech report) aka "tublets" (as described in ViViT paper).

Sequences of latent vectors from tublets are then modeled by a Transformer Decoder, and mapped back to pixels using FSQ-VAE decoder.

![](https://images.openai.com/blob/1d2955dd-9d05-4f33-b346-be531d2a7737/figure-patches.png?trim=0,0,0,0&width=2600)

![](https://i.imgur.com/9G7QTfV.png)

## Dataset Source
Steamboat Willie source: https://archive.org/download/steamboat-willie-mickey

## Current Highlights
Best clip reconstructions from VQ-VAE before switching to FSQ:

![](assets/wooing-infatuation-93-1.gif)
![](assets/wooing-infatuation-93-2.gif)

Current best clip reconstructions from FSQ-VAE:

![](assets/super_snowball_23_1.gif)
![](assets/super_snowball_23_2.gif)

## Project Roadmap

- [X] Part 1: VAE (for knowledge/context) - CIFAR10

- [X] Part 2: VQ-VAE/VQ-VAE 2 (Conv2D) - CIFAR10

- [X] Part 3: VQ-VAE (Conv3D) - Steamboat Willie

- [X] Part 3.5: FSQ-VAE (Conv3D) - Steamboat Willie

- [ ] Part 4: Transformer Decoder - latent codes from Part 3

- [ ] Part 5: VideoGPT - put it all together

VideoGPT utilizes a two-model, two-stage approach

## FSQ-VAE/VQ-VAE (Visual Compression)
Originally, I was going to compress 16-frame, full spatial dim video clips with a VAE as proposed in VideoGPT
to produce latents for a transformer decoder to take as input. However, this is problematic due to the number of
latent vectors that would be produced for a single clip (I want longer videos). Instead, I will use the
spatiotemporal "tubelet" approach from ViViT to produce d-dimensional latent vectors for a spatial patch across
a set temporal interval.

I decided to try Finite Scalar Quantization, as I saw the paper come out around the time I 
was thinking about this project. I then proceeded to run into almost all the problems it
mentioned with Vector Quantization, so I promptly replaced it with FSQ instead of attempting
all the hacky workarounds proposed to mitigate the issues.

## Transformer Decoder (Visual Prediction)
The resulting latent codebook is used as a vocabulary for a transformer decoder to learn to model sequences of
video frames in latent space

## Together - VideoGPT
Sequences of discrete latent vectors are predicted/generated by the Transformer Decoder and decoded back into
image space using the pre-trained VQ-VAE decoder
