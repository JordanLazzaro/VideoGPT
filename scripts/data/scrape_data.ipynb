{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q selenium undetected-chromedriver beautifulsoup4 webdriver-manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "GAMEBOY_LP_URL = 'https://longplays.org/infusions/longplays/longplays.php?cat_id=30'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import undetected_chromedriver as uc\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "\n",
    "def setup_browser():\n",
    "    options = uc.ChromeOptions()\n",
    "    options.add_argument('--disable-gpu')\n",
    "    options.add_argument('--no-sandbox')\n",
    "    options.add_argument('--disable-dev-shm-usage')\n",
    "    options.add_argument('--window-size=1920,1080')\n",
    "    \n",
    "    driver = uc.Chrome(options=options)\n",
    "    return driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from typing import Optional\n",
    "\n",
    "class Scraper:\n",
    "    def __init__(self):\n",
    "        self.driver = setup_browser()\n",
    "        self.wait = WebDriverWait(self.driver, 20)\n",
    "    \n",
    "    def load_page(self, url: str) -> bool:\n",
    "        try:\n",
    "            self.driver.get(url)\n",
    "            # Wait for Cloudflare challenge to pass (if any)\n",
    "            time.sleep(5)\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading page: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def wait_for_element(self, selector: str) -> Optional[str]:\n",
    "        try:\n",
    "            element = self.wait.until(\n",
    "                EC.presence_of_element_located((By.CSS_SELECTOR, selector))\n",
    "            )\n",
    "            return element\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "    def parse_content(self, selector: str = \"body\") -> Optional[dict]:\n",
    "        try:\n",
    "            # Wait for content to load\n",
    "            self.wait_for_element(selector)\n",
    "            \n",
    "            # Get the page source after JavaScript execution\n",
    "            html = self.driver.page_source\n",
    "            soup = BeautifulSoup(html, 'html.parser')\n",
    "            \n",
    "            # Example parsing (modify based on your needs)\n",
    "            data = {\n",
    "                'title': soup.title.text if soup.title else None,\n",
    "                'content': soup.select(selector)[0].text if soup.select(selector) else None\n",
    "            }\n",
    "            return data\n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing content: {e}\")\n",
    "            return None\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    scraper = DynamicScraper()\n",
    "    \n",
    "    # Example usage\n",
    "    url = \"https://example.com\"\n",
    "    if scraper.load_page(url):\n",
    "        # Wait for specific content (modify selector as needed)\n",
    "        data = scraper.parse_content(\"#main-content\")\n",
    "        \n",
    "        if data:\n",
    "            print(f\"Title: {data['title']}\")\n",
    "            print(f\"Content: {data['content'][:200]}...\")  # First 200 chars\n",
    "    \n",
    "    # Clean up\n",
    "    scraper.driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "detail_table = soup.find_all('table', class_='tblDetail')\n",
    "links = []\n",
    "for element in detail_table:\n",
    "    links = element.find_all('a', href=True)\n",
    "# download_cells = detail_table.find_all('a', href=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://longplays.org/infusions/longplays/longplays.php?cat_id=30&author=8923\n",
      "https://longplays.org/infusions/longplays/longplays.php?cat_id=30\n",
      "https://longplays.org/infusions/longplays/longplays.php?file_id=771\n",
      "https://youtu.be/cR-l6VbPQOE\n",
      "https://www.mobygames.com/game/5335/kirbys-dream-land/\n"
     ]
    }
   ],
   "source": [
    "for link in links:\n",
    "    if link['']\n",
    "    # link_url = link['href']\n",
    "    # if '../../' in link_url:\n",
    "    #     link_url = urljoin(url, link_url)\n",
    "    \n",
    "    # print(link_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
