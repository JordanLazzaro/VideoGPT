{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "GAMEBOY_LP_URL = 'https://longplays.org/infusions/longplays/longplays.php?cat_id=30'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import random\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "from tqdm.asyncio import tqdm_asyncio\n",
    "from tqdm.asyncio import tqdm_asyncio\n",
    "from playwright.async_api import async_playwright"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def scrape_site(url, selector='table'):\n",
    "    async with async_playwright() as p:\n",
    "        browser = await p.firefox.launch()\n",
    "        page = await browser.new_page()\n",
    "        await page.goto(url)\n",
    "        await page.wait_for_selector(selector)\n",
    "        content = await page.inner_html(selector)\n",
    "        await browser.close()\n",
    "        return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = await scrape_site(GAMEBOY_LP_URL, selector='tbody')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = soup.find_all('a')\n",
    "longplays = [{'name': link.text, 'url': urljoin(GAMEBOY_LP_URL, link.get('href'))} for link in links if 'longplay_id=' in link.get('href')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'Waterworld', 'url': 'https://longplays.org/infusions/longplays/longplays.php?cat_id=30&longplay_id=4913'}\n"
     ]
    }
   ],
   "source": [
    "for lp in longplays:\n",
    "    if lp['url'] == 'https://longplays.org/infusions/longplays/longplays.php?cat_id=30&longplay_id=4913':\n",
    "        print(lp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def scrape_longplays(longplays, max_concurrent=3, wait_for_selector='table.tblDetail'):\n",
    "    ''' assuming longplays: [{ 'name': '', url: '' }, ...]'''\n",
    "    async with async_playwright() as p:\n",
    "        browser = await p.firefox.launch()\n",
    "        semaphore = asyncio.Semaphore(max_concurrent)\n",
    "        data = []\n",
    "        \n",
    "        async def scrape_longplay(longplay):\n",
    "            async with semaphore:\n",
    "                await asyncio.sleep(random.uniform(1, 3))\n",
    "                page = await browser.new_page()\n",
    "                try:\n",
    "                    # get page content\n",
    "                    await page.goto(longplay['url'])\n",
    "                    await page.wait_for_selector(wait_for_selector)\n",
    "                    content = await page.content()\n",
    "                    await page.close()\n",
    "                    \n",
    "                    # extract and store longplay metadata\n",
    "                    soup = BeautifulSoup(content, 'html.parser')\n",
    "                    authors_ = soup.find_all('a', href=lambda x: x and 'author=' in x)\n",
    "                    download_links_ = soup.find_all('a', href=lambda x: x and 'file_id=' in x)\n",
    "\n",
    "                    authors = [{'username': link.text, 'url': urljoin(GAMEBOY_LP_URL, link.get('href'))} for link in authors_]\n",
    "                    download_links = [urljoin(GAMEBOY_LP_URL, link.get('href')) for link in download_links_]\n",
    "                    \n",
    "                    data.append({\n",
    "                        'name': longplay['name'],\n",
    "                        'authors': authors,\n",
    "                        'downloads': download_links\n",
    "                    })\n",
    "                    # return url, BeautifulSoup(content, 'html.parser')\n",
    "                except Exception as e:\n",
    "                    print(f\"Error scraping {longplay['url']}: {e}\")\n",
    "                    await page.close()\n",
    "                    return None\n",
    "\n",
    "        await tqdm_asyncio.gather(\n",
    "            *[scrape_longplay(longplay) for longplay in longplays],\n",
    "            desc=\"Scraping sites\",\n",
    "            total=len(longplays)\n",
    "        )\n",
    "        \n",
    "        await browser.close()\n",
    "        return { 'longplays': data }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = await scrape_longplays(longplays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('longplay_metadata.json', 'w') as f:\n",
    "    json.dump(results, f, indent=2)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
