{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q opencv-python h5py tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import av\n",
    "import cv2\n",
    "import numpy as np\n",
    "import h5py\n",
    "from pathlib import Path\n",
    "import concurrent.futures\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "from typing import List\n",
    "from pathlib import Path\n",
    "import zarr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ../gameboyprep/data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd VideoGen/data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp4_files = []\n",
    "for file in Path('./data/gameboy_longplays_mp4').glob('*.mp4'):\n",
    "   mp4_files.append(f'./data/gameboy_longplays_mp4/{file.name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp4_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_video_frames(video_path):\n",
    "    try:\n",
    "        with av.open(video_path) as container:\n",
    "            frame_count = container.streams.video[0].frames\n",
    "            return True, frame_count\n",
    "    except:\n",
    "        return False, None\n",
    "\n",
    "def scan_videos(directory):\n",
    "    video_files = list(Path(directory).glob(\"**/*.mp4\"))\n",
    "    \n",
    "    print(f\"Found {len(video_files)} MP4 files\")\n",
    "    issues = []\n",
    "    \n",
    "    for video_path in tqdm(video_files, desc=\"Scanning videos\"):\n",
    "        has_frames, count = check_video_frames(str(video_path))\n",
    "        if not has_frames:\n",
    "            issues.append(video_path)\n",
    "    \n",
    "    print(\"\\nVideos without accessible frame counts:\")\n",
    "    for path in issues:\n",
    "        print(f\"- {path}\")\n",
    "    \n",
    "    print(f\"\\nTotal: {len(issues)}/{len(video_files)} videos have inaccessible frame counts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scan_videos(\"./data/gameboy_longplays_mp4/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_video_to_hdf5_chunked(\n",
    "    video_path: str,\n",
    "    output_dir: str,\n",
    "    target_size: tuple = (256, 256),\n",
    "    chunk_size: int = 16,\n",
    "    dataset_name: str = \"video_frames\",\n",
    "    compression: str = \"gzip\",\n",
    "    compression_opts: int = 4\n",
    "):\n",
    "    \"\"\"\n",
    "    Decodes a single MP4 file in 'chunk_size' batches and writes frames to an HDF5 file.\n",
    "    \n",
    "    Args:\n",
    "        video_path (str): Path to the input MP4 file.\n",
    "        output_dir (str): Directory where the per-video HDF5 file will be created.\n",
    "        chunk_size (int): Number of frames to read and write at a time (batch size).\n",
    "        dataset_name (str): Name of the dataset inside the HDF5 file.\n",
    "        compression (str): HDF5 compression algorithm (e.g., 'gzip', 'lzf', or None).\n",
    "        compression_opts (int): Compression level (if using 'gzip', 1-9).\n",
    "    \"\"\"\n",
    "    longplay_id, video_id, _ = os.path.basename(video_path).split('_', maxsplit=2)\n",
    "    hdf5_out_path = os.path.join(output_dir, f\"{longplay_id}_{video_id}.h5\")\n",
    "    \n",
    "    def process_frame(frame):\n",
    "        arr = frame.to_ndarray(format='rgb24')\n",
    "        arr = cv2.cvtColor(arr, cv2.COLOR_RGB2GRAY)\n",
    "        return cv2.resize(arr, target_size)\n",
    "    \n",
    "    def write_batch(f, dset, batch_frames, frame_count):\n",
    "        batch_array = np.stack(batch_frames, axis=0)\n",
    "        if dset is None:\n",
    "            height, width = batch_array.shape[1:3]\n",
    "            dset = f.create_dataset(\n",
    "                dataset_name,\n",
    "                shape=(0, height, width, 1),  # since it's grayscale\n",
    "                maxshape=(None, height, width, 1),\n",
    "                dtype=batch_array.dtype,\n",
    "                chunks=(chunk_size, height, width, 1),\n",
    "                compression=compression,\n",
    "                compression_opts=compression_opts\n",
    "            )\n",
    "        \n",
    "        dset.resize(dset.shape[0] + batch_array.shape[0], axis=0)\n",
    "        dset[-batch_array.shape[0]:] = batch_array[..., np.newaxis]  # add channel dimension\n",
    "        frame_count += batch_array.shape[0]\n",
    "        return dset, frame_count\n",
    "\n",
    "    def get_total_frames(container):\n",
    "        return container.streams.video[0].frames\n",
    "\n",
    "    with av.open(video_path) as container:\n",
    "        with h5py.File(hdf5_out_path, \"w\") as f:\n",
    "            total_frames = get_total_frames(container)\n",
    "            with tqdm(total=total_frames, desc=f\"Processing {longplay_id}_{video_id}\", unit=\"frames\", leave=False, position=1) as pbar:\n",
    "        \n",
    "                dset = None\n",
    "                frame_count = 0\n",
    "                batch_frames = []\n",
    "                \n",
    "                for frame in container.decode(video=0):\n",
    "                    batch_frames.append(process_frame(frame))\n",
    "                    \n",
    "                    if len(batch_frames) == chunk_size:\n",
    "                        dset, frame_count = write_batch(f, dset, batch_frames, frame_count)\n",
    "                        batch_frames = []\n",
    "                        pbar.update(chunk_size)\n",
    "                \n",
    "                # write any remaining frames\n",
    "                if batch_frames:\n",
    "                    dset, frame_count = write_batch(f, dset, batch_frames, frame_count)\n",
    "                    pbar.update(len(batch_frames))\n",
    "                \n",
    "                if dset is not None:\n",
    "                    dset.attrs[\"video_path\"] = video_path\n",
    "                    \n",
    "                    # original video properties\n",
    "                    stream = container.streams.video[0]\n",
    "                    dset.attrs[\"original_width\"] = stream.width\n",
    "                    dset.attrs[\"original_height\"] = stream.height\n",
    "                    dset.attrs[\"fps\"] = float(stream.average_rate)\n",
    "                    dset.attrs[\"duration_seconds\"] = float(stream.duration * stream.time_base)\n",
    "                    dset.attrs[\"total_frames\"] = stream.frames\n",
    "                    \n",
    "                    # processing parameters\n",
    "                    dset.attrs[\"target_size\"] = target_size\n",
    "                    dset.attrs[\"compression\"] = compression\n",
    "                    dset.attrs[\"compression_level\"] = compression_opts\n",
    "                    dset.attrs[\"chunk_size\"] = chunk_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_videos_in_parallel_chunked(\n",
    "    video_paths: List[str],\n",
    "    output_dir: str,\n",
    "    target_size: tuple = (256, 256),\n",
    "    chunk_size: int = 16,\n",
    "    max_workers: int = 4\n",
    "):\n",
    "    \"\"\"\n",
    "    Decodes multiple MP4 files in parallel, each to its own HDF5 file using chunked writes.\n",
    "    \n",
    "    Args:\n",
    "        video_paths (List[str]): List of MP4 paths to decode.\n",
    "        output_dir (str): Directory where per-video HDF5 files will be placed.\n",
    "        chunk_size (int): Number of frames to read/write at a time for each video.\n",
    "        max_workers (int): Number of parallel processes.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    total_videos = len(video_paths)\n",
    "    \n",
    "    with tqdm(total=total_videos, desc=\"Processing videos\", unit=\"video\", position=0) as pbar:\n",
    "        with concurrent.futures.ProcessPoolExecutor(max_workers=max_workers) as executor:\n",
    "            futures = {\n",
    "                executor.submit(decode_video_to_hdf5_chunked, path, output_dir, target_size, chunk_size): path\n",
    "                for path in video_paths\n",
    "            }\n",
    "            \n",
    "            for future in concurrent.futures.as_completed(futures):\n",
    "                path = futures[future]\n",
    "                try:\n",
    "                    future.result()\n",
    "                    pbar.update(1)\n",
    "                except Exception as exc:\n",
    "                    print(f\"\\nError decoding {path}: {exc}\")\n",
    "                    pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decode_videos_in_parallel_chunked(\n",
    "    mp4_files[:5],\n",
    "    output_dir=\"data/longplay_h5_files\",\n",
    "    chunk_size=16,\n",
    "    max_workers=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_hdf5_with_viz(file_path, num_sample_frames=5):\n",
    "    with h5py.File(file_path, 'r') as f:\n",
    "        # Get dataset name (assuming one dataset)\n",
    "        dataset_name = list(f.keys())[0]\n",
    "        dataset = f[dataset_name]\n",
    "        \n",
    "        print(f\"Dataset: {dataset_name}\")\n",
    "        print(f\"Shape: {dataset.shape}\")\n",
    "        print(f\"Dtype: {dataset.dtype}\")\n",
    "        print(\"\\nAttributes:\")\n",
    "        for key, value in dataset.attrs.items():\n",
    "            print(f\"- {key}: {value}\")\n",
    "        \n",
    "        # Visualize sample frames\n",
    "        num_frames = dataset.shape[0]\n",
    "        indices = np.linspace(0, num_frames-1, num_sample_frames, dtype=int)\n",
    "        \n",
    "        fig, axes = plt.subplots(1, num_sample_frames, figsize=(20, 4))\n",
    "        for i, idx in enumerate(indices):\n",
    "            frame = dataset[idx]\n",
    "            if frame.shape[-1] == 1:  # If single channel\n",
    "                frame = frame.squeeze()  # Remove channel dimension\n",
    "            axes[i].imshow(frame, cmap='gray')\n",
    "            axes[i].axis('off')\n",
    "            axes[i].set_title(f'Frame {idx}')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Show frame statistics\n",
    "        print(\"\\nFrame Statistics:\")\n",
    "        print(f\"First Frame - Min: {np.min(dataset[0])}, Max: {np.max(dataset[0])}, Mean: {np.mean(dataset[0]):.2f}\")\n",
    "        print(f\"Last Frame - Min: {np.min(dataset[-1])}, Max: {np.max(dataset[-1])}, Mean: {np.mean(dataset[-1]):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect_hdf5_with_viz('longplays.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_zarr_with_viz(zarr_path, num_sample_frames=5):\n",
    "    \"\"\"\n",
    "    Inspect a Zarr array containing video frames and visualize sample frames.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    zarr_path : str\n",
    "        Path to input Zarr store\n",
    "    num_sample_frames : int\n",
    "        Number of sample frames to visualize (default: 5)\n",
    "    \"\"\"\n",
    "    # Open the zarr array (assuming frames are in a dataset called 'frames')\n",
    "    root = zarr.open(zarr_path)\n",
    "    dataset = root['frames']  # Assuming 'frames' is the dataset name\n",
    "    \n",
    "    # Print basic information\n",
    "    print(f\"Dataset: frames\")\n",
    "    print(f\"Shape: {dataset.shape}\")\n",
    "    print(f\"Dtype: {dataset.dtype}\")\n",
    "    print(f\"Chunks: {dataset.chunks}\")\n",
    "    print(\"\\nAttributes:\")\n",
    "    for key, value in dataset.attrs.items():\n",
    "        print(f\"- {key}: {value}\")\n",
    "    \n",
    "    # Visualize sample frames\n",
    "    num_frames = dataset.shape[0]\n",
    "    indices = np.linspace(0, num_frames-1, num_sample_frames, dtype=int)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, num_sample_frames, figsize=(20, 4))\n",
    "    \n",
    "    # Handle case where num_sample_frames = 1\n",
    "    if num_sample_frames == 1:\n",
    "        axes = [axes]\n",
    "        \n",
    "    for i, idx in enumerate(indices):\n",
    "        frame = dataset[idx]\n",
    "        if frame.shape[-1] == 1:  # If single channel\n",
    "            frame = frame.squeeze()  # Remove channel dimension\n",
    "        axes[i].imshow(frame, cmap='gray')\n",
    "        axes[i].axis('off')\n",
    "        axes[i].set_title(f'Frame {idx}')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Show frame statistics\n",
    "    print(\"\\nFrame Statistics:\")\n",
    "    # Using numpy operations directly on zarr arrays\n",
    "    print(f\"First Frame - Min: {np.min(dataset[0])}, Max: {np.max(dataset[0])}, \"\n",
    "          f\"Mean: {np.mean(dataset[0]):.2f}\")\n",
    "    print(f\"Last Frame - Min: {np.min(dataset[-1])}, Max: {np.max(dataset[-1])}, \"\n",
    "          f\"Mean: {np.mean(dataset[-1]):.2f}\")\n",
    "    \n",
    "    # Add memory usage information\n",
    "    nbytes = dataset.nbytes\n",
    "    print(f\"\\nMemory Usage:\")\n",
    "    print(f\"Total size: {nbytes / (1024**2):.2f} MB\")\n",
    "    print(f\"Chunk size: {np.prod(dataset.chunks) * dataset.dtype.itemsize / 1024:.2f} KB\")\n",
    "    \n",
    "    # Add compression information\n",
    "    if dataset.compressor:\n",
    "        print(\"\\nCompression Info:\")\n",
    "        print(f\"Compressor: {dataset.compressor}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect_zarr_with_viz('longplay_zarr_files/100_0.zarr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd VideoGen/data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File('./longplay_h5_files/0_0.h5', 'r') as f_old:\n",
    "    with h5py.File('./longplay_h5_files/0_0.new.h5', 'r') as f_new:\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rechunk_dataset(filename, dataset_name, new_chunk_size):\n",
    "    with h5py.File(filename, 'r+') as f:\n",
    "        # Get original dataset\n",
    "        old_dset = f[dataset_name]\n",
    "        \n",
    "        # Store original attributes and dtype\n",
    "        attrs = dict(old_dset.attrs)\n",
    "        dtype = old_dset.dtype\n",
    "        shape = old_dset.shape\n",
    "        \n",
    "        # Create temporary dataset name\n",
    "        temp_name = dataset_name + '_temp'\n",
    "        \n",
    "        # Create new dataset with desired chunk size\n",
    "        new_dset = f.create_dataset(\n",
    "            temp_name,\n",
    "            shape=shape,\n",
    "            dtype=dtype,\n",
    "            chunks=(new_chunk_size,) + shape[1:],  # Assuming first dim is frames\n",
    "            compression=old_dset.compression,\n",
    "            compression_opts=old_dset.compression_opts\n",
    "        )\n",
    "        \n",
    "        # Copy data in reasonable block sizes\n",
    "        block_size = 4096  # Adjust based on memory constraints\n",
    "        for i in tqdm(range(0, shape[0], block_size)):\n",
    "            end = min(i + block_size, shape[0])\n",
    "            new_dset[i:end] = old_dset[i:end]\n",
    "        \n",
    "        # Copy attributes\n",
    "        for key, value in attrs.items():\n",
    "            new_dset.attrs[key] = value\n",
    "        \n",
    "        # Delete old dataset\n",
    "        del f[dataset_name]\n",
    "        \n",
    "        # Rename new dataset to original name\n",
    "        f[dataset_name] = f[temp_name]\n",
    "        del f[temp_name]\n",
    "\n",
    "# Usage example:\n",
    "# rechunk_dataset('video.h5', '/frames', new_chunk_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rechunk_dataset(filename='longplay_h5_files/101_0.h5', dataset_name='video_frames', new_chunk_size=1)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
