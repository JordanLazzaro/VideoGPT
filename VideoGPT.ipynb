{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YM-sO50_MNBM"
      },
      "outputs": [],
      "source": [
        "!pip install -q wandb pytorch_lightning av"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "2bkr0JVXBgxw"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pytorch_lightning as pl\n",
        "from torchvision.datasets.video_utils import VideoClips\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torchvision.transforms import Compose, Lambda, Resize, ToTensor, CenterCrop, Grayscale"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wCCwpzdaEHw9",
        "outputId": "6faaa634-d8a2-4596-93f7-820b2885c312"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "steamboat_willie_gdrive_path = '/content/drive/My Drive/SteamboatWillie.mp4'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d1AMOdC0FLCM",
        "outputId": "91c5cc7f-e91a-4419-e4f3-22d498051ec4"
      },
      "outputs": [],
      "source": [
        "class SteamboatWillieDataset(Dataset):\n",
        "    def __init__(self, paths, clip_length=16, dest_dir='./clips/'):\n",
        "        super().__init__()\n",
        "        video_clips = VideoClips(\n",
        "            paths,\n",
        "            clip_length_in_frames=clip_length,\n",
        "            frames_between_clips=clip_length\n",
        "        )\n",
        "\n",
        "        transforms = Compose([\n",
        "            Lambda(lambda x: x.permute(0, 3, 1, 2)),     # (T, H, W, C) to (T, C, H, W) for Greyscale\n",
        "            Grayscale(num_output_channels=1),            # Convert to grayscale\n",
        "            Lambda(lambda x: x.permute(1, 0, 2, 3)),     # (T, C, H, W) to (C, T, H, W) for Conv3d\n",
        "            Lambda(lambda x: CenterCrop((480, 575))(x)), # Center crop to remove virtical bars\n",
        "            Lambda(lambda x: Resize((256, 256))(x)),     # Resize frames\n",
        "            Lambda(lambda x: x / 255.),                  # Scale pixel values to [0, 1]\n",
        "        ])\n",
        "\n",
        "        self.clips = self.build_clip_refs(self.build_clip_paths(video_clips, transforms, dest_dir))\n",
        "        \n",
        "    def build_clip_paths(self, video_clips, transforms, dest_dir):\n",
        "        \"\"\"\n",
        "        Build set of binary files to store processed video clips\n",
        "        \"\"\"\n",
        "        print('here')\n",
        "        clip_paths = {}\n",
        "        \n",
        "        if not os.path.exists(dest_dir):\n",
        "            os.makedirs(dest_dir)\n",
        "        \n",
        "        for idx in range(video_clips.num_clips()):\n",
        "            # transform clips and write to mmap file\n",
        "            clip, _, _, _ = video_clips.get_clip(idx)\n",
        "            clip = transforms(clip)\n",
        "            clip_np = clip.numpy()\n",
        "            mmapped_file_path = os.path.join(dest_dir, f'clip_{idx}.bin')\n",
        "            fp = np.memmap(mmapped_file_path, dtype='float32', mode='w+', shape=clip_np.shape)\n",
        "            fp[:] = clip_np[:]\n",
        "            fp.flush()\n",
        "            del fp\n",
        "            clip_paths[idx] = mmapped_file_path\n",
        "\n",
        "        return clip_paths\n",
        "\n",
        "    def build_clip_refs(self, clip_paths):\n",
        "        \"\"\"\n",
        "        Build reference to mmap files\n",
        "        \"\"\"\n",
        "        clips = {}\n",
        "        for idx, path in clip_paths.items():\n",
        "            clips[idx] = np.memmap(path, dtype='float32', mode='r')\n",
        "        \n",
        "        return clips\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.clips)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return torch.tensor(self.clips[idx], dtype=torch.float32)\n",
        "\n",
        "\n",
        "class SteamboatWillieDataModule(pl.LightningDataModule):\n",
        "    def __init__(self, paths, batch_size=8, train_split=0.8):\n",
        "        super().__init__()\n",
        "        self.paths = paths\n",
        "        self.batch_size = batch_size\n",
        "        self.train_split = train_split\n",
        "\n",
        "    def prepare_data(self):\n",
        "        self.full_dataset = SteamboatWillieDataset(self.paths)\n",
        "\n",
        "    def setup(self, stage=None):\n",
        "        if stage == 'fit' or stage is None:\n",
        "            train_len = int(len(self.full_dataset) * self.train_split)\n",
        "            val_len = len(self.full_dataset) - train_len\n",
        "            self.train_dataset, self.val_dataset = random_split(self.full_dataset, [train_len, val_len])\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return torch.utils.data.DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True, num_workers=0)\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return torch.utils.data.DataLoader(self.val_dataset, batch_size=self.batch_size, shuffle=False, num_workers=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_module = SteamboatWillieDataModule([steamboat_willie_gdrive_path])\n",
        "data_module.prepare_data()\n",
        "data_module.setup()\n",
        "\n",
        "train_loader = data_module.train_dataloader()\n",
        "val_loader = data_module.val_dataloader()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_batch = next(iter(train_loader))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!cp -r clips /content/drive/My\\ Drive/"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
