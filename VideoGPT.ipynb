{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YM-sO50_MNBM"
      },
      "outputs": [],
      "source": [
        "!pip install -q wandb pytorch_lightning av"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "2bkr0JVXBgxw"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "import pytorch_lightning as pl\n",
        "from tqdm.notebook import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.animation import FuncAnimation\n",
        "from IPython.display import display, HTML\n",
        "from torch import nn\n",
        "from torchvision.datasets.video_utils import VideoClips\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torchvision.transforms import Compose, Lambda, Resize, ToTensor, CenterCrop, Grayscale"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wCCwpzdaEHw9",
        "outputId": "6faaa634-d8a2-4596-93f7-820b2885c312"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "steamboat_willie_gdrive_path = '/content/drive/My Drive/SteamboatWillie/SteamboatWillie.mp4'\n",
        "!cp -r /content/drive/My\\ Drive/SteamboatWillie/clips ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d1AMOdC0FLCM",
        "outputId": "91c5cc7f-e91a-4419-e4f3-22d498051ec4"
      },
      "outputs": [],
      "source": [
        "class SteamboatWillieDataset(Dataset):\n",
        "    def __init__(self, paths, clip_length=16, dest_dir='./clips/'):\n",
        "        super().__init__()\n",
        "        if os.path.exists(dest_dir):\n",
        "            clip_paths = self.build_existing_clip_paths(dest_dir)\n",
        "            self.clips = self.build_clip_refs(clip_paths)\n",
        "        else:\n",
        "            video_clips = VideoClips(\n",
        "                paths,\n",
        "                clip_length_in_frames=clip_length,\n",
        "                frames_between_clips=clip_length\n",
        "            )\n",
        "\n",
        "            transforms = Compose([\n",
        "                Lambda(lambda x: x.permute(0, 3, 1, 2)),     # (T, H, W, C) to (T, C, H, W) for Greyscale\n",
        "                Grayscale(num_output_channels=1),            # Convert to grayscale\n",
        "                Lambda(lambda x: x.permute(1, 0, 2, 3)),     # (T, C, H, W) to (C, T, H, W) for Conv3d\n",
        "                Lambda(lambda x: CenterCrop((480, 575))(x)), # Center crop to remove virtical bars\n",
        "                Lambda(lambda x: Resize((256, 256))(x)),     # Resize frames\n",
        "                Lambda(lambda x: x / 255.),                  # Scale pixel values to [0, 1]\n",
        "            ])\n",
        "\n",
        "            self.clips = self.build_clip_refs(self.build_clip_paths(video_clips, transforms, dest_dir))\n",
        "        \n",
        "    def build_clip_paths(self, video_clips, transforms, dest_dir):\n",
        "        \"\"\"\n",
        "        Build set of binary files to store processed video clips\n",
        "        returns dict of clip_idx -> mmapped file path\n",
        "        \"\"\"\n",
        "        clip_paths = {}\n",
        "        \n",
        "        if not os.path.exists(dest_dir):\n",
        "            os.makedirs(dest_dir)\n",
        "        \n",
        "        for idx in tqdm(range(video_clips.num_clips()), desc='Creating clip .bin files'):\n",
        "            # transform clips and write to mmap file\n",
        "            clip, _, _, _ = video_clips.get_clip(idx)\n",
        "            clip = transforms(clip)\n",
        "            clip_np = clip.numpy()\n",
        "            \n",
        "            mmapped_file_path = os.path.join(dest_dir, f'clip_{idx}.bin')\n",
        "            fp = np.memmap(mmapped_file_path, dtype='float32', mode='w+', shape=clip_np.shape)\n",
        "            fp[:] = clip_np[:]\n",
        "            fp.flush()\n",
        "            del fp\n",
        "            clip_paths[idx] = mmapped_file_path\n",
        "\n",
        "        return clip_paths\n",
        "    \n",
        "    def build_existing_clip_paths(self, dest_dir):\n",
        "        \"\"\"\"\n",
        "        returns dict of clip_idx -> mmapped file path\n",
        "        from existing .bin files\n",
        "        \"\"\"\n",
        "        clips_paths = {}\n",
        "        for filename in os.listdir(dest_dir):\n",
        "            if filename.startswith('clip_') and filename.endswith('.bin'):\n",
        "                idx = int(filename.split('_')[1].split('.')[0])\n",
        "                file_path = os.path.join(dest_dir, filename)\n",
        "                clips_paths[idx] = file_path\n",
        "        \n",
        "        return clips_paths\n",
        "\n",
        "    def build_clip_refs(self, clip_paths):\n",
        "        \"\"\"\n",
        "        Build reference to mmap files\n",
        "        returns dict of clip_idx -> np array connected to respective mmap file\n",
        "        \"\"\"\n",
        "        clips = {}\n",
        "        for idx, path in tqdm(clip_paths.items(), desc='Building clip refs'):\n",
        "            clips[idx] = np.memmap(path, dtype='float32', mode='r')\n",
        "        \n",
        "        return clips\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.clips)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # TODO: change to config values (in_channels, -1, height, width)\n",
        "        return torch.tensor(self.clips[idx], dtype=torch.float32).view(1, -1, 256, 256)\n",
        "\n",
        "\n",
        "class SteamboatWillieDataModule(pl.LightningDataModule):\n",
        "    def __init__(self, paths, batch_size=8, train_split=0.8):\n",
        "        super().__init__()\n",
        "        self.paths = paths\n",
        "        self.batch_size = batch_size\n",
        "        self.train_split = train_split\n",
        "\n",
        "    def prepare_data(self):\n",
        "        self.full_dataset = SteamboatWillieDataset(self.paths)\n",
        "\n",
        "    def setup(self, stage=None):\n",
        "        if stage == 'fit' or stage is None:\n",
        "            train_len = int(len(self.full_dataset) * self.train_split)\n",
        "            val_len = len(self.full_dataset) - train_len\n",
        "            self.train_dataset, self.val_dataset = random_split(self.full_dataset, [train_len, val_len])\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return torch.utils.data.DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True, num_workers=0)\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return torch.utils.data.DataLoader(self.val_dataset, batch_size=self.batch_size, shuffle=False, num_workers=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_module = SteamboatWillieDataModule([steamboat_willie_gdrive_path])\n",
        "data_module.prepare_data()\n",
        "data_module.setup()\n",
        "\n",
        "train_loader = data_module.train_dataloader()\n",
        "val_loader = data_module.val_dataloader()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_batch = next(iter(train_loader))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ResBlock(nn.Module):\n",
        "    def __init__(\n",
        "            self,\n",
        "            in_channels,\n",
        "            out_channels\n",
        "        ):\n",
        "        super().__init__()\n",
        "        if in_channels != out_channels:\n",
        "            self.identity = nn.Conv3d(\n",
        "                in_channels=in_channels,\n",
        "                out_channels=out_channels,\n",
        "                kernel_size=1\n",
        "            )\n",
        "        else:\n",
        "            self.identity = nn.Identity()\n",
        "\n",
        "        self.block = nn.Sequential(\n",
        "            nn.Conv3d(\n",
        "                in_channels=in_channels,\n",
        "                out_channels=in_channels,\n",
        "                kernel_size=3,\n",
        "                padding=1\n",
        "            ),\n",
        "            nn.BatchNorm3d(in_channels),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv3d(\n",
        "                in_channels=in_channels,\n",
        "                out_channels=out_channels,\n",
        "                kernel_size=1\n",
        "            ),\n",
        "            nn.BatchNorm3d(out_channels)\n",
        "        )\n",
        "        self.res_act = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.block(x) + self.identity(x)\n",
        "        return self.res_act(out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, in_channels=1, hidden_channels=32, out_channels=16, nlayers=4, nblocks=1):\n",
        "        super().__init__()\n",
        "        self.downsample_blocks = nn.Sequential(*[\n",
        "            nn.Sequential(\n",
        "                nn.Conv3d(\n",
        "                    in_channels=in_channels if i==0 else hidden_channels,\n",
        "                    out_channels=hidden_channels,\n",
        "                    kernel_size=(4, 4, 4),\n",
        "                    stride=(2, 2, 2),\n",
        "                    padding=(1, 1, 1)\n",
        "                ),\n",
        "                nn.BatchNorm3d(hidden_channels),\n",
        "                nn.ReLU()\n",
        "            ) for i in range(nlayers)\n",
        "        ])\n",
        "\n",
        "        self.res_blocks = nn.Sequential(*[\n",
        "            ResBlock(\n",
        "                in_channels=hidden_channels,\n",
        "                out_channels=out_channels if i==nblocks-1 else hidden_channels\n",
        "            ) for i in range(nblocks)\n",
        "        ])\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.downsample_blocks(x)\n",
        "        x = self.res_blocks(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Decoder(torch.nn.Module):\n",
        "    def __init__(self, in_channels=16, hidden_channels=32, out_channels=1, nlayers=4, nblocks=1):\n",
        "        super().__init__()\n",
        "        self.res_blocks = nn.Sequential(*[\n",
        "            ResBlock(\n",
        "                in_channels=in_channels if i==0 else hidden_channels,\n",
        "                out_channels=hidden_channels,\n",
        "            )\n",
        "            for i in range(nblocks)\n",
        "        ])\n",
        "        \n",
        "        self.upsample_blocks = nn.Sequential(*[\n",
        "            nn.Sequential(\n",
        "                nn.ConvTranspose3d(\n",
        "                    in_channels=hidden_channels,\n",
        "                    out_channels=hidden_channels,\n",
        "                    kernel_size=(4, 4, 4),\n",
        "                    stride=(2, 2, 2),\n",
        "                    padding=(1, 1, 1)),\n",
        "                nn.BatchNorm3d(hidden_channels),\n",
        "                nn.ReLU()\n",
        "            ) for i in range(nlayers-1)\n",
        "        ])\n",
        "\n",
        "        self.out_layer = nn.ConvTranspose3d(\n",
        "            in_channels=hidden_channels,\n",
        "            out_channels=out_channels,\n",
        "            kernel_size=(4, 4, 4),\n",
        "            stride=(2, 2, 2),\n",
        "            padding=(1, 1, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.res_blocks(x)\n",
        "        x = self.upsample_blocks(x)\n",
        "        x = self.out_layer(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "encoder = Encoder()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sample_enc = encoder(train_batch)\n",
        "sample_enc.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "decoder = Decoder()\n",
        "recon_clip = decoder(sample_enc)\n",
        "recon_clip.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Display Clip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def display_clip(clip):\n",
        "    def update(frame_idx):\n",
        "        ax.clear()\n",
        "        ax.imshow(video_clip_np[frame_idx], cmap='gray')\n",
        "        ax.axis('off')\n",
        "\n",
        "    video_clip_np = clip.permute(1, 2, 3, 0).numpy()\n",
        "    fig, ax = plt.subplots()\n",
        "    ani = FuncAnimation(fig, update, frames=range(video_clip_np.shape[0]), interval=50)\n",
        "    plt.close()\n",
        "    display(HTML(ani.to_html5_video()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "clip = train_batch[7].view(1, -1, 256, 256)\n",
        "display_clip(clip)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
